{'loss': 0.0905, 'learning_rate': 4.429050444010465e-05, 'epoch': 0.11}
{'loss': 0.0611, 'learning_rate': 3.161501897638086e-05, 'epoch': 0.37}
{'loss': 0.0374, 'learning_rate': 2.9393697401560293e-05, 'epoch': 0.41}
{'loss': 0.1522, 'learning_rate': 4.591178746453443e-05, 'epoch': 0.08}
{'loss': 0.0345, 'learning_rate': 2.4233675287179802e-05, 'epoch': 0.52}
{'loss': 0.0827, 'learning_rate': 1.6892929541126603e-05, 'epoch': 0.66}
{'loss': 0.1247, 'learning_rate': 4.7882628156716834e-05, 'epoch': 0.04}
{'loss': 0.0604, 'learning_rate': 4.397730203765799e-05, 'epoch': 0.12}
{'loss': 0.0479, 'learning_rate': 1.8612936912586766e-05, 'epoch': 0.63}
{'loss': 0.0429, 'learning_rate': 3.721581487895648e-05, 'epoch': 0.26}
{'loss': 0.0513, 'learning_rate': 3.1467629610523604e-05, 'epoch': 0.37}
{'loss': 0.1489, 'learning_rate': 4.802998765864172e-05, 'epoch': 0.04}
{'loss': 0.0972, 'learning_rate': 4.832470666249149e-05, 'epoch': 0.03}
{'loss': 0.1463, 'learning_rate': 4.977988174399971e-05, 'epoch': 0.0}
{'loss': 0.1086, 'learning_rate': 4.414311507424739e-05, 'epoch': 0.12}
{'loss': 0.0596, 'learning_rate': 3.132024024466635e-05, 'epoch': 0.37}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0894, 'learning_rate': 4.7808948405754396e-05, 'epoch': 0.04}
{'loss': 0.085, 'learning_rate': 4.576439809867718e-05, 'epoch': 0.09}
{'loss': 0.0691, 'learning_rate': 2.9147982062780273e-05, 'epoch': 0.42}
{'loss': 0.0355, 'learning_rate': 1.664721420234658e-05, 'epoch': 0.67}
{'loss': 0.1051, 'learning_rate': 2.3987959948399778e-05, 'epoch': 0.52}
{'loss': 0.0776, 'learning_rate': 4.3829912671800734e-05, 'epoch': 0.12}
{'loss': 0.0671, 'learning_rate': 1.8367221573806746e-05, 'epoch': 0.63}
{'loss': 0.1018, 'learning_rate': 3.706842551309923e-05, 'epoch': 0.26}
{'loss': 0.0552, 'learning_rate': 3.1172850878809094e-05, 'epoch': 0.38}
{'loss': 0.1307, 'learning_rate': 4.795630790767928e-05, 'epoch': 0.04}
{'loss': 0.198, 'learning_rate': 4.8251026911529045e-05, 'epoch': 0.04}
{'loss': 0.21, 'learning_rate': 4.9706201993037275e-05, 'epoch': 0.01}
{'loss': 0.1357, 'learning_rate': 3.102546151295184e-05, 'epoch': 0.38}
{'loss': 0.1082, 'learning_rate': 4.399572570839014e-05, 'epoch': 0.12}
{'loss': 0.0679, 'learning_rate': 4.773526865479195e-05, 'epoch': 0.05}
{'loss': 0.0789, 'learning_rate': 2.8902266724000242e-05, 'epoch': 0.42}
{'loss': 0.0778, 'learning_rate': 4.561700873281993e-05, 'epoch': 0.09}
{'loss': 0.0303, 'learning_rate': 2.3742244609619754e-05, 'epoch': 0.53}
{'loss': 0.0719, 'learning_rate': 1.640149886356656e-05, 'epoch': 0.67}
{'loss': 0.1239, 'learning_rate': 4.368252330594348e-05, 'epoch': 0.13}
{'loss': 0.0536, 'learning_rate': 1.8121506235026722e-05, 'epoch': 0.64}
{'loss': 0.0679, 'learning_rate': 3.087807214709459e-05, 'epoch': 0.38}
{'loss': 0.1201, 'learning_rate': 3.692103614724198e-05, 'epoch': 0.26}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1196, 'learning_rate': 4.7882628156716834e-05, 'epoch': 0.04}
{'loss': 0.2634, 'learning_rate': 4.963252224207482e-05, 'epoch': 0.01}
{'loss': 0.1339, 'learning_rate': 4.817734716056661e-05, 'epoch': 0.04}
{'loss': 0.1101, 'learning_rate': 4.766158890382951e-05, 'epoch': 0.05}
{'loss': 0.0649, 'learning_rate': 3.0730682781237337e-05, 'epoch': 0.39}
{'loss': 0.0998, 'learning_rate': 4.3848336342532884e-05, 'epoch': 0.12}
{'loss': 0.0739, 'learning_rate': 2.8656551385220222e-05, 'epoch': 0.43}
{'loss': 0.0822, 'learning_rate': 4.5469619366962673e-05, 'epoch': 0.09}
{'loss': 0.0795, 'learning_rate': 2.349652927083973e-05, 'epoch': 0.53}
{'loss': 0.0547, 'learning_rate': 1.6155783524786536e-05, 'epoch': 0.68}
{'loss': 0.0565, 'learning_rate': 4.3535133940086225e-05, 'epoch': 0.13}
{'loss': 0.0762, 'learning_rate': 3.058329341538008e-05, 'epoch': 0.39}
{'loss': 0.0575, 'learning_rate': 1.78757908962467e-05, 'epoch': 0.64}
{'loss': 0.0527, 'learning_rate': 3.6773646781384725e-05, 'epoch': 0.27}
{'loss': 0.0814, 'learning_rate': 4.7808948405754396e-05, 'epoch': 0.04}
{'loss': 0.1101, 'learning_rate': 4.9558842491112385e-05, 'epoch': 0.01}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.13, 'learning_rate': 4.8103667409604155e-05, 'epoch': 0.04}
{'loss': 0.1501, 'learning_rate': 4.758790915286707e-05, 'epoch': 0.05}
{'loss': 0.0638, 'learning_rate': 3.043590404952283e-05, 'epoch': 0.39}
{'loss': 0.1253, 'learning_rate': 4.3700946976675635e-05, 'epoch': 0.13}
{'loss': 0.1085, 'learning_rate': 2.84108360464402e-05, 'epoch': 0.43}
{'loss': 0.1172, 'learning_rate': 4.532223000110542e-05, 'epoch': 0.09}
{'loss': 0.0626, 'learning_rate': 1.5910068186006512e-05, 'epoch': 0.68}
{'loss': 0.0649, 'learning_rate': 2.325081393205971e-05, 'epoch': 0.54}
{'loss': 0.0694, 'learning_rate': 4.338774457422897e-05, 'epoch': 0.13}
{'loss': 0.0423, 'learning_rate': 3.0288514683665575e-05, 'epoch': 0.39}
{'loss': 0.0748, 'learning_rate': 1.7630075557466675e-05, 'epoch': 0.65}
{'loss': 0.117, 'learning_rate': 3.662625741552747e-05, 'epoch': 0.27}
{'loss': 0.103, 'learning_rate': 4.773526865479195e-05, 'epoch': 0.05}
{'loss': 0.1739, 'learning_rate': 4.948516274014994e-05, 'epoch': 0.01}
{'loss': 0.1194, 'learning_rate': 4.7514229401904624e-05, 'epoch': 0.05}
{'loss': 0.1085, 'learning_rate': 4.802998765864172e-05, 'epoch': 0.04}
{'loss': 0.0583, 'learning_rate': 3.014112531780832e-05, 'epoch': 0.4}
{'loss': 0.0887, 'learning_rate': 4.355355761081838e-05, 'epoch': 0.13}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0894, 'learning_rate': 4.5174840635248164e-05, 'epoch': 0.1}
{'loss': 0.0759, 'learning_rate': 1.5664352847226488e-05, 'epoch': 0.69}
{'loss': 0.0556, 'learning_rate': 2.8165120707660175e-05, 'epoch': 0.44}
{'loss': 0.069, 'learning_rate': 2.3005098593279687e-05, 'epoch': 0.54}
{'loss': 0.1632, 'learning_rate': 4.324035520837172e-05, 'epoch': 0.14}
{'loss': 0.0886, 'learning_rate': 2.999373595195107e-05, 'epoch': 0.4}
{'loss': 0.1074, 'learning_rate': 4.766158890382951e-05, 'epoch': 0.05}
{'loss': 0.0575, 'learning_rate': 1.738436021868665e-05, 'epoch': 0.65}
{'loss': 0.0702, 'learning_rate': 3.6478868049670215e-05, 'epoch': 0.27}
{'loss': 0.0842, 'learning_rate': 4.7440549650942186e-05, 'epoch': 0.05}
{'loss': 0.1492, 'learning_rate': 4.94114829891875e-05, 'epoch': 0.01}
{'loss': 0.1076, 'learning_rate': 4.795630790767928e-05, 'epoch': 0.04}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0528, 'learning_rate': 2.9846346586093814e-05, 'epoch': 0.4}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0752, 'learning_rate': 4.3406168244961126e-05, 'epoch': 0.13}
{'loss': 0.1459, 'learning_rate': 4.5027451269390916e-05, 'epoch': 0.1}
{'loss': 0.0701, 'learning_rate': 1.5418637508446464e-05, 'epoch': 0.69}
{'loss': 0.0585, 'learning_rate': 2.275938325449966e-05, 'epoch': 0.55}
{'loss': 0.0543, 'learning_rate': 2.7919405368880154e-05, 'epoch': 0.44}
{'loss': 0.0983, 'learning_rate': 4.309296584251447e-05, 'epoch': 0.14}
{'loss': 0.0443, 'learning_rate': 2.9698957220236563e-05, 'epoch': 0.41}
{'loss': 0.0914, 'learning_rate': 4.758790915286707e-05, 'epoch': 0.05}
{'loss': 0.0613, 'learning_rate': 1.713864487990663e-05, 'epoch': 0.66}
{'loss': 0.0863, 'learning_rate': 3.633147868381297e-05, 'epoch': 0.27}
{'loss': 0.0914, 'learning_rate': 4.736686989997974e-05, 'epoch': 0.05}
{'loss': 0.1247, 'learning_rate': 4.9337803238225064e-05, 'epoch': 0.01}
{'loss': 0.0687, 'learning_rate': 4.7882628156716834e-05, 'epoch': 0.04}
{'loss': 0.0652, 'learning_rate': 2.9551567854379308e-05, 'epoch': 0.41}
{'loss': 0.0565, 'learning_rate': 4.325877887910387e-05, 'epoch': 0.14}
{'loss': 0.0776, 'learning_rate': 4.488006190353366e-05, 'epoch': 0.1}
{'loss': 0.0417, 'learning_rate': 1.5172922169666442e-05, 'epoch': 0.7}
{'loss': 0.0695, 'learning_rate': 2.251366791571964e-05, 'epoch': 0.55}
{'loss': 0.1012, 'learning_rate': 4.7514229401904624e-05, 'epoch': 0.05}
{'loss': 0.1138, 'learning_rate': 4.294557647665721e-05, 'epoch': 0.14}
{'loss': 0.0581, 'learning_rate': 2.767369003010013e-05, 'epoch': 0.45}
{'loss': 0.0905, 'learning_rate': 2.9404178488522056e-05, 'epoch': 0.41}
{'loss': 0.0294, 'learning_rate': 1.6892929541126603e-05, 'epoch': 0.66}
{'loss': 0.0767, 'learning_rate': 3.618408931795571e-05, 'epoch': 0.28}
{'loss': 0.1134, 'learning_rate': 4.72931901490173e-05, 'epoch': 0.05}
{'loss': 0.1332, 'learning_rate': 4.926412348726262e-05, 'epoch': 0.01}
{'loss': 0.0863, 'learning_rate': 4.7808948405754396e-05, 'epoch': 0.04}
{'loss': 0.06, 'learning_rate': 2.92567891226648e-05, 'epoch': 0.42}
{'loss': 0.0848, 'learning_rate': 4.311138951324662e-05, 'epoch': 0.14}
{'loss': 0.1142, 'learning_rate': 4.4732672537676406e-05, 'epoch': 0.11}
{'loss': 0.0489, 'learning_rate': 1.4927206830886417e-05, 'epoch': 0.7}
{'loss': 0.062, 'learning_rate': 2.2267952576939615e-05, 'epoch': 0.55}
{'loss': 0.135, 'learning_rate': 4.7440549650942186e-05, 'epoch': 0.05}
{'loss': 0.1109, 'learning_rate': 4.279818711079996e-05, 'epoch': 0.14}
{'loss': 0.0699, 'learning_rate': 2.7427974691320107e-05, 'epoch': 0.45}
{'loss': 0.0815, 'learning_rate': 2.9109399756807547e-05, 'epoch': 0.42}
{'loss': 0.0601, 'learning_rate': 1.664721420234658e-05, 'epoch': 0.67}
{'loss': 0.092, 'learning_rate': 3.603669995209846e-05, 'epoch': 0.28}
{'loss': 0.0631, 'learning_rate': 4.7219510398054865e-05, 'epoch': 0.06}
{'loss': 0.1478, 'learning_rate': 4.919044373630018e-05, 'epoch': 0.02}
{'loss': 0.081, 'learning_rate': 4.773526865479195e-05, 'epoch': 0.05}
{'loss': 0.0499, 'learning_rate': 2.8962010390950295e-05, 'epoch': 0.42}
{'loss': 0.0822, 'learning_rate': 4.296400014738937e-05, 'epoch': 0.14}
{'loss': 0.1359, 'learning_rate': 4.458528317181915e-05, 'epoch': 0.11}
{'loss': 0.0629, 'learning_rate': 2.202223723815959e-05, 'epoch': 0.56}
{'loss': 0.0796, 'learning_rate': 1.4681491492106395e-05, 'epoch': 0.71}
{'eval_loss': 0.05208653584122658, 'eval_accuracy': 0.9829574886901268, 'eval_f1': 0.9829318484444982, 'eval_precision': 0.9878345498783455, 'eval_recall': 0.9780775716694773, 'eval_runtime': 259.6044, 'eval_samples_per_second': 223.086, 'eval_steps_per_second': 27.889, 'epoch': 1.0}
{'loss': 0.1093, 'learning_rate': 4.736686989997974e-05, 'epoch': 0.05}
{'loss': 0.1083, 'learning_rate': 4.714583064709242e-05, 'epoch': 0.06}
{'loss': 0.0861, 'learning_rate': 4.26507977449427e-05, 'epoch': 0.15}
{'train_runtime': 3533.2255, 'train_samples_per_second': 29.504, 'train_steps_per_second': 0.461, 'train_loss': 0.08206545673219226, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0663, 'learning_rate': 2.881462102509304e-05, 'epoch': 0.42}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0573, 'learning_rate': 2.7182259352540083e-05, 'epoch': 0.46}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0628, 'learning_rate': 1.640149886356656e-05, 'epoch': 0.67}
{'loss': 0.099, 'learning_rate': 3.58893105862412e-05, 'epoch': 0.28}
{'loss': 0.0664, 'learning_rate': 4.911676398533773e-05, 'epoch': 0.02}
{'loss': 0.1582, 'learning_rate': 4.766158890382951e-05, 'epoch': 0.05}
{'loss': 0.1, 'learning_rate': 2.866723165923579e-05, 'epoch': 0.43}
{'loss': 0.0722, 'learning_rate': 4.281661078153211e-05, 'epoch': 0.14}
{'loss': 0.1007, 'learning_rate': 4.44378938059619e-05, 'epoch': 0.11}
{'loss': 0.0854, 'learning_rate': 4.72931901490173e-05, 'epoch': 0.05}
{'loss': 0.1322, 'learning_rate': 4.7072150896129975e-05, 'epoch': 0.06}
{'loss': 0.0829, 'learning_rate': 1.4435776153326371e-05, 'epoch': 0.71}
{'loss': 0.0516, 'learning_rate': 2.1776521899379568e-05, 'epoch': 0.56}
{'loss': 0.069, 'learning_rate': 4.2503408379085454e-05, 'epoch': 0.15}
{'loss': 0.0407, 'learning_rate': 2.8519842293378534e-05, 'epoch': 0.43}
{'loss': 0.0858, 'learning_rate': 2.6936544013760056e-05, 'epoch': 0.46}
{'loss': 0.1153, 'learning_rate': 1.6155783524786536e-05, 'epoch': 0.68}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1182, 'learning_rate': 3.574192122038395e-05, 'epoch': 0.29}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1522, 'learning_rate': 4.904308423437529e-05, 'epoch': 0.02}
{'loss': 0.035, 'learning_rate': 2.8372452927521283e-05, 'epoch': 0.43}
{'loss': 0.1364, 'learning_rate': 4.758790915286707e-05, 'epoch': 0.05}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0682, 'learning_rate': 4.266922141567486e-05, 'epoch': 0.15}
{'loss': 0.068, 'learning_rate': 4.429050444010465e-05, 'epoch': 0.11}
{'loss': 0.1119, 'learning_rate': 4.7219510398054865e-05, 'epoch': 0.06}
{'loss': 0.1168, 'learning_rate': 4.699847114516753e-05, 'epoch': 0.06}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0855, 'learning_rate': 2.1530806560599544e-05, 'epoch': 0.57}
{'loss': 0.0553, 'learning_rate': 1.419006081454635e-05, 'epoch': 0.72}
{'loss': 0.0678, 'learning_rate': 4.23560190132282e-05, 'epoch': 0.15}
{'loss': 0.0821, 'learning_rate': 2.8225063561664028e-05, 'epoch': 0.44}
{'loss': 0.0914, 'learning_rate': 2.6690828674980036e-05, 'epoch': 0.47}
{'loss': 0.0947, 'learning_rate': 1.5910068186006512e-05, 'epoch': 0.68}
{'loss': 0.0388, 'learning_rate': 3.55945318545267e-05, 'epoch': 0.29}
{'loss': 0.096, 'learning_rate': 4.896940448341285e-05, 'epoch': 0.02}
{'loss': 0.1713, 'learning_rate': 2.8077674195806776e-05, 'epoch': 0.44}
{'loss': 0.1152, 'learning_rate': 4.7514229401904624e-05, 'epoch': 0.05}
{'loss': 0.6706, 'learning_rate': 4.9927241245924596e-05, 'epoch': 0.0}
{'loss': 0.1243, 'learning_rate': 4.25218320498176e-05, 'epoch': 0.15}
{'loss': 0.0828, 'learning_rate': 4.692479139420509e-05, 'epoch': 0.06}
{'loss': 0.1102, 'learning_rate': 4.714583064709242e-05, 'epoch': 0.06}
{'loss': 0.0932, 'learning_rate': 4.414311507424739e-05, 'epoch': 0.12}
{'loss': 0.0631, 'learning_rate': 2.1285091221819524e-05, 'epoch': 0.57}
{'loss': 0.0811, 'learning_rate': 1.3944345475766324e-05, 'epoch': 0.72}
{'loss': 0.1137, 'learning_rate': 2.793028482994952e-05, 'epoch': 0.44}
{'loss': 0.0254, 'learning_rate': 4.2208629647370944e-05, 'epoch': 0.16}
{'loss': 0.0681, 'learning_rate': 2.6445113336200012e-05, 'epoch': 0.47}
{'loss': 0.0827, 'learning_rate': 1.5664352847226488e-05, 'epoch': 0.69}
{'loss': 0.0765, 'learning_rate': 3.5447142488669445e-05, 'epoch': 0.29}
{'loss': 0.1221, 'learning_rate': 4.8904934701320715e-05, 'epoch': 0.02}
{'loss': 0.0481, 'learning_rate': 2.7782895464092267e-05, 'epoch': 0.44}
{'loss': 0.0843, 'learning_rate': 4.7440549650942186e-05, 'epoch': 0.05}
{'loss': 0.3389, 'learning_rate': 4.985356149496215e-05, 'epoch': 0.0}
{'loss': 0.0819, 'learning_rate': 4.2374442683960355e-05, 'epoch': 0.15}
{'loss': 0.0864, 'learning_rate': 4.6851111643242654e-05, 'epoch': 0.06}
{'loss': 0.071, 'learning_rate': 4.7072150896129975e-05, 'epoch': 0.06}
{'loss': 0.0752, 'learning_rate': 4.399572570839014e-05, 'epoch': 0.12}
{'loss': 0.1009, 'learning_rate': 2.10393758830395e-05, 'epoch': 0.58}
{'loss': 0.0786, 'learning_rate': 1.3698630136986302e-05, 'epoch': 0.73}
{'loss': 0.0276, 'learning_rate': 2.7635506098235015e-05, 'epoch': 0.45}
{'loss': 0.0715, 'learning_rate': 4.206124028151369e-05, 'epoch': 0.16}
{'loss': 0.0876, 'learning_rate': 1.5418637508446464e-05, 'epoch': 0.69}
{'loss': 0.0849, 'learning_rate': 2.6199397997419988e-05, 'epoch': 0.48}
{'loss': 0.0769, 'learning_rate': 3.529975312281219e-05, 'epoch': 0.29}
{'loss': 0.1816, 'learning_rate': 4.883125495035828e-05, 'epoch': 0.02}
{'loss': 0.1293, 'learning_rate': 2.748811673237776e-05, 'epoch': 0.45}
{'loss': 0.2887, 'learning_rate': 4.978909171287002e-05, 'epoch': 0.0}
{'loss': 0.1039, 'learning_rate': 4.736686989997974e-05, 'epoch': 0.05}
{'loss': 0.0654, 'learning_rate': 4.677743189228021e-05, 'epoch': 0.06}
{'loss': 0.0804, 'learning_rate': 4.22270533181031e-05, 'epoch': 0.16}
{'loss': 0.1129, 'learning_rate': 4.699847114516753e-05, 'epoch': 0.06}
{'loss': 0.0934, 'learning_rate': 4.3848336342532884e-05, 'epoch': 0.12}
{'loss': 0.0521, 'learning_rate': 1.3452914798206278e-05, 'epoch': 0.73}
{'loss': 0.0645, 'learning_rate': 2.0793660544259473e-05, 'epoch': 0.58}
{'loss': 0.0474, 'learning_rate': 2.734072736652051e-05, 'epoch': 0.45}
{'loss': 0.1142, 'learning_rate': 4.191385091565644e-05, 'epoch': 0.16}
{'loss': 0.0521, 'learning_rate': 1.5172922169666442e-05, 'epoch': 0.7}
{'loss': 0.0933, 'learning_rate': 3.5152363756954935e-05, 'epoch': 0.3}
{'loss': 0.0915, 'learning_rate': 2.5953682658639965e-05, 'epoch': 0.48}
{'loss': 0.1257, 'learning_rate': 4.875757519939583e-05, 'epoch': 0.03}
{'loss': 0.342, 'learning_rate': 4.972462193077788e-05, 'epoch': 0.01}
{'loss': 0.08, 'learning_rate': 2.7193338000663254e-05, 'epoch': 0.46}
{'loss': 0.1313, 'learning_rate': 4.72931901490173e-05, 'epoch': 0.05}
{'loss': 0.0601, 'learning_rate': 4.670375214131777e-05, 'epoch': 0.07}
{'loss': 0.0921, 'learning_rate': 4.2079663952245846e-05, 'epoch': 0.16}
{'loss': 0.1128, 'learning_rate': 4.692479139420509e-05, 'epoch': 0.06}
{'loss': 0.0975, 'learning_rate': 4.3700946976675635e-05, 'epoch': 0.13}
{'loss': 0.0343, 'learning_rate': 2.0547945205479453e-05, 'epoch': 0.59}
{'loss': 0.0657, 'learning_rate': 1.3207199459426253e-05, 'epoch': 0.74}
{'loss': 0.0675, 'learning_rate': 2.7045948634806002e-05, 'epoch': 0.46}
{'loss': 0.0984, 'learning_rate': 4.1766461549799187e-05, 'epoch': 0.17}
{'loss': 0.0453, 'learning_rate': 1.4927206830886417e-05, 'epoch': 0.7}
{'loss': 0.0638, 'learning_rate': 3.500497439109769e-05, 'epoch': 0.3}
{'loss': 0.1098, 'learning_rate': 4.8683895448433394e-05, 'epoch': 0.03}
{'loss': 0.0901, 'learning_rate': 2.5707967319859944e-05, 'epoch': 0.49}
{'loss': 0.1705, 'learning_rate': 4.965094217981544e-05, 'epoch': 0.01}
{'loss': 0.0541, 'learning_rate': 2.6898559268948748e-05, 'epoch': 0.46}
{'loss': 0.1262, 'learning_rate': 4.6630072390355326e-05, 'epoch': 0.07}
{'loss': 0.1112, 'learning_rate': 4.7219510398054865e-05, 'epoch': 0.06}
{'loss': 0.1154, 'learning_rate': 4.193227458638859e-05, 'epoch': 0.16}
{'loss': 0.1146, 'learning_rate': 4.6851111643242654e-05, 'epoch': 0.06}
{'loss': 0.0658, 'learning_rate': 4.355355761081838e-05, 'epoch': 0.13}
{'loss': 0.0988, 'learning_rate': 2.030222986669943e-05, 'epoch': 0.59}
{'loss': 0.0808, 'learning_rate': 1.296148412064623e-05, 'epoch': 0.74}
{'loss': 0.1157, 'learning_rate': 2.6751169903091493e-05, 'epoch': 0.47}
{'loss': 0.1003, 'learning_rate': 4.161907218394193e-05, 'epoch': 0.17}
{'loss': 0.0218, 'learning_rate': 1.4681491492106395e-05, 'epoch': 0.71}
{'loss': 0.0573, 'learning_rate': 3.485758502524043e-05, 'epoch': 0.3}
{'loss': 0.1059, 'learning_rate': 2.546225198107992e-05, 'epoch': 0.49}
{'loss': 0.109, 'learning_rate': 4.861021569747094e-05, 'epoch': 0.03}
{'loss': 0.1658, 'learning_rate': 4.957726242885299e-05, 'epoch': 0.01}
{'loss': 0.0937, 'learning_rate': 4.655639263939288e-05, 'epoch': 0.07}
{'loss': 0.0923, 'learning_rate': 2.660378053723424e-05, 'epoch': 0.47}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1565, 'learning_rate': 4.714583064709242e-05, 'epoch': 0.06}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0888, 'learning_rate': 4.1784885220531336e-05, 'epoch': 0.17}
{'loss': 0.082, 'learning_rate': 4.677743189228021e-05, 'epoch': 0.06}
{'loss': 0.1064, 'learning_rate': 4.3406168244961126e-05, 'epoch': 0.13}
{'loss': 0.0601, 'learning_rate': 1.2715768781866209e-05, 'epoch': 0.75}
{'loss': 0.066, 'learning_rate': 2.0056514527919405e-05, 'epoch': 0.6}
{'loss': 0.038, 'learning_rate': 2.6456391171376986e-05, 'epoch': 0.47}
{'loss': 0.0536, 'learning_rate': 4.147168281808468e-05, 'epoch': 0.17}
{'loss': 0.0684, 'learning_rate': 1.4435776153326371e-05, 'epoch': 0.71}
{'loss': 0.0744, 'learning_rate': 3.471019565938318e-05, 'epoch': 0.31}
{'loss': 0.1446, 'learning_rate': 4.950358267789055e-05, 'epoch': 0.01}
{'loss': 0.1077, 'learning_rate': 4.8536535946508504e-05, 'epoch': 0.03}
{'loss': 0.1332, 'learning_rate': 4.648271288843044e-05, 'epoch': 0.07}
{'loss': 0.0766, 'learning_rate': 2.5216536642299893e-05, 'epoch': 0.5}
{'loss': 0.076, 'learning_rate': 2.6309001805519735e-05, 'epoch': 0.47}
{'loss': 0.0886, 'learning_rate': 4.7072150896129975e-05, 'epoch': 0.06}
{'loss': 0.0612, 'learning_rate': 4.163749585467409e-05, 'epoch': 0.17}
{'loss': 0.0774, 'learning_rate': 4.670375214131777e-05, 'epoch': 0.07}
{'loss': 0.0545, 'learning_rate': 4.325877887910387e-05, 'epoch': 0.14}
{'loss': 0.0489, 'learning_rate': 1.2470053443086185e-05, 'epoch': 0.75}
{'loss': 0.0517, 'learning_rate': 1.981079918913938e-05, 'epoch': 0.6}
{'loss': 0.0575, 'learning_rate': 2.616161243966248e-05, 'epoch': 0.48}
{'loss': 0.1224, 'learning_rate': 4.132429345222742e-05, 'epoch': 0.17}
{'loss': 0.0317, 'learning_rate': 1.419006081454635e-05, 'epoch': 0.72}
{'loss': 0.2239, 'learning_rate': 4.9429902926928115e-05, 'epoch': 0.01}
{'loss': 0.1159, 'learning_rate': 3.456280629352592e-05, 'epoch': 0.31}
{'loss': 0.0932, 'learning_rate': 4.6409033137468e-05, 'epoch': 0.07}
{'loss': 0.0777, 'learning_rate': 4.8462856195546066e-05, 'epoch': 0.03}
{'loss': 0.0488, 'learning_rate': 2.4970821303519873e-05, 'epoch': 0.5}
{'loss': 0.0685, 'learning_rate': 2.601422307380523e-05, 'epoch': 0.48}
{'loss': 0.0755, 'learning_rate': 4.699847114516753e-05, 'epoch': 0.06}
{'loss': 0.1095, 'learning_rate': 4.149010648881683e-05, 'epoch': 0.17}
{'loss': 0.0808, 'learning_rate': 4.6630072390355326e-05, 'epoch': 0.07}
{'loss': 0.0876, 'learning_rate': 4.311138951324662e-05, 'epoch': 0.14}
{'loss': 0.0554, 'learning_rate': 1.2224338104306161e-05, 'epoch': 0.76}
{'loss': 0.0521, 'learning_rate': 2.5866833707947974e-05, 'epoch': 0.48}
{'loss': 0.0802, 'learning_rate': 1.9565083850359358e-05, 'epoch': 0.61}
{'loss': 0.0733, 'learning_rate': 4.1176904086370174e-05, 'epoch': 0.18}
{'loss': 0.1227, 'learning_rate': 4.935622317596567e-05, 'epoch': 0.01}
{'loss': 0.049, 'learning_rate': 1.3944345475766324e-05, 'epoch': 0.72}
{'loss': 0.1338, 'learning_rate': 4.633535338650556e-05, 'epoch': 0.07}
{'loss': 0.0806, 'learning_rate': 3.441541692766867e-05, 'epoch': 0.31}
{'loss': 0.1211, 'learning_rate': 4.838917644458362e-05, 'epoch': 0.03}
{'loss': 0.0602, 'learning_rate': 2.4725105964739846e-05, 'epoch': 0.51}
{'loss': 0.0358, 'learning_rate': 2.5719444342090722e-05, 'epoch': 0.49}
{'loss': 0.0686, 'learning_rate': 4.692479139420509e-05, 'epoch': 0.06}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0431, 'learning_rate': 1.1978622765526137e-05, 'epoch': 0.76}
{'loss': 0.1364, 'learning_rate': 4.134271712295958e-05, 'epoch': 0.17}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1199, 'learning_rate': 4.655639263939288e-05, 'epoch': 0.07}
{'loss': 0.1096, 'learning_rate': 4.296400014738937e-05, 'epoch': 0.14}
{'loss': 0.1342, 'learning_rate': 4.928254342500323e-05, 'epoch': 0.01}
{'loss': 0.0614, 'learning_rate': 2.5572054976233467e-05, 'epoch': 0.49}
{'loss': 0.06, 'learning_rate': 1.9319368511579337e-05, 'epoch': 0.61}
{'loss': 0.0983, 'learning_rate': 4.102951472051292e-05, 'epoch': 0.18}
{'loss': 0.0655, 'learning_rate': 1.3698630136986302e-05, 'epoch': 0.73}
{'loss': 0.2355, 'learning_rate': 4.6261673635543116e-05, 'epoch': 0.08}
{'loss': 0.0939, 'learning_rate': 3.426802756181142e-05, 'epoch': 0.32}
{'loss': 0.1019, 'learning_rate': 4.831549669362118e-05, 'epoch': 0.03}
{'loss': 0.0513, 'learning_rate': 2.4479390625959826e-05, 'epoch': 0.51}
{'loss': 0.0998, 'learning_rate': 2.5424665610376212e-05, 'epoch': 0.49}
{'loss': 0.0874, 'learning_rate': 4.6851111643242654e-05, 'epoch': 0.06}
{'loss': 0.0998, 'learning_rate': 1.1732907426746115e-05, 'epoch': 0.77}
{'loss': 0.1093, 'learning_rate': 4.119532775710232e-05, 'epoch': 0.18}
{'loss': 0.0759, 'learning_rate': 4.648271288843044e-05, 'epoch': 0.07}
{'loss': 0.1153, 'learning_rate': 4.281661078153211e-05, 'epoch': 0.14}
{'loss': 0.191, 'learning_rate': 4.920886367404079e-05, 'epoch': 0.02}
{'loss': 0.1444, 'learning_rate': 4.618799388458068e-05, 'epoch': 0.08}
{'loss': 0.0718, 'learning_rate': 2.527727624451896e-05, 'epoch': 0.5}
{'loss': 0.0723, 'learning_rate': 4.0882125354655664e-05, 'epoch': 0.18}
{'loss': 0.0741, 'learning_rate': 1.3452914798206278e-05, 'epoch': 0.73}
{'loss': 0.0704, 'learning_rate': 1.907365317279931e-05, 'epoch': 0.62}
{'loss': 0.0765, 'learning_rate': 3.4120638195954164e-05, 'epoch': 0.32}
{'loss': 0.1359, 'learning_rate': 4.824181694265874e-05, 'epoch': 0.04}
{'loss': 0.0471, 'learning_rate': 2.4233675287179802e-05, 'epoch': 0.52}
{'loss': 0.0567, 'learning_rate': 2.5129886878661706e-05, 'epoch': 0.5}
{'loss': 0.0702, 'learning_rate': 4.677743189228021e-05, 'epoch': 0.06}
{'loss': 0.0694, 'learning_rate': 1.1487192087966092e-05, 'epoch': 0.77}
{'loss': 0.0897, 'learning_rate': 4.1047938391245075e-05, 'epoch': 0.18}
{'loss': 0.1088, 'learning_rate': 4.6409033137468e-05, 'epoch': 0.07}
{'loss': 0.1017, 'learning_rate': 4.266922141567486e-05, 'epoch': 0.15}
{'loss': 0.099, 'learning_rate': 4.913518392307835e-05, 'epoch': 0.02}
{'loss': 0.1188, 'learning_rate': 4.611431413361823e-05, 'epoch': 0.08}
{'loss': 0.0839, 'learning_rate': 2.498249751280445e-05, 'epoch': 0.5}
{'loss': 0.0625, 'learning_rate': 1.3207199459426253e-05, 'epoch': 0.74}
{'loss': 0.1237, 'learning_rate': 4.073473598879841e-05, 'epoch': 0.19}
{'loss': 0.064, 'learning_rate': 1.8827937834019287e-05, 'epoch': 0.62}
{'loss': 0.1172, 'learning_rate': 3.397324883009691e-05, 'epoch': 0.32}
{'loss': 0.1293, 'learning_rate': 4.81681371916963e-05, 'epoch': 0.04}
{'loss': 0.083, 'learning_rate': 2.3987959948399778e-05, 'epoch': 0.52}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0793, 'learning_rate': 2.4835108146947196e-05, 'epoch': 0.5}
{'loss': 0.0945, 'learning_rate': 4.670375214131777e-05, 'epoch': 0.07}
{'loss': 0.0774, 'learning_rate': 1.1241476749186068e-05, 'epoch': 0.78}
{'loss': 0.0951, 'learning_rate': 4.090054902538782e-05, 'epoch': 0.18}
{'loss': 0.0713, 'learning_rate': 4.9061504172115904e-05, 'epoch': 0.02}
{'loss': 0.1215, 'learning_rate': 4.633535338650556e-05, 'epoch': 0.07}
{'loss': 0.0696, 'learning_rate': 4.25218320498176e-05, 'epoch': 0.15}
{'loss': 0.083, 'learning_rate': 4.604063438265579e-05, 'epoch': 0.08}
{'loss': 0.0606, 'learning_rate': 2.4687718781089945e-05, 'epoch': 0.51}
{'loss': 0.0142, 'learning_rate': 1.296148412064623e-05, 'epoch': 0.74}
{'loss': 0.1598, 'learning_rate': 4.058734662294116e-05, 'epoch': 0.19}
{'loss': 0.0325, 'learning_rate': 1.8582222495239266e-05, 'epoch': 0.63}
{'loss': 0.0646, 'learning_rate': 3.3825859464239655e-05, 'epoch': 0.32}
{'loss': 0.0681, 'learning_rate': 2.3742244609619754e-05, 'epoch': 0.53}
{'loss': 0.0567, 'learning_rate': 4.8094457440733856e-05, 'epoch': 0.04}
{'loss': 0.0577, 'learning_rate': 2.454032941523269e-05, 'epoch': 0.51}
{'loss': 0.0478, 'learning_rate': 1.0995761410406044e-05, 'epoch': 0.78}
{'loss': 0.1103, 'learning_rate': 4.6630072390355326e-05, 'epoch': 0.07}
{'loss': 0.1439, 'learning_rate': 4.898782442115346e-05, 'epoch': 0.02}
{'loss': 0.1046, 'learning_rate': 4.0753159659530565e-05, 'epoch': 0.19}
{'loss': 0.0675, 'learning_rate': 4.596695463169335e-05, 'epoch': 0.08}
{'loss': 0.0904, 'learning_rate': 4.6261673635543116e-05, 'epoch': 0.08}
{'loss': 0.0868, 'learning_rate': 4.2374442683960355e-05, 'epoch': 0.15}
{'loss': 0.0514, 'learning_rate': 2.439294004937544e-05, 'epoch': 0.51}
{'loss': 0.1257, 'learning_rate': 4.0439957257083906e-05, 'epoch': 0.19}
{'loss': 0.0747, 'learning_rate': 1.2715768781866209e-05, 'epoch': 0.75}
{'loss': 0.0141, 'learning_rate': 1.8336507156459242e-05, 'epoch': 0.63}
{'loss': 0.0801, 'learning_rate': 3.36784700983824e-05, 'epoch': 0.33}
{'loss': 0.0822, 'learning_rate': 2.349652927083973e-05, 'epoch': 0.53}
{'loss': 0.0894, 'learning_rate': 4.802077768977141e-05, 'epoch': 0.04}
{'loss': 0.0544, 'learning_rate': 1.0750046071626022e-05, 'epoch': 0.79}
{'loss': 0.0449, 'learning_rate': 2.4245550683518184e-05, 'epoch': 0.52}
{'loss': 0.0884, 'learning_rate': 4.655639263939288e-05, 'epoch': 0.07}
{'loss': 0.076, 'learning_rate': 4.891414467019102e-05, 'epoch': 0.02}
{'loss': 0.0787, 'learning_rate': 4.5893274880730905e-05, 'epoch': 0.08}
{'loss': 0.0671, 'learning_rate': 4.060577029367331e-05, 'epoch': 0.19}
{'loss': 0.0808, 'learning_rate': 4.618799388458068e-05, 'epoch': 0.08}
{'loss': 0.0843, 'learning_rate': 4.22270533181031e-05, 'epoch': 0.16}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0503, 'learning_rate': 2.409816131766093e-05, 'epoch': 0.52}
{'loss': 0.1252, 'learning_rate': 4.029256789122665e-05, 'epoch': 0.19}
{'loss': 0.0271, 'learning_rate': 1.2470053443086185e-05, 'epoch': 0.75}
{'loss': 0.0493, 'learning_rate': 1.809079181767922e-05, 'epoch': 0.64}
{'loss': 0.0753, 'learning_rate': 3.353108073252515e-05, 'epoch': 0.33}
{'loss': 0.173, 'learning_rate': 4.794709793880897e-05, 'epoch': 0.04}
{'loss': 0.0632, 'learning_rate': 2.325081393205971e-05, 'epoch': 0.54}
{'loss': 0.0831, 'learning_rate': 1.0504330732845998e-05, 'epoch': 0.79}
{'loss': 0.0303, 'learning_rate': 2.3950771951803677e-05, 'epoch': 0.52}
{'loss': 0.1235, 'learning_rate': 4.8840464919228577e-05, 'epoch': 0.02}
{'loss': 0.1788, 'learning_rate': 4.581959512976847e-05, 'epoch': 0.08}
{'loss': 0.0825, 'learning_rate': 4.648271288843044e-05, 'epoch': 0.07}
{'loss': 0.068, 'learning_rate': 4.611431413361823e-05, 'epoch': 0.08}
{'loss': 0.1232, 'learning_rate': 4.0458380927816056e-05, 'epoch': 0.19}
{'loss': 0.1353, 'learning_rate': 4.2079663952245846e-05, 'epoch': 0.16}
{'loss': 0.0275, 'learning_rate': 2.3803382585946423e-05, 'epoch': 0.52}
{'loss': 0.0701, 'learning_rate': 1.2224338104306161e-05, 'epoch': 0.76}
{'loss': 0.0627, 'learning_rate': 4.01451785253694e-05, 'epoch': 0.2}
{'loss': 0.0996, 'learning_rate': 1.7845076478899195e-05, 'epoch': 0.64}
{'loss': 0.0737, 'learning_rate': 3.33836913666679e-05, 'epoch': 0.33}
{'loss': 0.0223, 'learning_rate': 1.0258615394065975e-05, 'epoch': 0.8}
{'loss': 0.1408, 'learning_rate': 4.787341818784653e-05, 'epoch': 0.04}
{'loss': 0.0624, 'learning_rate': 2.3005098593279687e-05, 'epoch': 0.54}
{'loss': 0.1124, 'learning_rate': 4.876678516826614e-05, 'epoch': 0.03}
{'loss': 0.0528, 'learning_rate': 4.574591537880603e-05, 'epoch': 0.09}
{'loss': 0.0754, 'learning_rate': 2.365599322008917e-05, 'epoch': 0.53}
{'loss': 0.0646, 'learning_rate': 4.6409033137468e-05, 'epoch': 0.07}
{'loss': 0.0972, 'learning_rate': 4.193227458638859e-05, 'epoch': 0.16}
{'loss': 0.0442, 'learning_rate': 4.031099156195881e-05, 'epoch': 0.19}
{'loss': 0.0581, 'learning_rate': 4.604063438265579e-05, 'epoch': 0.08}
{'loss': 0.1339, 'learning_rate': 2.3508603854231916e-05, 'epoch': 0.53}
{'loss': 0.0301, 'learning_rate': 1.1978622765526137e-05, 'epoch': 0.76}
{'loss': 0.0928, 'learning_rate': 3.999778915951214e-05, 'epoch': 0.2}
{'loss': 0.0678, 'learning_rate': 1.759936114011917e-05, 'epoch': 0.65}
{'loss': 0.0997, 'learning_rate': 3.323630200081064e-05, 'epoch': 0.34}
{'loss': 0.0705, 'learning_rate': 1.0012900055285951e-05, 'epoch': 0.8}
{'loss': 0.0844, 'learning_rate': 4.779973843688409e-05, 'epoch': 0.04}
{'loss': 0.1057, 'learning_rate': 4.5672235627843584e-05, 'epoch': 0.09}
{'loss': 0.066, 'learning_rate': 4.86931054173037e-05, 'epoch': 0.03}
{'loss': 0.067, 'learning_rate': 2.275938325449966e-05, 'epoch': 0.55}
{'loss': 0.076, 'learning_rate': 2.3361214488374665e-05, 'epoch': 0.53}
{'loss': 0.1215, 'learning_rate': 4.633535338650556e-05, 'epoch': 0.07}
{'loss': 0.0972, 'learning_rate': 4.1784885220531336e-05, 'epoch': 0.17}
{'loss': 0.0846, 'learning_rate': 4.596695463169335e-05, 'epoch': 0.08}
{'loss': 0.0919, 'learning_rate': 4.016360219610155e-05, 'epoch': 0.2}
{'loss': 0.0632, 'learning_rate': 2.321382512251741e-05, 'epoch': 0.54}
{'loss': 0.0277, 'learning_rate': 1.1732907426746115e-05, 'epoch': 0.77}
{'loss': 0.0984, 'learning_rate': 3.985039979365489e-05, 'epoch': 0.2}
{'loss': 0.0771, 'learning_rate': 1.735364580133915e-05, 'epoch': 0.65}
{'loss': 0.089, 'learning_rate': 3.308891263495339e-05, 'epoch': 0.34}
{'loss': 0.0348, 'learning_rate': 9.767184716505929e-06, 'epoch': 0.81}
{'loss': 0.1914, 'learning_rate': 4.5598555876881146e-05, 'epoch': 0.09}
{'loss': 0.1212, 'learning_rate': 4.772605868592165e-05, 'epoch': 0.05}
{'loss': 0.0945, 'learning_rate': 4.861942566634125e-05, 'epoch': 0.03}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.084, 'learning_rate': 2.306643575666016e-05, 'epoch': 0.54}
{'loss': 0.0648, 'learning_rate': 2.251366791571964e-05, 'epoch': 0.55}
{'loss': 0.0717, 'learning_rate': 4.6261673635543116e-05, 'epoch': 0.08}
{'loss': 0.0678, 'learning_rate': 4.163749585467409e-05, 'epoch': 0.17}
{'loss': 0.1238, 'learning_rate': 4.5893274880730905e-05, 'epoch': 0.08}
{'loss': 0.0797, 'learning_rate': 4.00162128302443e-05, 'epoch': 0.2}
{'loss': 0.0617, 'learning_rate': 2.2919046390802904e-05, 'epoch': 0.54}
{'loss': 0.0417, 'learning_rate': 1.1487192087966092e-05, 'epoch': 0.77}
{'loss': 0.0741, 'learning_rate': 3.970301042779763e-05, 'epoch': 0.21}
{'loss': 0.0964, 'learning_rate': 1.7107930462559124e-05, 'epoch': 0.66}
{'loss': 0.0706, 'learning_rate': 4.5524876125918694e-05, 'epoch': 0.09}
{'loss': 0.0962, 'learning_rate': 3.294152326909614e-05, 'epoch': 0.34}
{'loss': 0.1002, 'learning_rate': 4.854574591537881e-05, 'epoch': 0.03}
{'loss': 0.0762, 'learning_rate': 9.521469377725903e-06, 'epoch': 0.81}
{'loss': 0.1058, 'learning_rate': 4.765237893495921e-05, 'epoch': 0.05}
{'loss': 0.0505, 'learning_rate': 2.277165702494565e-05, 'epoch': 0.55}
{'loss': 0.0404, 'learning_rate': 2.2267952576939615e-05, 'epoch': 0.55}
{'loss': 0.1042, 'learning_rate': 4.618799388458068e-05, 'epoch': 0.08}
{'loss': 0.1055, 'learning_rate': 4.581959512976847e-05, 'epoch': 0.08}
{'loss': 0.0867, 'learning_rate': 4.149010648881683e-05, 'epoch': 0.17}
{'loss': 0.0851, 'learning_rate': 3.986882346438704e-05, 'epoch': 0.2}
{'loss': 0.053, 'learning_rate': 2.2624267659088397e-05, 'epoch': 0.55}
{'loss': 0.0794, 'learning_rate': 1.1241476749186068e-05, 'epoch': 0.78}
{'loss': 0.0757, 'learning_rate': 3.955562106194038e-05, 'epoch': 0.21}
{'loss': 0.1169, 'learning_rate': 4.5451196374956256e-05, 'epoch': 0.09}
{'loss': 0.0337, 'learning_rate': 9.275754038945881e-06, 'epoch': 0.82}
{'loss': 0.0549, 'learning_rate': 1.68622151237791e-05, 'epoch': 0.66}
{'loss': 0.1118, 'learning_rate': 4.8472066164416366e-05, 'epoch': 0.03}
{'loss': 0.0856, 'learning_rate': 3.2794133903238884e-05, 'epoch': 0.34}
{'loss': 0.1135, 'learning_rate': 4.757869918399676e-05, 'epoch': 0.05}
{'loss': 0.0742, 'learning_rate': 2.2476878293231142e-05, 'epoch': 0.55}
{'loss': 0.0717, 'learning_rate': 2.202223723815959e-05, 'epoch': 0.56}
{'loss': 0.1072, 'learning_rate': 4.574591537880603e-05, 'epoch': 0.09}
{'loss': 0.0866, 'learning_rate': 4.611431413361823e-05, 'epoch': 0.08}
{'loss': 0.0719, 'learning_rate': 4.134271712295958e-05, 'epoch': 0.17}
{'loss': 0.0675, 'learning_rate': 3.9721434098529795e-05, 'epoch': 0.21}
{'loss': 0.0576, 'learning_rate': 2.232948892737389e-05, 'epoch': 0.55}
{'loss': 0.1095, 'learning_rate': 4.537751662399382e-05, 'epoch': 0.09}
{'loss': 0.0725, 'learning_rate': 1.0995761410406044e-05, 'epoch': 0.78}
{'loss': 0.1156, 'learning_rate': 3.940823169608312e-05, 'epoch': 0.21}
{'loss': 0.235, 'learning_rate': 4.839838641345393e-05, 'epoch': 0.03}
{'loss': 0.0556, 'learning_rate': 9.030038700165858e-06, 'epoch': 0.82}
{'loss': 0.0408, 'learning_rate': 1.661649978499908e-05, 'epoch': 0.67}
{'loss': 0.068, 'learning_rate': 3.264674453738163e-05, 'epoch': 0.35}
{'loss': 0.0661, 'learning_rate': 4.750501943303432e-05, 'epoch': 0.05}
{'loss': 0.0622, 'learning_rate': 2.2182099561516636e-05, 'epoch': 0.56}
{'loss': 0.0589, 'learning_rate': 4.5672235627843584e-05, 'epoch': 0.09}
{'loss': 0.042, 'learning_rate': 2.1776521899379568e-05, 'epoch': 0.56}
{'loss': 0.0983, 'learning_rate': 4.119532775710232e-05, 'epoch': 0.18}
{'loss': 0.0563, 'learning_rate': 4.604063438265579e-05, 'epoch': 0.08}
{'loss': 0.1041, 'learning_rate': 3.957404473267254e-05, 'epoch': 0.21}
{'loss': 0.0988, 'learning_rate': 4.530383687303137e-05, 'epoch': 0.09}
{'loss': 0.0637, 'learning_rate': 2.2034710195659385e-05, 'epoch': 0.56}
{'loss': 0.1027, 'learning_rate': 4.832470666249149e-05, 'epoch': 0.03}
{'loss': 0.0489, 'learning_rate': 8.784323361385836e-06, 'epoch': 0.83}
{'loss': 0.0676, 'learning_rate': 1.0750046071626022e-05, 'epoch': 0.79}
{'loss': 0.1009, 'learning_rate': 3.9260842330225874e-05, 'epoch': 0.22}
{'loss': 0.0199, 'learning_rate': 1.6370784446219056e-05, 'epoch': 0.67}
{'loss': 0.0611, 'learning_rate': 3.2499355171524375e-05, 'epoch': 0.35}
{'loss': 0.1437, 'learning_rate': 4.743133968207188e-05, 'epoch': 0.05}
{'loss': 0.061, 'learning_rate': 2.188732082980213e-05, 'epoch': 0.56}
{'loss': 0.0919, 'learning_rate': 4.5598555876881146e-05, 'epoch': 0.09}
{'loss': 0.0726, 'learning_rate': 4.1047938391245075e-05, 'epoch': 0.18}
{'loss': 0.086, 'learning_rate': 4.596695463169335e-05, 'epoch': 0.08}
{'loss': 0.0769, 'learning_rate': 2.1530806560599544e-05, 'epoch': 0.57}
{'loss': 0.0559, 'learning_rate': 3.9426655366815285e-05, 'epoch': 0.21}
{'loss': 0.0623, 'learning_rate': 4.5230157122068935e-05, 'epoch': 0.1}
{'loss': 0.1525, 'learning_rate': 4.8251026911529045e-05, 'epoch': 0.04}
{'loss': 0.0606, 'learning_rate': 2.1739931463944875e-05, 'epoch': 0.57}
{'loss': 0.0631, 'learning_rate': 1.0504330732845998e-05, 'epoch': 0.79}
{'loss': 0.0201, 'learning_rate': 8.53860802260581e-06, 'epoch': 0.83}
{'loss': 0.0871, 'learning_rate': 3.911345296436862e-05, 'epoch': 0.22}
{'loss': 0.0907, 'learning_rate': 1.6125069107439032e-05, 'epoch': 0.68}
{'loss': 0.0837, 'learning_rate': 3.235196580566712e-05, 'epoch': 0.35}
{'loss': 0.0608, 'learning_rate': 4.735765993110944e-05, 'epoch': 0.05}
{'loss': 0.0398, 'learning_rate': 2.1592542098087623e-05, 'epoch': 0.57}
{'loss': 0.0879, 'learning_rate': 4.5524876125918694e-05, 'epoch': 0.09}
{'loss': 0.0845, 'learning_rate': 4.515647737110649e-05, 'epoch': 0.1}
{'loss': 0.085, 'learning_rate': 4.090054902538782e-05, 'epoch': 0.18}
{'loss': 0.0652, 'learning_rate': 4.5893274880730905e-05, 'epoch': 0.08}
{'loss': 0.0673, 'learning_rate': 3.927926600095803e-05, 'epoch': 0.22}
{'loss': 0.0744, 'learning_rate': 4.817734716056661e-05, 'epoch': 0.04}
{'loss': 0.0848, 'learning_rate': 2.1285091221819524e-05, 'epoch': 0.57}
{'loss': 0.0737, 'learning_rate': 2.144515273223037e-05, 'epoch': 0.57}
{'loss': 0.0236, 'learning_rate': 8.292892683825788e-06, 'epoch': 0.83}
{'loss': 0.0896, 'learning_rate': 3.8966063598511365e-05, 'epoch': 0.22}
{'loss': 0.063, 'learning_rate': 1.0258615394065975e-05, 'epoch': 0.8}
{'loss': 0.0924, 'learning_rate': 1.587935376865901e-05, 'epoch': 0.68}
{'loss': 0.092, 'learning_rate': 3.220457643980987e-05, 'epoch': 0.36}
{'loss': 0.1145, 'learning_rate': 4.7283980180146996e-05, 'epoch': 0.05}
{'loss': 0.0455, 'learning_rate': 2.1297763366373117e-05, 'epoch': 0.57}
{'loss': 0.1004, 'learning_rate': 4.508279762014405e-05, 'epoch': 0.1}
{'loss': 0.0517, 'learning_rate': 4.5451196374956256e-05, 'epoch': 0.09}
{'loss': 0.0697, 'learning_rate': 4.0753159659530565e-05, 'epoch': 0.19}
{'loss': 0.0689, 'learning_rate': 4.8103667409604155e-05, 'epoch': 0.04}
{'loss': 0.1255, 'learning_rate': 3.9131876635100775e-05, 'epoch': 0.22}
{'loss': 0.1236, 'learning_rate': 4.581959512976847e-05, 'epoch': 0.08}
{'loss': 0.0382, 'learning_rate': 2.10393758830395e-05, 'epoch': 0.58}
{'loss': 0.034, 'learning_rate': 2.1150374000515862e-05, 'epoch': 0.58}
{'loss': 0.053, 'learning_rate': 8.047177345045764e-06, 'epoch': 0.84}
{'loss': 0.0397, 'learning_rate': 1.0012900055285951e-05, 'epoch': 0.8}
{'loss': 0.073, 'learning_rate': 3.881867423265411e-05, 'epoch': 0.22}
{'loss': 0.0596, 'learning_rate': 1.5633638429878985e-05, 'epoch': 0.69}
{'loss': 0.1002, 'learning_rate': 4.721030042918456e-05, 'epoch': 0.06}
{'loss': 0.0946, 'learning_rate': 3.205718707395262e-05, 'epoch': 0.36}
{'loss': 0.0372, 'learning_rate': 2.100298463465861e-05, 'epoch': 0.58}
{'loss': 0.0727, 'learning_rate': 4.500911786918161e-05, 'epoch': 0.1}
{'loss': 0.0911, 'learning_rate': 4.802998765864172e-05, 'epoch': 0.04}
{'loss': 0.0864, 'learning_rate': 4.537751662399382e-05, 'epoch': 0.09}
{'loss': 0.1121, 'learning_rate': 4.060577029367331e-05, 'epoch': 0.19}
{'loss': 0.0724, 'learning_rate': 3.898448726924353e-05, 'epoch': 0.22}
{'loss': 0.0866, 'learning_rate': 4.574591537880603e-05, 'epoch': 0.09}
{'loss': 0.0523, 'learning_rate': 2.0793660544259473e-05, 'epoch': 0.58}
{'loss': 0.0769, 'learning_rate': 7.801462006265742e-06, 'epoch': 0.84}
{'loss': 0.0449, 'learning_rate': 2.0855595268801356e-05, 'epoch': 0.58}
{'loss': 0.0485, 'learning_rate': 9.767184716505929e-06, 'epoch': 0.81}
{'loss': 0.0809, 'learning_rate': 3.867128486679686e-05, 'epoch': 0.23}
{'loss': 0.0588, 'learning_rate': 1.5387923091098964e-05, 'epoch': 0.69}
{'loss': 0.0726, 'learning_rate': 4.713662067822211e-05, 'epoch': 0.06}
{'loss': 0.0411, 'learning_rate': 3.190979770809536e-05, 'epoch': 0.36}
{'loss': 0.1186, 'learning_rate': 2.0708205902944104e-05, 'epoch': 0.59}
{'loss': 0.0719, 'learning_rate': 4.493543811821916e-05, 'epoch': 0.1}
{'loss': 0.077, 'learning_rate': 4.795630790767928e-05, 'epoch': 0.04}
{'loss': 0.1373, 'learning_rate': 4.0458380927816056e-05, 'epoch': 0.19}
{'loss': 0.0852, 'learning_rate': 4.530383687303137e-05, 'epoch': 0.09}
{'loss': 0.0917, 'learning_rate': 3.883709790338627e-05, 'epoch': 0.22}
{'loss': 0.0917, 'learning_rate': 4.5672235627843584e-05, 'epoch': 0.09}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0397, 'learning_rate': 2.056081653708685e-05, 'epoch': 0.59}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0777, 'learning_rate': 7.555746667485718e-06, 'epoch': 0.85}
{'loss': 0.0516, 'learning_rate': 9.521469377725903e-06, 'epoch': 0.81}
{'loss': 0.0428, 'learning_rate': 2.0547945205479453e-05, 'epoch': 0.59}
{'loss': 0.0648, 'learning_rate': 3.852389550093961e-05, 'epoch': 0.23}
{'loss': 0.0686, 'learning_rate': 1.5142207752318937e-05, 'epoch': 0.7}
{'loss': 0.0756, 'learning_rate': 4.706294092725967e-05, 'epoch': 0.06}
{'loss': 0.1221, 'learning_rate': 3.176240834223811e-05, 'epoch': 0.37}
{'loss': 0.057, 'learning_rate': 2.0413427171229595e-05, 'epoch': 0.59}
{'loss': 0.0612, 'learning_rate': 4.4861758367256725e-05, 'epoch': 0.1}
{'loss': 0.103, 'learning_rate': 4.7882628156716834e-05, 'epoch': 0.04}
{'loss': 0.078, 'learning_rate': 4.031099156195881e-05, 'epoch': 0.19}
{'loss': 0.1155, 'learning_rate': 4.5230157122068935e-05, 'epoch': 0.1}
{'loss': 0.1065, 'learning_rate': 4.5598555876881146e-05, 'epoch': 0.09}
{'loss': 0.0712, 'learning_rate': 3.868970853752902e-05, 'epoch': 0.23}
{'loss': 0.0346, 'learning_rate': 2.0266037805372343e-05, 'epoch': 0.6}
{'loss': 0.0735, 'learning_rate': 7.310031328705694e-06, 'epoch': 0.85}
{'loss': 0.0882, 'learning_rate': 2.030222986669943e-05, 'epoch': 0.59}
{'loss': 0.0844, 'learning_rate': 9.275754038945881e-06, 'epoch': 0.82}
{'loss': 0.1109, 'learning_rate': 3.837650613508235e-05, 'epoch': 0.23}
{'loss': 0.0841, 'learning_rate': 1.4896492413538915e-05, 'epoch': 0.7}
{'loss': 0.1109, 'learning_rate': 4.698926117629723e-05, 'epoch': 0.06}
{'loss': 0.1105, 'learning_rate': 3.161501897638086e-05, 'epoch': 0.37}
{'loss': 0.061, 'learning_rate': 2.011864843951509e-05, 'epoch': 0.6}
{'loss': 0.1311, 'learning_rate': 4.478807861629428e-05, 'epoch': 0.1}
{'loss': 0.0974, 'learning_rate': 4.7808948405754396e-05, 'epoch': 0.04}
{'loss': 0.1075, 'learning_rate': 4.016360219610155e-05, 'epoch': 0.2}
{'loss': 0.0693, 'learning_rate': 4.515647737110649e-05, 'epoch': 0.1}
{'loss': 0.0731, 'learning_rate': 4.5524876125918694e-05, 'epoch': 0.09}
{'loss': 0.0745, 'learning_rate': 3.854231917167176e-05, 'epoch': 0.23}
{'loss': 0.0521, 'learning_rate': 1.9971259073657837e-05, 'epoch': 0.6}
{'loss': 0.0846, 'learning_rate': 7.064315989925671e-06, 'epoch': 0.86}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0497, 'learning_rate': 9.030038700165858e-06, 'epoch': 0.82}
{'loss': 0.0845, 'learning_rate': 3.82291167692251e-05, 'epoch': 0.24}
{'loss': 0.1142, 'learning_rate': 2.0056514527919405e-05, 'epoch': 0.6}
{'loss': 0.0731, 'learning_rate': 1.4650777074758892e-05, 'epoch': 0.71}
{'loss': 0.0725, 'learning_rate': 4.6915581425334786e-05, 'epoch': 0.06}
{'loss': 0.0908, 'learning_rate': 3.1467629610523604e-05, 'epoch': 0.37}
{'loss': 0.0816, 'learning_rate': 1.9823869707800582e-05, 'epoch': 0.6}
{'loss': 0.0477, 'learning_rate': 4.471439886533184e-05, 'epoch': 0.11}
{'loss': 0.0894, 'learning_rate': 4.773526865479195e-05, 'epoch': 0.05}
{'loss': 0.0542, 'learning_rate': 4.00162128302443e-05, 'epoch': 0.2}
{'loss': 0.0833, 'learning_rate': 4.508279762014405e-05, 'epoch': 0.1}
{'loss': 0.06, 'learning_rate': 4.5451196374956256e-05, 'epoch': 0.09}
{'loss': 0.1037, 'learning_rate': 3.839492980581451e-05, 'epoch': 0.23}
{'loss': 0.0601, 'learning_rate': 1.967648034194333e-05, 'epoch': 0.61}
{'loss': 0.0367, 'learning_rate': 6.8186006511456475e-06, 'epoch': 0.86}
{'loss': 0.0531, 'learning_rate': 8.784323361385836e-06, 'epoch': 0.83}
{'loss': 0.0795, 'learning_rate': 3.808172740336784e-05, 'epoch': 0.24}
{'loss': 0.0699, 'learning_rate': 1.981079918913938e-05, 'epoch': 0.6}
{'loss': 0.0556, 'learning_rate': 1.4405061735978866e-05, 'epoch': 0.71}
{'loss': 0.0859, 'learning_rate': 4.684190167437235e-05, 'epoch': 0.06}
{'loss': 0.0738, 'learning_rate': 3.132024024466635e-05, 'epoch': 0.37}
{'loss': 0.1242, 'learning_rate': 4.4640719114369404e-05, 'epoch': 0.11}
{'loss': 0.0404, 'learning_rate': 1.9529090976086076e-05, 'epoch': 0.61}
{'loss': 0.0794, 'learning_rate': 4.766158890382951e-05, 'epoch': 0.05}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0749, 'learning_rate': 3.986882346438704e-05, 'epoch': 0.2}
{'loss': 0.0988, 'learning_rate': 4.500911786918161e-05, 'epoch': 0.1}
{'loss': 0.0663, 'learning_rate': 1.938170161022882e-05, 'epoch': 0.61}
{'loss': 0.035, 'learning_rate': 4.537751662399382e-05, 'epoch': 0.09}
{'loss': 0.0794, 'learning_rate': 3.824754043995726e-05, 'epoch': 0.24}
{'loss': 0.0407, 'learning_rate': 6.572885312365625e-06, 'epoch': 0.87}
{'loss': 0.0922, 'learning_rate': 8.53860802260581e-06, 'epoch': 0.83}
{'loss': 0.0682, 'learning_rate': 3.7934338037510594e-05, 'epoch': 0.24}
{'loss': 0.0463, 'learning_rate': 1.4159346397198844e-05, 'epoch': 0.72}
{'loss': 0.1481, 'learning_rate': 4.456703936340696e-05, 'epoch': 0.11}
{'loss': 0.076, 'learning_rate': 4.67682219234099e-05, 'epoch': 0.06}
{'loss': 0.0507, 'learning_rate': 1.9565083850359358e-05, 'epoch': 0.61}
{'loss': 0.0767, 'learning_rate': 3.1172850878809094e-05, 'epoch': 0.38}
{'loss': 0.0653, 'learning_rate': 1.923431224437157e-05, 'epoch': 0.62}
{'loss': 0.1142, 'learning_rate': 4.758790915286707e-05, 'epoch': 0.05}
{'loss': 0.0766, 'learning_rate': 4.493543811821916e-05, 'epoch': 0.1}
{'loss': 0.075, 'learning_rate': 3.9721434098529795e-05, 'epoch': 0.21}
{'loss': 0.0413, 'learning_rate': 1.9086922878514315e-05, 'epoch': 0.62}
{'loss': 0.1316, 'learning_rate': 4.530383687303137e-05, 'epoch': 0.09}
{'loss': 0.1044, 'learning_rate': 3.8100151074100005e-05, 'epoch': 0.24}
{'loss': 0.0789, 'learning_rate': 6.327169973585601e-06, 'epoch': 0.87}
{'loss': 0.0716, 'learning_rate': 8.292892683825788e-06, 'epoch': 0.83}
{'loss': 0.1154, 'learning_rate': 4.4493359612444514e-05, 'epoch': 0.11}
{'loss': 0.0799, 'learning_rate': 3.778694867165334e-05, 'epoch': 0.24}
{'loss': 0.0647, 'learning_rate': 4.6694542172447465e-05, 'epoch': 0.07}
{'loss': 0.0626, 'learning_rate': 1.3913631058418822e-05, 'epoch': 0.72}
{'loss': 0.1051, 'learning_rate': 4.7514229401904624e-05, 'epoch': 0.05}
{'loss': 0.0641, 'learning_rate': 3.102546151295184e-05, 'epoch': 0.38}
{'loss': 0.0689, 'learning_rate': 1.9319368511579337e-05, 'epoch': 0.61}
{'loss': 0.0712, 'learning_rate': 1.8939533512657063e-05, 'epoch': 0.62}
{'loss': 0.0765, 'learning_rate': 4.4861758367256725e-05, 'epoch': 0.1}
{'loss': 0.0834, 'learning_rate': 3.957404473267254e-05, 'epoch': 0.21}
{'loss': 0.0567, 'learning_rate': 1.8792144146799808e-05, 'epoch': 0.62}
{'loss': 0.0893, 'learning_rate': 4.5230157122068935e-05, 'epoch': 0.1}
{'loss': 0.0717, 'learning_rate': 3.795276170824275e-05, 'epoch': 0.24}
{'loss': 0.0734, 'learning_rate': 4.441967986148207e-05, 'epoch': 0.11}
{'loss': 0.0514, 'learning_rate': 6.081454634805578e-06, 'epoch': 0.88}
{'loss': 0.0686, 'learning_rate': 8.047177345045764e-06, 'epoch': 0.84}
{'loss': 0.0517, 'learning_rate': 3.7639559305796084e-05, 'epoch': 0.25}
{'loss': 0.1055, 'learning_rate': 4.662086242148502e-05, 'epoch': 0.07}
{'loss': 0.0608, 'learning_rate': 1.3667915719638798e-05, 'epoch': 0.73}
{'loss': 0.0565, 'learning_rate': 4.7440549650942186e-05, 'epoch': 0.05}
{'loss': 0.0959, 'learning_rate': 1.907365317279931e-05, 'epoch': 0.62}
{'loss': 0.0497, 'learning_rate': 3.087807214709459e-05, 'epoch': 0.38}
{'loss': 0.0385, 'learning_rate': 1.8644754780942557e-05, 'epoch': 0.63}
{'loss': 0.1013, 'learning_rate': 4.478807861629428e-05, 'epoch': 0.1}
{'loss': 0.0782, 'learning_rate': 3.9426655366815285e-05, 'epoch': 0.21}
{'loss': 0.0368, 'learning_rate': 1.8497365415085302e-05, 'epoch': 0.63}
{'loss': 0.0674, 'learning_rate': 4.434600011051963e-05, 'epoch': 0.11}
{'loss': 0.0961, 'learning_rate': 3.7805372342385495e-05, 'epoch': 0.24}
{'loss': 0.0534, 'learning_rate': 4.515647737110649e-05, 'epoch': 0.1}
{'loss': 0.0317, 'learning_rate': 5.835739296025554e-06, 'epoch': 0.88}
{'loss': 0.0569, 'learning_rate': 7.801462006265742e-06, 'epoch': 0.84}
{'loss': 0.0743, 'learning_rate': 3.749216993993883e-05, 'epoch': 0.25}
{'loss': 0.0864, 'learning_rate': 4.736686989997974e-05, 'epoch': 0.05}
{'loss': 0.1113, 'learning_rate': 4.6547182670522575e-05, 'epoch': 0.07}
{'loss': 0.0371, 'learning_rate': 1.3422200380858773e-05, 'epoch': 0.73}
{'loss': 0.0924, 'learning_rate': 3.0730682781237337e-05, 'epoch': 0.39}
{'loss': 0.0509, 'learning_rate': 1.8349976049228047e-05, 'epoch': 0.63}
{'loss': 0.0407, 'learning_rate': 1.8827937834019287e-05, 'epoch': 0.62}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.092, 'learning_rate': 4.471439886533184e-05, 'epoch': 0.11}
{'loss': 0.07, 'learning_rate': 3.927926600095803e-05, 'epoch': 0.22}
{'loss': 0.0532, 'learning_rate': 1.8202586683370796e-05, 'epoch': 0.64}
{'loss': 0.1143, 'learning_rate': 4.427232035955719e-05, 'epoch': 0.11}
{'loss': 0.0673, 'learning_rate': 4.508279762014405e-05, 'epoch': 0.1}
{'loss': 0.0733, 'learning_rate': 3.765798297652825e-05, 'epoch': 0.25}
{'loss': 0.0602, 'learning_rate': 5.590023957245531e-06, 'epoch': 0.89}
{'loss': 0.048, 'learning_rate': 7.555746667485718e-06, 'epoch': 0.85}
{'loss': 0.0754, 'learning_rate': 3.736320424481374e-05, 'epoch': 0.25}
{'loss': 0.1929, 'learning_rate': 4.72931901490173e-05, 'epoch': 0.05}
{'loss': 0.1327, 'learning_rate': 4.647350291956014e-05, 'epoch': 0.07}
{'loss': 0.0473, 'learning_rate': 1.3176485042078751e-05, 'epoch': 0.74}
{'loss': 0.0377, 'learning_rate': 1.805519731751354e-05, 'epoch': 0.64}
{'loss': 0.1017, 'learning_rate': 3.058329341538008e-05, 'epoch': 0.39}
{'loss': 0.0586, 'learning_rate': 1.8582222495239266e-05, 'epoch': 0.63}
{'loss': 0.0515, 'learning_rate': 4.4640719114369404e-05, 'epoch': 0.11}
{'loss': 0.0588, 'learning_rate': 3.9131876635100775e-05, 'epoch': 0.22}
{'loss': 0.0995, 'learning_rate': 4.419864060859475e-05, 'epoch': 0.12}
{'loss': 0.0575, 'learning_rate': 1.790780795165629e-05, 'epoch': 0.64}
{'loss': 0.0731, 'learning_rate': 3.751059361067099e-05, 'epoch': 0.25}
{'loss': 0.0916, 'learning_rate': 4.500911786918161e-05, 'epoch': 0.1}
{'loss': 0.0337, 'learning_rate': 5.344308618465508e-06, 'epoch': 0.89}
{'loss': 0.1071, 'learning_rate': 4.7219510398054865e-05, 'epoch': 0.06}
{'loss': 0.0602, 'learning_rate': 7.310031328705694e-06, 'epoch': 0.85}
{'loss': 0.0621, 'learning_rate': 3.721581487895648e-05, 'epoch': 0.26}
{'loss': 0.0861, 'learning_rate': 4.639982316859769e-05, 'epoch': 0.07}
{'loss': 0.089, 'learning_rate': 1.2930769703298729e-05, 'epoch': 0.74}
{'loss': 0.0615, 'learning_rate': 1.7760418585799034e-05, 'epoch': 0.65}
{'loss': 0.078, 'learning_rate': 3.043590404952283e-05, 'epoch': 0.39}
{'loss': 0.075, 'learning_rate': 1.8336507156459242e-05, 'epoch': 0.63}
{'loss': 0.0409, 'learning_rate': 4.456703936340696e-05, 'epoch': 0.11}
{'loss': 0.0868, 'learning_rate': 4.412496085763231e-05, 'epoch': 0.12}
{'loss': 0.0623, 'learning_rate': 3.898448726924353e-05, 'epoch': 0.22}
{'loss': 0.0555, 'learning_rate': 1.7613029219941783e-05, 'epoch': 0.65}
{'loss': 0.0824, 'learning_rate': 3.736320424481374e-05, 'epoch': 0.25}
{'loss': 0.0763, 'learning_rate': 4.493543811821916e-05, 'epoch': 0.1}
{'loss': 0.0839, 'learning_rate': 4.714583064709242e-05, 'epoch': 0.06}
{'loss': 0.0443, 'learning_rate': 5.098593279685485e-06, 'epoch': 0.9}
{'loss': 0.0433, 'learning_rate': 7.064315989925671e-06, 'epoch': 0.86}
{'loss': 0.0919, 'learning_rate': 3.706842551309923e-05, 'epoch': 0.26}
{'loss': 0.0994, 'learning_rate': 4.6326143417635254e-05, 'epoch': 0.07}
{'loss': 0.0661, 'learning_rate': 1.2685054364518705e-05, 'epoch': 0.75}
{'loss': 0.0895, 'learning_rate': 1.7465639854084528e-05, 'epoch': 0.65}
{'loss': 0.1129, 'learning_rate': 3.0288514683665575e-05, 'epoch': 0.39}
{'loss': 0.0847, 'learning_rate': 1.809079181767922e-05, 'epoch': 0.64}
{'loss': 0.0694, 'learning_rate': 4.4051281106669865e-05, 'epoch': 0.12}
{'loss': 0.0829, 'learning_rate': 4.4493359612444514e-05, 'epoch': 0.11}
{'loss': 0.0749, 'learning_rate': 3.883709790338627e-05, 'epoch': 0.22}
{'loss': 0.0646, 'learning_rate': 1.7318250488227277e-05, 'epoch': 0.65}
{'loss': 0.0969, 'learning_rate': 4.7072150896129975e-05, 'epoch': 0.06}
{'loss': 0.059, 'learning_rate': 3.721581487895648e-05, 'epoch': 0.26}
{'loss': 0.0952, 'learning_rate': 4.4861758367256725e-05, 'epoch': 0.1}
{'loss': 0.0383, 'learning_rate': 4.852877940905461e-06, 'epoch': 0.9}
{'loss': 0.0519, 'learning_rate': 6.8186006511456475e-06, 'epoch': 0.86}
{'loss': 0.0881, 'learning_rate': 3.692103614724198e-05, 'epoch': 0.26}
{'loss': 0.1095, 'learning_rate': 4.6252463666672816e-05, 'epoch': 0.08}
{'loss': 0.0941, 'learning_rate': 1.2439339025738681e-05, 'epoch': 0.75}
{'loss': 0.0474, 'learning_rate': 1.717086112237002e-05, 'epoch': 0.66}
{'loss': 0.075, 'learning_rate': 3.014112531780832e-05, 'epoch': 0.4}
{'loss': 0.0969, 'learning_rate': 4.397760135570742e-05, 'epoch': 0.12}
{'loss': 0.0742, 'learning_rate': 1.7845076478899195e-05, 'epoch': 0.64}
{'loss': 0.1172, 'learning_rate': 4.441967986148207e-05, 'epoch': 0.11}
{'loss': 0.0833, 'learning_rate': 3.868970853752902e-05, 'epoch': 0.23}
{'loss': 0.0486, 'learning_rate': 1.7023471756512767e-05, 'epoch': 0.66}
{'loss': 0.1372, 'learning_rate': 4.699847114516753e-05, 'epoch': 0.06}
{'loss': 0.0524, 'learning_rate': 3.706842551309923e-05, 'epoch': 0.26}
{'loss': 0.0998, 'learning_rate': 4.478807861629428e-05, 'epoch': 0.1}
{'loss': 0.0402, 'learning_rate': 4.607162602125438e-06, 'epoch': 0.91}
{'loss': 0.0782, 'learning_rate': 6.572885312365625e-06, 'epoch': 0.87}
{'loss': 0.053, 'learning_rate': 3.6773646781384725e-05, 'epoch': 0.27}
{'loss': 0.138, 'learning_rate': 4.617878391571037e-05, 'epoch': 0.08}
{'loss': 0.0536, 'learning_rate': 1.2193623686958658e-05, 'epoch': 0.76}
{'loss': 0.0134, 'learning_rate': 1.6876082390655515e-05, 'epoch': 0.66}
{'loss': 0.0847, 'learning_rate': 2.999373595195107e-05, 'epoch': 0.4}
{'loss': 0.0819, 'learning_rate': 4.390392160474498e-05, 'epoch': 0.12}
{'loss': 0.0556, 'learning_rate': 1.759936114011917e-05, 'epoch': 0.65}
{'loss': 0.1018, 'learning_rate': 4.434600011051963e-05, 'epoch': 0.11}
{'loss': 0.091, 'learning_rate': 4.692479139420509e-05, 'epoch': 0.06}
{'loss': 0.0836, 'learning_rate': 3.854231917167176e-05, 'epoch': 0.23}
{'loss': 0.0502, 'learning_rate': 1.672869302479826e-05, 'epoch': 0.67}
{'loss': 0.109, 'learning_rate': 3.692103614724198e-05, 'epoch': 0.26}
{'loss': 0.0849, 'learning_rate': 4.471439886533184e-05, 'epoch': 0.11}
{'loss': 0.0219, 'learning_rate': 4.361447263345414e-06, 'epoch': 0.91}
{'loss': 0.0879, 'learning_rate': 6.327169973585601e-06, 'epoch': 0.87}
{'loss': 0.0871, 'learning_rate': 3.662625741552747e-05, 'epoch': 0.27}
{'loss': 0.1038, 'learning_rate': 4.610510416474793e-05, 'epoch': 0.08}
{'loss': 0.1203, 'learning_rate': 1.658130365894101e-05, 'epoch': 0.67}
{'loss': 0.0402, 'learning_rate': 1.1947908348178636e-05, 'epoch': 0.76}
{'loss': 0.0964, 'learning_rate': 2.9846346586093814e-05, 'epoch': 0.4}
{'loss': 0.0766, 'learning_rate': 4.383024185378254e-05, 'epoch': 0.12}
{'loss': 0.1541, 'learning_rate': 4.427232035955719e-05, 'epoch': 0.11}
{'loss': 0.0653, 'learning_rate': 1.735364580133915e-05, 'epoch': 0.65}
{'loss': 0.0647, 'learning_rate': 4.6851111643242654e-05, 'epoch': 0.06}
{'loss': 0.1073, 'learning_rate': 3.839492980581451e-05, 'epoch': 0.23}
{'loss': 0.0826, 'learning_rate': 1.6433914293083754e-05, 'epoch': 0.67}
{'loss': 0.0743, 'learning_rate': 3.6773646781384725e-05, 'epoch': 0.27}
{'loss': 0.0437, 'learning_rate': 4.4640719114369404e-05, 'epoch': 0.11}
{'loss': 0.0298, 'learning_rate': 4.115731924565391e-06, 'epoch': 0.92}
{'loss': 0.0532, 'learning_rate': 6.081454634805578e-06, 'epoch': 0.88}
{'loss': 0.0644, 'learning_rate': 3.6478868049670215e-05, 'epoch': 0.27}
{'loss': 0.0562, 'learning_rate': 4.603142441378548e-05, 'epoch': 0.08}
{'loss': 0.0196, 'learning_rate': 1.6286524927226503e-05, 'epoch': 0.67}
{'loss': 0.0688, 'learning_rate': 4.37565621028201e-05, 'epoch': 0.13}
{'loss': 0.0344, 'learning_rate': 1.1702193009398612e-05, 'epoch': 0.77}
{'loss': 0.0397, 'learning_rate': 2.9698957220236563e-05, 'epoch': 0.41}
{'loss': 0.068, 'learning_rate': 4.419864060859475e-05, 'epoch': 0.12}
{'loss': 0.0766, 'learning_rate': 1.7107930462559124e-05, 'epoch': 0.66}
{'loss': 0.0449, 'learning_rate': 4.677743189228021e-05, 'epoch': 0.06}
{'loss': 0.0702, 'learning_rate': 1.6139135561369248e-05, 'epoch': 0.68}
{'loss': 0.0463, 'learning_rate': 3.824754043995726e-05, 'epoch': 0.24}
{'loss': 0.0845, 'learning_rate': 3.662625741552747e-05, 'epoch': 0.27}
{'loss': 0.0514, 'learning_rate': 3.870016585785367e-06, 'epoch': 0.92}
{'loss': 0.1127, 'learning_rate': 4.456703936340696e-05, 'epoch': 0.11}
{'loss': 0.04, 'learning_rate': 5.835739296025554e-06, 'epoch': 0.88}
{'loss': 0.0821, 'learning_rate': 3.633147868381297e-05, 'epoch': 0.27}
{'loss': 0.0448, 'learning_rate': 4.595774466282304e-05, 'epoch': 0.08}
{'loss': 0.1224, 'learning_rate': 4.3682882351857655e-05, 'epoch': 0.13}
{'loss': 0.0421, 'learning_rate': 1.5991746195511993e-05, 'epoch': 0.68}
{'loss': 0.0592, 'learning_rate': 1.1456477670618588e-05, 'epoch': 0.77}
{'loss': 0.0702, 'learning_rate': 2.9551567854379308e-05, 'epoch': 0.41}
{'loss': 0.07, 'learning_rate': 4.412496085763231e-05, 'epoch': 0.12}
{'loss': 0.0898, 'learning_rate': 4.670375214131777e-05, 'epoch': 0.07}
{'loss': 0.0898, 'learning_rate': 1.68622151237791e-05, 'epoch': 0.66}
{'loss': 0.0562, 'learning_rate': 1.584435682965474e-05, 'epoch': 0.68}
{'loss': 0.1025, 'learning_rate': 3.8100151074100005e-05, 'epoch': 0.24}
{'loss': 0.0667, 'learning_rate': 3.6478868049670215e-05, 'epoch': 0.27}
{'loss': 0.0722, 'learning_rate': 3.624301247005344e-06, 'epoch': 0.93}
{'loss': 0.0748, 'learning_rate': 4.4493359612444514e-05, 'epoch': 0.11}
{'loss': 0.0531, 'learning_rate': 5.590023957245531e-06, 'epoch': 0.89}
{'loss': 0.0757, 'learning_rate': 3.618408931795571e-05, 'epoch': 0.28}
{'loss': 0.1105, 'learning_rate': 4.5884064911860605e-05, 'epoch': 0.08}
{'loss': 0.0952, 'learning_rate': 4.3609202600895217e-05, 'epoch': 0.13}
{'loss': 0.0809, 'learning_rate': 1.5696967463797487e-05, 'epoch': 0.69}
{'loss': 0.0659, 'learning_rate': 1.1210762331838564e-05, 'epoch': 0.78}
{'loss': 0.0487, 'learning_rate': 2.9404178488522056e-05, 'epoch': 0.41}
{'loss': 0.0983, 'learning_rate': 4.4051281106669865e-05, 'epoch': 0.12}
{'loss': 0.1506, 'learning_rate': 4.6630072390355326e-05, 'epoch': 0.07}
{'loss': 0.0381, 'learning_rate': 1.661649978499908e-05, 'epoch': 0.67}
{'loss': 0.0437, 'learning_rate': 1.5549578097940235e-05, 'epoch': 0.69}
{'loss': 0.0653, 'learning_rate': 3.795276170824275e-05, 'epoch': 0.24}
{'loss': 0.0744, 'learning_rate': 3.633147868381297e-05, 'epoch': 0.27}
{'loss': 0.0384, 'learning_rate': 3.3785859082253208e-06, 'epoch': 0.93}
{'loss': 0.0424, 'learning_rate': 4.4428889830352376e-05, 'epoch': 0.11}
{'loss': 0.088, 'learning_rate': 5.344308618465508e-06, 'epoch': 0.89}
{'loss': 0.0718, 'learning_rate': 3.603669995209846e-05, 'epoch': 0.28}
{'loss': 0.1035, 'learning_rate': 4.581038516089816e-05, 'epoch': 0.08}
{'loss': 0.0692, 'learning_rate': 4.353552284993277e-05, 'epoch': 0.13}
{'loss': 0.0384, 'learning_rate': 1.540218873208298e-05, 'epoch': 0.69}
{'loss': 0.0547, 'learning_rate': 1.0965046993058542e-05, 'epoch': 0.78}
{'loss': 0.0801, 'learning_rate': 2.92567891226648e-05, 'epoch': 0.42}
{'loss': 0.1146, 'learning_rate': 4.655639263939288e-05, 'epoch': 0.07}
{'loss': 0.0577, 'learning_rate': 4.397760135570742e-05, 'epoch': 0.12}
{'loss': 0.044, 'learning_rate': 1.5254799366225727e-05, 'epoch': 0.7}
{'loss': 0.0544, 'learning_rate': 1.6370784446219056e-05, 'epoch': 0.67}
{'loss': 0.1278, 'learning_rate': 3.7805372342385495e-05, 'epoch': 0.24}
{'loss': 0.0603, 'learning_rate': 3.618408931795571e-05, 'epoch': 0.28}
{'loss': 0.0309, 'learning_rate': 3.1328705694452975e-06, 'epoch': 0.94}
{'loss': 0.0869, 'learning_rate': 4.435521007938994e-05, 'epoch': 0.11}
{'loss': 0.0448, 'learning_rate': 5.098593279685485e-06, 'epoch': 0.9}
{'loss': 0.064, 'learning_rate': 3.58893105862412e-05, 'epoch': 0.28}
{'loss': 0.087, 'learning_rate': 4.573670540993572e-05, 'epoch': 0.09}
{'loss': 0.0622, 'learning_rate': 4.346184309897033e-05, 'epoch': 0.13}
{'loss': 0.0603, 'learning_rate': 1.5107410000368474e-05, 'epoch': 0.7}
{'loss': 0.0524, 'learning_rate': 1.0719331654278519e-05, 'epoch': 0.79}
{'loss': 0.0819, 'learning_rate': 2.9109399756807547e-05, 'epoch': 0.42}
{'loss': 0.1271, 'learning_rate': 4.648271288843044e-05, 'epoch': 0.07}
{'loss': 0.0775, 'learning_rate': 4.390392160474498e-05, 'epoch': 0.12}
{'loss': 0.0664, 'learning_rate': 1.496002063451122e-05, 'epoch': 0.7}
{'loss': 0.0456, 'learning_rate': 3.765798297652825e-05, 'epoch': 0.25}
{'loss': 0.0577, 'learning_rate': 1.6125069107439032e-05, 'epoch': 0.68}
{'loss': 0.0451, 'learning_rate': 2.887155230665274e-06, 'epoch': 0.94}
{'loss': 0.0385, 'learning_rate': 3.603669995209846e-05, 'epoch': 0.28}
{'loss': 0.0601, 'learning_rate': 4.338816334800789e-05, 'epoch': 0.13}
{'loss': 0.0303, 'learning_rate': 4.852877940905461e-06, 'epoch': 0.9}
{'loss': 0.0697, 'learning_rate': 4.428153032842749e-05, 'epoch': 0.11}
{'loss': 0.0769, 'learning_rate': 3.574192122038395e-05, 'epoch': 0.29}
{'loss': 0.1169, 'learning_rate': 4.566302565897328e-05, 'epoch': 0.09}
{'loss': 0.0518, 'learning_rate': 1.4812631268653968e-05, 'epoch': 0.7}
{'loss': 0.0653, 'learning_rate': 1.0473616315498495e-05, 'epoch': 0.79}
{'loss': 0.0415, 'learning_rate': 2.8962010390950295e-05, 'epoch': 0.42}
{'loss': 0.1073, 'learning_rate': 4.6409033137468e-05, 'epoch': 0.07}
{'loss': 0.064, 'learning_rate': 4.383024185378254e-05, 'epoch': 0.12}
{'loss': 0.0976, 'learning_rate': 1.4665241902796714e-05, 'epoch': 0.71}
{'loss': 0.0593, 'learning_rate': 3.751059361067099e-05, 'epoch': 0.25}
{'loss': 0.0544, 'learning_rate': 1.587935376865901e-05, 'epoch': 0.68}
{'loss': 0.0442, 'learning_rate': 2.641439891885251e-06, 'epoch': 0.95}
{'loss': 0.0928, 'learning_rate': 4.3314483597045444e-05, 'epoch': 0.13}
{'loss': 0.0751, 'learning_rate': 3.58893105862412e-05, 'epoch': 0.28}
{'loss': 0.1038, 'learning_rate': 4.4207850577465055e-05, 'epoch': 0.12}
{'loss': 0.0578, 'learning_rate': 4.607162602125438e-06, 'epoch': 0.91}
{'loss': 0.0967, 'learning_rate': 4.558934590801084e-05, 'epoch': 0.09}
{'loss': 0.0539, 'learning_rate': 3.55945318545267e-05, 'epoch': 0.29}
{'loss': 0.0593, 'learning_rate': 1.4517852536939461e-05, 'epoch': 0.71}
{'loss': 0.0463, 'learning_rate': 1.0227900976718471e-05, 'epoch': 0.8}
{'loss': 0.1125, 'learning_rate': 2.881462102509304e-05, 'epoch': 0.42}
{'loss': 0.1047, 'learning_rate': 4.633535338650556e-05, 'epoch': 0.07}
{'loss': 0.1319, 'learning_rate': 4.37565621028201e-05, 'epoch': 0.13}
{'loss': 0.0301, 'learning_rate': 1.4370463171082208e-05, 'epoch': 0.71}
{'loss': 0.0682, 'learning_rate': 3.736320424481374e-05, 'epoch': 0.25}
{'loss': 0.0512, 'learning_rate': 1.5633638429878985e-05, 'epoch': 0.69}
{'loss': 0.1155, 'learning_rate': 4.3240803846083006e-05, 'epoch': 0.14}
{'loss': 0.0469, 'learning_rate': 2.3957245531052276e-06, 'epoch': 0.95}
{'loss': 0.1151, 'learning_rate': 4.4134170826502617e-05, 'epoch': 0.12}
{'loss': 0.0936, 'learning_rate': 3.574192122038395e-05, 'epoch': 0.29}
{'loss': 0.0538, 'learning_rate': 4.361447263345414e-06, 'epoch': 0.91}
{'loss': 0.067, 'learning_rate': 4.5515666157048395e-05, 'epoch': 0.09}
{'loss': 0.0602, 'learning_rate': 3.5447142488669445e-05, 'epoch': 0.29}
{'loss': 0.019, 'learning_rate': 1.4223073805224953e-05, 'epoch': 0.72}
{'loss': 0.0463, 'learning_rate': 9.98218563793845e-06, 'epoch': 0.8}
{'loss': 0.0735, 'learning_rate': 4.6261673635543116e-05, 'epoch': 0.08}
{'loss': 0.0628, 'learning_rate': 2.866723165923579e-05, 'epoch': 0.43}
{'loss': 0.1047, 'learning_rate': 4.3682882351857655e-05, 'epoch': 0.13}
{'loss': 0.0767, 'learning_rate': 1.40756844393677e-05, 'epoch': 0.72}
{'loss': 0.0997, 'learning_rate': 3.721581487895648e-05, 'epoch': 0.26}
{'loss': 0.0971, 'learning_rate': 4.316712409512057e-05, 'epoch': 0.14}
{'loss': 0.0391, 'learning_rate': 1.5387923091098964e-05, 'epoch': 0.69}
{'loss': 0.0558, 'learning_rate': 2.1500092143252042e-06, 'epoch': 0.96}
{'loss': 0.0566, 'learning_rate': 3.55945318545267e-05, 'epoch': 0.29}
{'loss': 0.0538, 'learning_rate': 4.406049107554017e-05, 'epoch': 0.12}
{'loss': 0.0512, 'learning_rate': 4.115731924565391e-06, 'epoch': 0.92}
{'loss': 0.0918, 'learning_rate': 4.544198640608595e-05, 'epoch': 0.09}
{'loss': 0.0498, 'learning_rate': 1.3928295073510447e-05, 'epoch': 0.72}
{'loss': 0.0988, 'learning_rate': 3.529975312281219e-05, 'epoch': 0.29}
{'loss': 0.0672, 'learning_rate': 4.618799388458068e-05, 'epoch': 0.08}
{'loss': 0.0347, 'learning_rate': 2.8519842293378534e-05, 'epoch': 0.43}
{'loss': 0.0367, 'learning_rate': 9.736470299158425e-06, 'epoch': 0.81}
{'loss': 0.0984, 'learning_rate': 4.3609202600895217e-05, 'epoch': 0.13}
{'loss': 0.0335, 'learning_rate': 1.3780905707653194e-05, 'epoch': 0.72}
{'loss': 0.0995, 'learning_rate': 3.706842551309923e-05, 'epoch': 0.26}
{'loss': 0.0752, 'learning_rate': 4.309344434415812e-05, 'epoch': 0.14}
{'loss': 0.0801, 'learning_rate': 1.5142207752318937e-05, 'epoch': 0.7}
{'loss': 0.0549, 'learning_rate': 1.904293875545181e-06, 'epoch': 0.96}
{'loss': 0.0723, 'learning_rate': 3.5447142488669445e-05, 'epoch': 0.29}
{'loss': 0.0679, 'learning_rate': 4.398681132457773e-05, 'epoch': 0.12}
{'loss': 0.0537, 'learning_rate': 3.870016585785367e-06, 'epoch': 0.92}
{'loss': 0.0766, 'learning_rate': 1.363351634179594e-05, 'epoch': 0.73}
{'loss': 0.0992, 'learning_rate': 4.536830665512351e-05, 'epoch': 0.09}
{'loss': 0.0698, 'learning_rate': 3.5152363756954935e-05, 'epoch': 0.3}
{'loss': 0.0699, 'learning_rate': 4.611431413361823e-05, 'epoch': 0.08}
{'loss': 0.0731, 'learning_rate': 2.8372452927521283e-05, 'epoch': 0.43}
{'loss': 0.0621, 'learning_rate': 9.490754960378402e-06, 'epoch': 0.81}
{'loss': 0.0581, 'learning_rate': 1.3486126975938687e-05, 'epoch': 0.73}
{'loss': 0.1041, 'learning_rate': 4.353552284993277e-05, 'epoch': 0.13}
{'loss': 0.0579, 'learning_rate': 3.692103614724198e-05, 'epoch': 0.26}
{'loss': 0.0935, 'learning_rate': 4.301976459319568e-05, 'epoch': 0.14}
{'loss': 0.0806, 'learning_rate': 1.6585785367651576e-06, 'epoch': 0.97}
{'loss': 0.0474, 'learning_rate': 1.4896492413538915e-05, 'epoch': 0.7}
{'loss': 0.0685, 'learning_rate': 1.3338737610081434e-05, 'epoch': 0.73}
{'loss': 0.0558, 'learning_rate': 3.529975312281219e-05, 'epoch': 0.29}
{'loss': 0.0539, 'learning_rate': 3.624301247005344e-06, 'epoch': 0.93}
{'loss': 0.0632, 'learning_rate': 4.391313157361528e-05, 'epoch': 0.12}
{'loss': 0.0619, 'learning_rate': 4.529462690416107e-05, 'epoch': 0.09}
{'loss': 0.0894, 'learning_rate': 3.500497439109769e-05, 'epoch': 0.3}
{'loss': 0.0763, 'learning_rate': 4.604063438265579e-05, 'epoch': 0.08}
{'loss': 0.0929, 'learning_rate': 2.8225063561664028e-05, 'epoch': 0.44}
{'loss': 0.0886, 'learning_rate': 9.245039621598378e-06, 'epoch': 0.82}
{'loss': 0.048, 'learning_rate': 1.3191348244224181e-05, 'epoch': 0.74}
{'loss': 0.0559, 'learning_rate': 4.294608484223323e-05, 'epoch': 0.14}
{'loss': 0.0578, 'learning_rate': 4.346184309897033e-05, 'epoch': 0.13}
{'loss': 0.0438, 'learning_rate': 3.6773646781384725e-05, 'epoch': 0.27}
{'loss': 0.0832, 'learning_rate': 1.4128631979851343e-06, 'epoch': 0.97}
{'loss': 0.033, 'learning_rate': 1.3043958878366926e-05, 'epoch': 0.74}
{'loss': 0.069, 'learning_rate': 1.4650777074758892e-05, 'epoch': 0.71}
{'loss': 0.1182, 'learning_rate': 4.596695463169335e-05, 'epoch': 0.08}
{'loss': 0.0743, 'learning_rate': 3.5152363756954935e-05, 'epoch': 0.3}
{'loss': 0.0533, 'learning_rate': 3.3785859082253208e-06, 'epoch': 0.93}
{'loss': 0.0673, 'learning_rate': 4.522094715319863e-05, 'epoch': 0.1}
{'loss': 0.0914, 'learning_rate': 4.3839451822652844e-05, 'epoch': 0.12}
{'loss': 0.0664, 'learning_rate': 3.485758502524043e-05, 'epoch': 0.3}
{'loss': 0.0528, 'learning_rate': 2.8077674195806776e-05, 'epoch': 0.44}
{'loss': 0.0673, 'learning_rate': 1.2896569512509673e-05, 'epoch': 0.74}
{'loss': 0.0689, 'learning_rate': 8.999324282818356e-06, 'epoch': 0.82}
{'loss': 0.0877, 'learning_rate': 4.2872405091270795e-05, 'epoch': 0.14}
{'loss': 0.0797, 'learning_rate': 4.338816334800789e-05, 'epoch': 0.13}
{'loss': 0.0868, 'learning_rate': 3.662625741552747e-05, 'epoch': 0.27}
{'loss': 0.0452, 'learning_rate': 1.1671478592051108e-06, 'epoch': 0.98}
{'loss': 0.0905, 'learning_rate': 4.5893274880730905e-05, 'epoch': 0.08}
{'loss': 0.0471, 'learning_rate': 1.274918014665242e-05, 'epoch': 0.75}
{'loss': 0.0914, 'learning_rate': 3.500497439109769e-05, 'epoch': 0.3}
{'loss': 0.0638, 'learning_rate': 3.1328705694452975e-06, 'epoch': 0.94}
{'loss': 0.0396, 'learning_rate': 1.4405061735978866e-05, 'epoch': 0.71}
{'loss': 0.1216, 'learning_rate': 4.514726740223619e-05, 'epoch': 0.1}
{'loss': 0.0962, 'learning_rate': 4.3765772071690406e-05, 'epoch': 0.13}
{'loss': 0.0838, 'learning_rate': 3.471019565938318e-05, 'epoch': 0.31}
{'loss': 0.0767, 'learning_rate': 2.793028482994952e-05, 'epoch': 0.44}
{'loss': 0.0596, 'learning_rate': 4.279872534030836e-05, 'epoch': 0.14}
{'loss': 0.0467, 'learning_rate': 1.2601790780795167e-05, 'epoch': 0.75}
{'loss': 0.0885, 'learning_rate': 8.753608944038332e-06, 'epoch': 0.83}
{'loss': 0.0286, 'learning_rate': 4.3314483597045444e-05, 'epoch': 0.13}
{'loss': 0.0939, 'learning_rate': 3.6478868049670215e-05, 'epoch': 0.27}
{'loss': 0.0466, 'learning_rate': 9.214325204250876e-07, 'epoch': 0.98}
{'loss': 0.0868, 'learning_rate': 4.581959512976847e-05, 'epoch': 0.08}
{'loss': 0.0677, 'learning_rate': 1.2454401414937912e-05, 'epoch': 0.75}
{'loss': 0.0704, 'learning_rate': 3.485758502524043e-05, 'epoch': 0.3}
{'loss': 0.036, 'learning_rate': 2.887155230665274e-06, 'epoch': 0.94}
{'loss': 0.0688, 'learning_rate': 3.456280629352592e-05, 'epoch': 0.31}
{'loss': 0.0795, 'learning_rate': 4.5073587651273746e-05, 'epoch': 0.1}
{'loss': 0.0726, 'learning_rate': 4.369209232072796e-05, 'epoch': 0.13}
{'loss': 0.0612, 'learning_rate': 1.4159346397198844e-05, 'epoch': 0.72}
{'loss': 0.0873, 'learning_rate': 2.7782895464092267e-05, 'epoch': 0.44}
{'loss': 0.0758, 'learning_rate': 4.272504558934591e-05, 'epoch': 0.15}
{'loss': 0.0567, 'learning_rate': 1.2307012049080659e-05, 'epoch': 0.75}
{'loss': 0.0678, 'learning_rate': 8.507893605258309e-06, 'epoch': 0.83}
{'loss': 0.1053, 'learning_rate': 4.3240803846083006e-05, 'epoch': 0.14}
{'loss': 0.0726, 'learning_rate': 3.633147868381297e-05, 'epoch': 0.27}
{'loss': 0.0486, 'learning_rate': 6.757171816450642e-07, 'epoch': 0.99}
{'loss': 0.1674, 'learning_rate': 4.574591537880603e-05, 'epoch': 0.09}
{'loss': 0.0596, 'learning_rate': 1.2159622683223406e-05, 'epoch': 0.76}
{'loss': 0.0838, 'learning_rate': 3.471019565938318e-05, 'epoch': 0.31}
{'loss': 0.0275, 'learning_rate': 2.641439891885251e-06, 'epoch': 0.95}
{'loss': 0.0752, 'learning_rate': 4.49999079003113e-05, 'epoch': 0.1}
{'loss': 0.0865, 'learning_rate': 3.441541692766867e-05, 'epoch': 0.31}
{'loss': 0.0579, 'learning_rate': 4.361841256976552e-05, 'epoch': 0.13}
{'loss': 0.0486, 'learning_rate': 1.3913631058418822e-05, 'epoch': 0.72}
{'loss': 0.0834, 'learning_rate': 4.2651365838383474e-05, 'epoch': 0.15}
{'loss': 0.0727, 'learning_rate': 2.7635506098235015e-05, 'epoch': 0.45}
{'loss': 0.0378, 'learning_rate': 1.2012233317366152e-05, 'epoch': 0.76}
{'loss': 0.0532, 'learning_rate': 8.262178266478285e-06, 'epoch': 0.83}
{'loss': 0.0394, 'learning_rate': 4.316712409512057e-05, 'epoch': 0.14}
{'loss': 0.1071, 'learning_rate': 3.618408931795571e-05, 'epoch': 0.28}
{'loss': 0.0745, 'learning_rate': 4.300018428650409e-07, 'epoch': 0.99}
{'loss': 0.13, 'learning_rate': 4.5672235627843584e-05, 'epoch': 0.09}
{'loss': 0.065, 'learning_rate': 1.18648439515089e-05, 'epoch': 0.76}
{'loss': 0.0707, 'learning_rate': 3.456280629352592e-05, 'epoch': 0.31}
{'loss': 0.0683, 'learning_rate': 4.4926228149348856e-05, 'epoch': 0.1}
{'loss': 0.0379, 'learning_rate': 2.3957245531052276e-06, 'epoch': 0.95}
{'loss': 0.0502, 'learning_rate': 3.426802756181142e-05, 'epoch': 0.32}
{'loss': 0.0567, 'learning_rate': 4.354473281880308e-05, 'epoch': 0.13}
{'loss': 0.0545, 'learning_rate': 4.257768608742103e-05, 'epoch': 0.15}
{'loss': 0.0576, 'learning_rate': 1.3667915719638798e-05, 'epoch': 0.73}
{'loss': 0.0868, 'learning_rate': 2.748811673237776e-05, 'epoch': 0.45}
{'loss': 0.0329, 'learning_rate': 1.1717454585651644e-05, 'epoch': 0.77}
{'loss': 0.0608, 'learning_rate': 8.016462927698261e-06, 'epoch': 0.84}
{'loss': 0.0538, 'learning_rate': 4.309344434415812e-05, 'epoch': 0.14}
{'loss': 0.0735, 'learning_rate': 3.603669995209846e-05, 'epoch': 0.28}
{'loss': 0.0499, 'learning_rate': 1.842865040850175e-07, 'epoch': 1.0}
{'loss': 0.0766, 'learning_rate': 4.5598555876881146e-05, 'epoch': 0.09}
{'loss': 0.0354, 'learning_rate': 1.1570065219794391e-05, 'epoch': 0.77}
{'loss': 0.068, 'learning_rate': 3.441541692766867e-05, 'epoch': 0.31}
{'loss': 0.1128, 'learning_rate': 4.485254839838642e-05, 'epoch': 0.1}
{'loss': 0.0493, 'learning_rate': 2.1500092143252042e-06, 'epoch': 0.96}
{'loss': 0.0778, 'learning_rate': 3.4120638195954164e-05, 'epoch': 0.32}
{'loss': 0.0707, 'learning_rate': 4.347105306784063e-05, 'epoch': 0.13}
{'loss': 0.0512, 'learning_rate': 4.2504006336458585e-05, 'epoch': 0.15}
{'loss': 0.0381, 'learning_rate': 1.3422200380858773e-05, 'epoch': 0.73}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0826, 'learning_rate': 2.734072736652051e-05, 'epoch': 0.45}
{'loss': 0.0669, 'learning_rate': 1.1422675853937138e-05, 'epoch': 0.77}
{'loss': 0.0541, 'learning_rate': 7.770747588918239e-06, 'epoch': 0.84}
{'loss': 0.0696, 'learning_rate': 4.301976459319568e-05, 'epoch': 0.14}
{'loss': 0.0797, 'learning_rate': 3.58893105862412e-05, 'epoch': 0.28}
{'loss': 0.1148, 'learning_rate': 4.5524876125918694e-05, 'epoch': 0.09}
{'loss': 0.0259, 'learning_rate': 1.1275286488079885e-05, 'epoch': 0.78}
{'loss': 0.0957, 'learning_rate': 4.477886864742398e-05, 'epoch': 0.1}
{'loss': 0.0516, 'learning_rate': 1.904293875545181e-06, 'epoch': 0.96}
{'loss': 0.0675, 'learning_rate': 3.426802756181142e-05, 'epoch': 0.32}
{'loss': 0.0947, 'learning_rate': 3.397324883009691e-05, 'epoch': 0.32}
{'loss': 0.0514, 'learning_rate': 4.2430326585496146e-05, 'epoch': 0.15}
{'loss': 0.0908, 'learning_rate': 4.3397373316878195e-05, 'epoch': 0.13}
{'loss': 0.0622, 'learning_rate': 1.3176485042078751e-05, 'epoch': 0.74}
{'loss': 0.0765, 'learning_rate': 2.7193338000663254e-05, 'epoch': 0.46}
{'loss': 0.0577, 'learning_rate': 1.1127897122222632e-05, 'epoch': 0.78}
{'loss': 0.0727, 'learning_rate': 7.5250322501382144e-06, 'epoch': 0.85}
{'loss': 0.0862, 'learning_rate': 4.294608484223323e-05, 'epoch': 0.14}
{'loss': 0.1172, 'learning_rate': 4.5451196374956256e-05, 'epoch': 0.09}
{'loss': 0.0651, 'learning_rate': 3.574192122038395e-05, 'epoch': 0.29}
{'loss': 0.0523, 'learning_rate': 1.0980507756365379e-05, 'epoch': 0.78}
{'loss': 0.0765, 'learning_rate': 4.4705188896461535e-05, 'epoch': 0.11}
{'loss': 0.1272, 'learning_rate': 4.23566468345337e-05, 'epoch': 0.15}
{'loss': 0.0453, 'learning_rate': 1.6585785367651576e-06, 'epoch': 0.97}
{'loss': 0.077, 'learning_rate': 3.4120638195954164e-05, 'epoch': 0.32}
{'loss': 0.0755, 'learning_rate': 3.3825859464239655e-05, 'epoch': 0.32}
{'loss': 0.0707, 'learning_rate': 4.332369356591575e-05, 'epoch': 0.13}
{'loss': 0.0694, 'learning_rate': 1.0833118390508125e-05, 'epoch': 0.78}
{'loss': 0.0748, 'learning_rate': 2.7045948634806002e-05, 'epoch': 0.46}
{'loss': 0.0842, 'learning_rate': 1.2930769703298729e-05, 'epoch': 0.74}
{'loss': 0.0676, 'learning_rate': 7.2793169113581916e-06, 'epoch': 0.85}
{'loss': 0.0702, 'learning_rate': 4.537751662399382e-05, 'epoch': 0.09}
{'loss': 0.0688, 'learning_rate': 4.2872405091270795e-05, 'epoch': 0.14}
{'loss': 0.0848, 'learning_rate': 3.55945318545267e-05, 'epoch': 0.29}
{'loss': 0.0593, 'learning_rate': 1.0685729024650872e-05, 'epoch': 0.79}
{'loss': 0.0525, 'learning_rate': 4.2282967083571264e-05, 'epoch': 0.15}
{'loss': 0.0981, 'learning_rate': 4.46315091454991e-05, 'epoch': 0.11}
{'loss': 0.0681, 'learning_rate': 1.4128631979851343e-06, 'epoch': 0.97}
{'loss': 0.1016, 'learning_rate': 3.397324883009691e-05, 'epoch': 0.32}
{'loss': 0.0716, 'learning_rate': 3.36784700983824e-05, 'epoch': 0.33}
{'loss': 0.0566, 'learning_rate': 4.325001381495331e-05, 'epoch': 0.14}
{'loss': 0.0494, 'learning_rate': 2.6898559268948748e-05, 'epoch': 0.46}
{'loss': 0.0806, 'learning_rate': 1.0538339658793617e-05, 'epoch': 0.79}
{'loss': 0.0325, 'learning_rate': 1.2685054364518705e-05, 'epoch': 0.75}
{'loss': 0.0375, 'learning_rate': 7.033601572578168e-06, 'epoch': 0.86}
{'loss': 0.0673, 'learning_rate': 4.530383687303137e-05, 'epoch': 0.09}
{'loss': 0.0697, 'learning_rate': 4.279872534030836e-05, 'epoch': 0.14}
{'loss': 0.079, 'learning_rate': 3.5447142488669445e-05, 'epoch': 0.29}
{'loss': 0.0321, 'learning_rate': 1.0390950292936364e-05, 'epoch': 0.79}
{'loss': 0.1065, 'learning_rate': 4.220928733260882e-05, 'epoch': 0.16}
{'loss': 0.0478, 'learning_rate': 4.455782939453665e-05, 'epoch': 0.11}
{'loss': 0.0783, 'learning_rate': 1.1671478592051108e-06, 'epoch': 0.98}
{'loss': 0.084, 'learning_rate': 3.3825859464239655e-05, 'epoch': 0.32}
{'loss': 0.0614, 'learning_rate': 3.353108073252515e-05, 'epoch': 0.33}
{'loss': 0.1068, 'learning_rate': 4.317633406399087e-05, 'epoch': 0.14}
{'loss': 0.0361, 'learning_rate': 2.6751169903091493e-05, 'epoch': 0.47}
{'loss': 0.081, 'learning_rate': 1.0243560927079111e-05, 'epoch': 0.8}
{'loss': 0.0428, 'learning_rate': 1.2439339025738681e-05, 'epoch': 0.75}
{'loss': 0.0588, 'learning_rate': 6.787886233798145e-06, 'epoch': 0.86}
{'loss': 0.1094, 'learning_rate': 4.5230157122068935e-05, 'epoch': 0.1}
{'loss': 0.0854, 'learning_rate': 4.272504558934591e-05, 'epoch': 0.15}
{'loss': 0.0547, 'learning_rate': 3.529975312281219e-05, 'epoch': 0.29}
{'loss': 0.0662, 'learning_rate': 1.0096171561221858e-05, 'epoch': 0.8}
{'loss': 0.0837, 'learning_rate': 4.213560758164638e-05, 'epoch': 0.16}
{'loss': 0.0877, 'learning_rate': 4.448414964357421e-05, 'epoch': 0.11}
{'loss': 0.0536, 'learning_rate': 9.214325204250876e-07, 'epoch': 0.98}
{'loss': 0.0952, 'learning_rate': 3.36784700983824e-05, 'epoch': 0.33}
{'loss': 0.057, 'learning_rate': 3.33836913666679e-05, 'epoch': 0.33}
{'loss': 0.0676, 'learning_rate': 4.310265431302843e-05, 'epoch': 0.14}
{'loss': 0.0858, 'learning_rate': 2.660378053723424e-05, 'epoch': 0.47}
{'loss': 0.0419, 'learning_rate': 9.948782195364605e-06, 'epoch': 0.8}
{'loss': 0.0663, 'learning_rate': 1.2193623686958658e-05, 'epoch': 0.76}
{'loss': 0.0615, 'learning_rate': 4.515647737110649e-05, 'epoch': 0.1}
{'loss': 0.0594, 'learning_rate': 6.542170895018121e-06, 'epoch': 0.87}
{'loss': 0.0915, 'learning_rate': 4.2651365838383474e-05, 'epoch': 0.15}
{'loss': 0.1363, 'learning_rate': 3.5152363756954935e-05, 'epoch': 0.3}
{'loss': 0.0539, 'learning_rate': 4.206192783068394e-05, 'epoch': 0.16}
{'loss': 0.0737, 'learning_rate': 9.801392829507352e-06, 'epoch': 0.8}
{'loss': 0.0754, 'learning_rate': 4.441046989261177e-05, 'epoch': 0.11}
{'loss': 0.0248, 'learning_rate': 6.757171816450642e-07, 'epoch': 0.99}
{'loss': 0.1066, 'learning_rate': 3.323630200081064e-05, 'epoch': 0.34}
{'loss': 0.0756, 'learning_rate': 3.353108073252515e-05, 'epoch': 0.33}
{'loss': 0.1087, 'learning_rate': 4.3028974562065985e-05, 'epoch': 0.14}
{'loss': 0.0346, 'learning_rate': 9.654003463650098e-06, 'epoch': 0.81}
{'loss': 0.051, 'learning_rate': 2.6456391171376986e-05, 'epoch': 0.47}
{'loss': 0.0924, 'learning_rate': 4.508279762014405e-05, 'epoch': 0.1}
{'loss': 0.0676, 'learning_rate': 6.296455556238098e-06, 'epoch': 0.87}
{'loss': 0.0797, 'learning_rate': 1.1947908348178636e-05, 'epoch': 0.76}
{'loss': 0.058, 'learning_rate': 4.257768608742103e-05, 'epoch': 0.15}
{'loss': 0.0939, 'learning_rate': 3.500497439109769e-05, 'epoch': 0.3}
{'loss': 0.1241, 'learning_rate': 4.198824807972149e-05, 'epoch': 0.16}
{'loss': 0.0409, 'learning_rate': 9.506614097792845e-06, 'epoch': 0.81}
{'loss': 0.0611, 'learning_rate': 4.4336790141649325e-05, 'epoch': 0.11}
{'loss': 0.0657, 'learning_rate': 4.300018428650409e-07, 'epoch': 0.99}
{'loss': 0.0729, 'learning_rate': 3.33836913666679e-05, 'epoch': 0.33}
{'loss': 0.0583, 'learning_rate': 3.308891263495339e-05, 'epoch': 0.34}
{'loss': 0.0812, 'learning_rate': 4.295529481110354e-05, 'epoch': 0.14}
{'loss': 0.0677, 'learning_rate': 4.500911786918161e-05, 'epoch': 0.1}
{'loss': 0.0454, 'learning_rate': 9.35922473193559e-06, 'epoch': 0.81}
{'loss': 0.0795, 'learning_rate': 2.6309001805519735e-05, 'epoch': 0.47}
{'loss': 0.0524, 'learning_rate': 6.0507402174580754e-06, 'epoch': 0.88}
{'loss': 0.0359, 'learning_rate': 1.1702193009398612e-05, 'epoch': 0.77}
{'loss': 0.1104, 'learning_rate': 4.2504006336458585e-05, 'epoch': 0.15}
{'loss': 0.0727, 'learning_rate': 3.485758502524043e-05, 'epoch': 0.3}
{'loss': 0.1288, 'learning_rate': 4.191456832875905e-05, 'epoch': 0.16}
{'loss': 0.0489, 'learning_rate': 9.211835366078337e-06, 'epoch': 0.82}
{'loss': 0.0716, 'learning_rate': 4.4263110390686887e-05, 'epoch': 0.11}
{'loss': 0.0491, 'learning_rate': 1.842865040850175e-07, 'epoch': 1.0}
{'loss': 0.0944, 'learning_rate': 3.323630200081064e-05, 'epoch': 0.34}
{'loss': 0.0468, 'learning_rate': 3.294152326909614e-05, 'epoch': 0.34}
{'loss': 0.0839, 'learning_rate': 4.493543811821916e-05, 'epoch': 0.1}
{'loss': 0.109, 'learning_rate': 4.28816150601411e-05, 'epoch': 0.14}
{'loss': 0.0533, 'learning_rate': 9.064446000221084e-06, 'epoch': 0.82}
{'loss': 0.0622, 'learning_rate': 2.616161243966248e-05, 'epoch': 0.48}
{'loss': 0.0498, 'learning_rate': 5.805024878678052e-06, 'epoch': 0.88}
{'loss': 0.0613, 'learning_rate': 4.2430326585496146e-05, 'epoch': 0.15}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0578, 'learning_rate': 1.1456477670618588e-05, 'epoch': 0.77}
{'loss': 0.1053, 'learning_rate': 4.184088857779661e-05, 'epoch': 0.16}
{'loss': 0.0718, 'learning_rate': 3.471019565938318e-05, 'epoch': 0.31}
{'loss': 0.0788, 'learning_rate': 8.917056634363831e-06, 'epoch': 0.82}
{'loss': 0.0708, 'learning_rate': 4.418943063972444e-05, 'epoch': 0.12}
{'loss': 0.0654, 'learning_rate': 3.2794133903238884e-05, 'epoch': 0.34}
{'loss': 0.0785, 'learning_rate': 3.308891263495339e-05, 'epoch': 0.34}
{'loss': 0.0937, 'learning_rate': 4.4861758367256725e-05, 'epoch': 0.1}
{'loss': 0.1053, 'learning_rate': 4.280793530917866e-05, 'epoch': 0.14}
{'loss': 0.0193, 'learning_rate': 8.769667268506578e-06, 'epoch': 0.83}
{'loss': 0.0653, 'learning_rate': 2.601422307380523e-05, 'epoch': 0.48}
{'loss': 0.0524, 'learning_rate': 5.559309539898029e-06, 'epoch': 0.89}
{'loss': 0.0641, 'learning_rate': 4.23566468345337e-05, 'epoch': 0.15}
{'loss': 0.0773, 'learning_rate': 4.176720882683417e-05, 'epoch': 0.17}
{'loss': 0.0544, 'learning_rate': 1.1210762331838564e-05, 'epoch': 0.78}
{'loss': 0.0564, 'learning_rate': 3.456280629352592e-05, 'epoch': 0.31}
{'loss': 0.0387, 'learning_rate': 8.622277902649325e-06, 'epoch': 0.83}
{'loss': 0.0613, 'learning_rate': 4.4115750888762004e-05, 'epoch': 0.12}
{'loss': 0.0758, 'learning_rate': 3.294152326909614e-05, 'epoch': 0.34}
{'loss': 0.085, 'learning_rate': 3.264674453738163e-05, 'epoch': 0.35}
{'loss': 0.0739, 'learning_rate': 4.478807861629428e-05, 'epoch': 0.1}
{'loss': 0.0971, 'learning_rate': 4.273425555821622e-05, 'epoch': 0.15}
{'loss': 0.0728, 'learning_rate': 8.474888536792071e-06, 'epoch': 0.83}
{'loss': 0.062, 'learning_rate': 2.5866833707947974e-05, 'epoch': 0.48}
{'loss': 0.0378, 'learning_rate': 4.2282967083571264e-05, 'epoch': 0.15}
{'loss': 0.0475, 'learning_rate': 5.313594201118004e-06, 'epoch': 0.89}
{'loss': 0.0757, 'learning_rate': 4.169352907587173e-05, 'epoch': 0.17}
{'loss': 0.0625, 'learning_rate': 3.441541692766867e-05, 'epoch': 0.31}
{'loss': 0.062, 'learning_rate': 8.327499170934818e-06, 'epoch': 0.83}
{'loss': 0.0331, 'learning_rate': 4.404207113779956e-05, 'epoch': 0.12}
{'loss': 0.0664, 'learning_rate': 3.2499355171524375e-05, 'epoch': 0.35}
{'loss': 0.068, 'learning_rate': 3.2794133903238884e-05, 'epoch': 0.34}
{'loss': 0.0777, 'learning_rate': 4.471439886533184e-05, 'epoch': 0.11}
{'loss': 0.1075, 'learning_rate': 4.266057580725378e-05, 'epoch': 0.15}
{'loss': 0.0349, 'learning_rate': 1.0965046993058542e-05, 'epoch': 0.78}
{'loss': 0.0322, 'learning_rate': 8.180109805077563e-06, 'epoch': 0.84}
{'loss': 0.094, 'learning_rate': 2.5719444342090722e-05, 'epoch': 0.49}
{'loss': 0.0494, 'learning_rate': 4.220928733260882e-05, 'epoch': 0.16}
{'loss': 0.0507, 'learning_rate': 5.067878862337981e-06, 'epoch': 0.9}
{'loss': 0.0644, 'learning_rate': 4.161984932490929e-05, 'epoch': 0.17}
{'loss': 0.0621, 'learning_rate': 3.426802756181142e-05, 'epoch': 0.32}
{'loss': 0.0343, 'learning_rate': 8.03272043922031e-06, 'epoch': 0.84}
{'loss': 0.0739, 'learning_rate': 4.3968391386837114e-05, 'epoch': 0.12}
{'loss': 0.0691, 'learning_rate': 4.4640719114369404e-05, 'epoch': 0.11}
{'loss': 0.0873, 'learning_rate': 3.235196580566712e-05, 'epoch': 0.35}
{'loss': 0.0872, 'learning_rate': 3.264674453738163e-05, 'epoch': 0.35}
{'loss': 0.0513, 'learning_rate': 4.2586896056291336e-05, 'epoch': 0.15}
{'loss': 0.0939, 'learning_rate': 1.0719331654278519e-05, 'epoch': 0.79}
{'loss': 0.0576, 'learning_rate': 7.885331073363057e-06, 'epoch': 0.84}
{'loss': 0.0678, 'learning_rate': 2.5572054976233467e-05, 'epoch': 0.49}
{'loss': 0.1109, 'learning_rate': 4.213560758164638e-05, 'epoch': 0.16}
{'loss': 0.0821, 'learning_rate': 4.154616957394685e-05, 'epoch': 0.17}
{'loss': 0.0434, 'learning_rate': 4.822163523557958e-06, 'epoch': 0.9}
{'loss': 0.0857, 'learning_rate': 3.4120638195954164e-05, 'epoch': 0.32}
{'loss': 0.0495, 'learning_rate': 7.737941707505804e-06, 'epoch': 0.85}
{'loss': 0.0699, 'learning_rate': 4.3894711635874676e-05, 'epoch': 0.12}
{'loss': 0.0523, 'learning_rate': 4.456703936340696e-05, 'epoch': 0.11}
{'loss': 0.0972, 'learning_rate': 3.220457643980987e-05, 'epoch': 0.36}
{'loss': 0.0862, 'learning_rate': 3.2499355171524375e-05, 'epoch': 0.35}
{'loss': 0.0556, 'learning_rate': 4.251321630532889e-05, 'epoch': 0.15}
{'loss': 0.0276, 'learning_rate': 7.590552341648551e-06, 'epoch': 0.85}
{'loss': 0.0331, 'learning_rate': 1.0473616315498495e-05, 'epoch': 0.79}
{'loss': 0.079, 'learning_rate': 2.5424665610376212e-05, 'epoch': 0.49}
{'loss': 0.0502, 'learning_rate': 4.206192783068394e-05, 'epoch': 0.16}
{'loss': 0.0826, 'learning_rate': 4.14724898229844e-05, 'epoch': 0.17}
{'loss': 0.0342, 'learning_rate': 4.576448184777935e-06, 'epoch': 0.91}
{'loss': 0.0583, 'learning_rate': 3.397324883009691e-05, 'epoch': 0.32}
{'loss': 0.0654, 'learning_rate': 7.4431629757912975e-06, 'epoch': 0.85}
{'loss': 0.0812, 'learning_rate': 4.382103188491223e-05, 'epoch': 0.12}
{'loss': 0.0776, 'learning_rate': 4.4493359612444514e-05, 'epoch': 0.11}
{'loss': 0.0647, 'learning_rate': 3.205718707395262e-05, 'epoch': 0.36}
{'loss': 0.0706, 'learning_rate': 3.235196580566712e-05, 'epoch': 0.35}
{'loss': 0.0652, 'learning_rate': 4.2439536554366446e-05, 'epoch': 0.15}
{'loss': 0.0603, 'learning_rate': 4.139881007202196e-05, 'epoch': 0.17}
{'loss': 0.053, 'learning_rate': 7.2957736099340435e-06, 'epoch': 0.85}
{'loss': 0.0509, 'learning_rate': 2.527727624451896e-05, 'epoch': 0.5}
{'loss': 0.0823, 'learning_rate': 4.198824807972149e-05, 'epoch': 0.16}
{'loss': 0.0365, 'learning_rate': 1.0227900976718471e-05, 'epoch': 0.8}
{'loss': 0.0606, 'learning_rate': 4.330732845997911e-06, 'epoch': 0.91}
{'loss': 0.0432, 'learning_rate': 3.3825859464239655e-05, 'epoch': 0.32}
{'loss': 0.0499, 'learning_rate': 7.14838424407679e-06, 'epoch': 0.86}
{'loss': 0.101, 'learning_rate': 4.441967986148207e-05, 'epoch': 0.11}
{'loss': 0.1218, 'learning_rate': 4.374735213394979e-05, 'epoch': 0.13}
{'loss': 0.0495, 'learning_rate': 3.190979770809536e-05, 'epoch': 0.36}
{'loss': 0.0567, 'learning_rate': 3.220457643980987e-05, 'epoch': 0.36}
{'loss': 0.117, 'learning_rate': 4.236585680340401e-05, 'epoch': 0.15}
{'loss': 0.0668, 'learning_rate': 4.132513032105952e-05, 'epoch': 0.17}
{'loss': 0.0438, 'learning_rate': 7.000994878219537e-06, 'epoch': 0.86}
{'loss': 0.0579, 'learning_rate': 9.98218563793845e-06, 'epoch': 0.8}
{'loss': 0.0592, 'learning_rate': 2.5129886878661706e-05, 'epoch': 0.5}
{'loss': 0.1037, 'learning_rate': 4.191456832875905e-05, 'epoch': 0.16}
{'loss': 0.0315, 'learning_rate': 4.085017507217888e-06, 'epoch': 0.92}
{'loss': 0.0626, 'learning_rate': 3.36784700983824e-05, 'epoch': 0.33}
{'loss': 0.0326, 'learning_rate': 6.853605512362283e-06, 'epoch': 0.86}
{'loss': 0.0768, 'learning_rate': 4.434600011051963e-05, 'epoch': 0.11}
{'loss': 0.0763, 'learning_rate': 4.3673672382987355e-05, 'epoch': 0.13}
{'loss': 0.0607, 'learning_rate': 3.176240834223811e-05, 'epoch': 0.37}
{'loss': 0.0854, 'learning_rate': 3.205718707395262e-05, 'epoch': 0.36}
{'loss': 0.0696, 'learning_rate': 4.1251450570097076e-05, 'epoch': 0.18}
{'loss': 0.0806, 'learning_rate': 4.184088857779661e-05, 'epoch': 0.16}
{'loss': 0.0864, 'learning_rate': 4.229217705244157e-05, 'epoch': 0.15}
{'loss': 0.0306, 'learning_rate': 6.70621614650503e-06, 'epoch': 0.87}
{'loss': 0.0714, 'learning_rate': 2.498249751280445e-05, 'epoch': 0.5}
{'loss': 0.0376, 'learning_rate': 9.736470299158425e-06, 'epoch': 0.81}
{'loss': 0.0468, 'learning_rate': 3.839302168437864e-06, 'epoch': 0.92}
{'loss': 0.0399, 'learning_rate': 3.353108073252515e-05, 'epoch': 0.33}
{'loss': 0.0239, 'learning_rate': 6.558826780647777e-06, 'epoch': 0.87}
{'loss': 0.0856, 'learning_rate': 4.427232035955719e-05, 'epoch': 0.11}
{'loss': 0.1076, 'learning_rate': 4.359999263202491e-05, 'epoch': 0.13}
{'loss': 0.0712, 'learning_rate': 3.161501897638086e-05, 'epoch': 0.37}
{'loss': 0.0651, 'learning_rate': 4.117777081913464e-05, 'epoch': 0.18}
{'loss': 0.0381, 'learning_rate': 3.190979770809536e-05, 'epoch': 0.36}
{'loss': 0.1009, 'learning_rate': 4.176720882683417e-05, 'epoch': 0.17}
{'loss': 0.0908, 'learning_rate': 4.2218497301479125e-05, 'epoch': 0.16}
{'loss': 0.0646, 'learning_rate': 6.411437414790524e-06, 'epoch': 0.87}
{'loss': 0.0638, 'learning_rate': 2.4835108146947196e-05, 'epoch': 0.5}
{'loss': 0.0596, 'learning_rate': 9.490754960378402e-06, 'epoch': 0.81}
{'loss': 0.076, 'learning_rate': 3.593586829657841e-06, 'epoch': 0.93}
{'loss': 0.0958, 'learning_rate': 3.33836913666679e-05, 'epoch': 0.33}
{'loss': 0.1168, 'learning_rate': 4.419864060859475e-05, 'epoch': 0.12}
{'loss': 0.0948, 'learning_rate': 6.26404804893327e-06, 'epoch': 0.88}
{'eval_loss': 0.05244537815451622, 'eval_accuracy': 0.9825948820665125, 'eval_f1': 0.9825436408977557, 'eval_precision': 0.9888799804789626, 'eval_recall': 0.9762879856833121, 'eval_runtime': 408.9332, 'eval_samples_per_second': 141.622, 'eval_steps_per_second': 17.705, 'epoch': 1.0}
{'loss': 0.0971, 'learning_rate': 4.3526312881062465e-05, 'epoch': 0.13}
{'loss': 0.0568, 'learning_rate': 4.1104091068172194e-05, 'epoch': 0.18}
{'train_runtime': 5111.827, 'train_samples_per_second': 20.393, 'train_steps_per_second': 0.318, 'train_loss': 0.08089796352115544, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0581, 'learning_rate': 3.1467629610523604e-05, 'epoch': 0.37}
{'loss': 0.0737, 'learning_rate': 4.169352907587173e-05, 'epoch': 0.17}
{'loss': 0.0686, 'learning_rate': 3.176240834223811e-05, 'epoch': 0.37}
{'loss': 0.0478, 'learning_rate': 6.116658683076016e-06, 'epoch': 0.88}
{'loss': 0.0784, 'learning_rate': 4.214481755051669e-05, 'epoch': 0.16}
{'loss': 0.0474, 'learning_rate': 2.4687718781089945e-05, 'epoch': 0.51}
{'loss': 0.0393, 'learning_rate': 9.245039621598378e-06, 'epoch': 0.82}
{'loss': 0.0528, 'learning_rate': 3.347871490877818e-06, 'epoch': 0.93}
{'loss': 0.0718, 'learning_rate': 3.323630200081064e-05, 'epoch': 0.34}
{'loss': 0.0986, 'learning_rate': 4.412496085763231e-05, 'epoch': 0.12}
{'loss': 0.0537, 'learning_rate': 5.9692693172187625e-06, 'epoch': 0.88}
{'loss': 0.0933, 'learning_rate': 4.345263313010002e-05, 'epoch': 0.13}
{'loss': 0.0782, 'learning_rate': 4.1030411317209756e-05, 'epoch': 0.18}
{'loss': 0.0865, 'learning_rate': 4.161984932490929e-05, 'epoch': 0.17}
{'loss': 0.1053, 'learning_rate': 3.132024024466635e-05, 'epoch': 0.37}
{'loss': 0.0689, 'learning_rate': 3.161501897638086e-05, 'epoch': 0.37}
{'loss': 0.0487, 'learning_rate': 5.821879951361509e-06, 'epoch': 0.88}
{'loss': 0.0955, 'learning_rate': 4.207113779955424e-05, 'epoch': 0.16}
{'loss': 0.0547, 'learning_rate': 2.454032941523269e-05, 'epoch': 0.51}
{'loss': 0.0642, 'learning_rate': 8.999324282818356e-06, 'epoch': 0.82}
{'loss': 0.0432, 'learning_rate': 3.1021561520977945e-06, 'epoch': 0.94}
{'loss': 0.0645, 'learning_rate': 3.308891263495339e-05, 'epoch': 0.34}
{'loss': 0.0909, 'learning_rate': 4.4051281106669865e-05, 'epoch': 0.12}
{'loss': 0.0631, 'learning_rate': 5.674490585504256e-06, 'epoch': 0.89}
{'loss': 0.0842, 'learning_rate': 4.337895337913758e-05, 'epoch': 0.13}
{'loss': 0.0867, 'learning_rate': 4.095673156624731e-05, 'epoch': 0.18}
{'loss': 0.0831, 'learning_rate': 4.154616957394685e-05, 'epoch': 0.17}
{'loss': 0.0716, 'learning_rate': 3.1172850878809094e-05, 'epoch': 0.38}
{'loss': 0.0648, 'learning_rate': 3.1467629610523604e-05, 'epoch': 0.37}
{'loss': 0.0208, 'learning_rate': 5.527101219647002e-06, 'epoch': 0.89}
{'loss': 0.0987, 'learning_rate': 4.19974580485918e-05, 'epoch': 0.16}
{'loss': 0.0565, 'learning_rate': 2.439294004937544e-05, 'epoch': 0.51}
{'loss': 0.0562, 'learning_rate': 2.856440813317771e-06, 'epoch': 0.94}
{'loss': 0.0875, 'learning_rate': 3.294152326909614e-05, 'epoch': 0.34}
{'loss': 0.0249, 'learning_rate': 8.753608944038332e-06, 'epoch': 0.83}
{'loss': 0.094, 'learning_rate': 4.397760135570742e-05, 'epoch': 0.12}
{'loss': 0.0604, 'learning_rate': 5.379711853789749e-06, 'epoch': 0.89}
{'loss': 0.0524, 'learning_rate': 4.3305273628175144e-05, 'epoch': 0.13}
{'loss': 0.1187, 'learning_rate': 4.0883051815284866e-05, 'epoch': 0.18}
{'loss': 0.0512, 'learning_rate': 4.14724898229844e-05, 'epoch': 0.17}
{'loss': 0.0744, 'learning_rate': 3.102546151295184e-05, 'epoch': 0.38}
{'loss': 0.0616, 'learning_rate': 3.132024024466635e-05, 'epoch': 0.37}
{'loss': 0.0367, 'learning_rate': 5.232322487932496e-06, 'epoch': 0.9}
{'loss': 0.0624, 'learning_rate': 2.4245550683518184e-05, 'epoch': 0.52}
{'loss': 0.0758, 'learning_rate': 4.192377829762936e-05, 'epoch': 0.16}
{'loss': 0.0525, 'learning_rate': 2.610725474537748e-06, 'epoch': 0.95}
{'loss': 0.0571, 'learning_rate': 3.2794133903238884e-05, 'epoch': 0.34}
{'loss': 0.0615, 'learning_rate': 8.507893605258309e-06, 'epoch': 0.83}
{'loss': 0.1154, 'learning_rate': 4.390392160474498e-05, 'epoch': 0.12}
{'loss': 0.0891, 'learning_rate': 5.084933122075243e-06, 'epoch': 0.9}
{'loss': 0.1289, 'learning_rate': 4.32315938772127e-05, 'epoch': 0.14}
{'loss': 0.0688, 'learning_rate': 4.080937206432243e-05, 'epoch': 0.18}
{'loss': 0.0767, 'learning_rate': 4.139881007202196e-05, 'epoch': 0.17}
{'loss': 0.0794, 'learning_rate': 3.087807214709459e-05, 'epoch': 0.38}
{'loss': 0.06, 'learning_rate': 3.1172850878809094e-05, 'epoch': 0.38}
{'loss': 0.0635, 'learning_rate': 4.937543756217989e-06, 'epoch': 0.9}
{'loss': 0.0341, 'learning_rate': 2.409816131766093e-05, 'epoch': 0.52}
{'loss': 0.0797, 'learning_rate': 4.1850098546666915e-05, 'epoch': 0.16}
{'loss': 0.0549, 'learning_rate': 2.3650101357577246e-06, 'epoch': 0.95}
{'loss': 0.077, 'learning_rate': 3.264674453738163e-05, 'epoch': 0.35}
{'loss': 0.0895, 'learning_rate': 4.383024185378254e-05, 'epoch': 0.12}
{'loss': 0.0546, 'learning_rate': 8.262178266478285e-06, 'epoch': 0.83}
{'loss': 0.0619, 'learning_rate': 4.7901543903607355e-06, 'epoch': 0.9}
{'loss': 0.0491, 'learning_rate': 4.073569231335998e-05, 'epoch': 0.19}
{'loss': 0.1355, 'learning_rate': 4.315791412625026e-05, 'epoch': 0.14}
{'loss': 0.0859, 'learning_rate': 4.132513032105952e-05, 'epoch': 0.17}
{'loss': 0.0896, 'learning_rate': 3.0730682781237337e-05, 'epoch': 0.39}
{'loss': 0.0871, 'learning_rate': 3.102546151295184e-05, 'epoch': 0.38}
{'loss': 0.0695, 'learning_rate': 4.642765024503482e-06, 'epoch': 0.91}
{'loss': 0.0592, 'learning_rate': 2.3950771951803677e-05, 'epoch': 0.52}
{'loss': 0.0612, 'learning_rate': 4.1776418795704476e-05, 'epoch': 0.17}
{'loss': 0.035, 'learning_rate': 2.1192947969777013e-06, 'epoch': 0.96}
{'loss': 0.07, 'learning_rate': 3.2499355171524375e-05, 'epoch': 0.35}
{'loss': 0.0617, 'learning_rate': 8.016462927698261e-06, 'epoch': 0.84}
{'loss': 0.1068, 'learning_rate': 4.37565621028201e-05, 'epoch': 0.13}
{'loss': 0.0511, 'learning_rate': 4.495375658646229e-06, 'epoch': 0.91}
{'loss': 0.0812, 'learning_rate': 4.0662012562397545e-05, 'epoch': 0.19}
{'loss': 0.0656, 'learning_rate': 4.3084234375287817e-05, 'epoch': 0.14}
{'loss': 0.1201, 'learning_rate': 4.1251450570097076e-05, 'epoch': 0.18}
{'loss': 0.0663, 'learning_rate': 3.058329341538008e-05, 'epoch': 0.39}
{'loss': 0.0739, 'learning_rate': 3.087807214709459e-05, 'epoch': 0.38}
{'eval_loss': 0.04981526732444763, 'eval_accuracy': 0.9834236972062023, 'eval_f1': 0.9834191163770769, 'eval_precision': 0.9871016955029298, 'eval_recall': 0.9797639123102867, 'eval_runtime': 363.9579, 'eval_samples_per_second': 159.123, 'eval_steps_per_second': 19.892, 'epoch': 1.0}
{'loss': 0.0539, 'learning_rate': 4.347986292788975e-06, 'epoch': 0.91}
{'loss': 0.0899, 'learning_rate': 2.3803382585946423e-05, 'epoch': 0.52}
{'loss': 0.0938, 'learning_rate': 4.170273904474203e-05, 'epoch': 0.17}
{'train_runtime': 5014.6377, 'train_samples_per_second': 20.788, 'train_steps_per_second': 0.325, 'train_loss': 0.08323603182921245, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0699, 'learning_rate': 4.3682882351857655e-05, 'epoch': 0.13}
{'loss': 0.0529, 'learning_rate': 1.873579458197678e-06, 'epoch': 0.96}
{'loss': 0.0663, 'learning_rate': 3.235196580566712e-05, 'epoch': 0.35}
{'loss': 0.0339, 'learning_rate': 7.770747588918239e-06, 'epoch': 0.84}
{'loss': 0.0519, 'learning_rate': 4.200596926931722e-06, 'epoch': 0.92}
{'loss': 0.075, 'learning_rate': 4.058833281143511e-05, 'epoch': 0.19}
{'loss': 0.0648, 'learning_rate': 4.301055462432537e-05, 'epoch': 0.14}
{'loss': 0.0776, 'learning_rate': 4.117777081913464e-05, 'epoch': 0.18}
{'loss': 0.0855, 'learning_rate': 3.043590404952283e-05, 'epoch': 0.39}
{'loss': 0.0808, 'learning_rate': 3.0730682781237337e-05, 'epoch': 0.39}
{'loss': 0.0242, 'learning_rate': 4.053207561074469e-06, 'epoch': 0.92}
{'loss': 0.0269, 'learning_rate': 2.365599322008917e-05, 'epoch': 0.53}
{'loss': 0.0541, 'learning_rate': 4.1629059293779594e-05, 'epoch': 0.17}
{'loss': 0.0484, 'learning_rate': 4.3609202600895217e-05, 'epoch': 0.13}
{'loss': 0.0393, 'learning_rate': 1.6278641194176547e-06, 'epoch': 0.97}
{'loss': 0.0747, 'learning_rate': 3.220457643980987e-05, 'epoch': 0.36}
{'loss': 0.0578, 'learning_rate': 7.5250322501382144e-06, 'epoch': 0.85}
{'loss': 0.073, 'learning_rate': 4.051465306047266e-05, 'epoch': 0.19}
{'loss': 0.0653, 'learning_rate': 3.905818195217216e-06, 'epoch': 0.92}
{'loss': 0.0895, 'learning_rate': 4.1104091068172194e-05, 'epoch': 0.18}
{'loss': 0.0652, 'learning_rate': 4.2936874873362934e-05, 'epoch': 0.14}
{'loss': 0.0718, 'learning_rate': 3.0288514683665575e-05, 'epoch': 0.39}
{'loss': 0.0807, 'learning_rate': 3.058329341538008e-05, 'epoch': 0.39}
{'loss': 0.0539, 'learning_rate': 3.758428829359962e-06, 'epoch': 0.93}
{'loss': 0.0518, 'learning_rate': 2.3508603854231916e-05, 'epoch': 0.53}
{'loss': 0.0455, 'learning_rate': 4.1555379542817156e-05, 'epoch': 0.17}
{'loss': 0.089, 'learning_rate': 4.353552284993277e-05, 'epoch': 0.13}
{'loss': 0.0847, 'learning_rate': 1.3821487806376311e-06, 'epoch': 0.97}
{'loss': 0.0879, 'learning_rate': 3.205718707395262e-05, 'epoch': 0.36}
{'loss': 0.0562, 'learning_rate': 4.044097330951022e-05, 'epoch': 0.19}
{'loss': 0.0537, 'learning_rate': 7.2793169113581916e-06, 'epoch': 0.85}
{'loss': 0.0404, 'learning_rate': 3.6110394635027085e-06, 'epoch': 0.93}
{'loss': 0.0712, 'learning_rate': 4.1030411317209756e-05, 'epoch': 0.18}
{'loss': 0.0614, 'learning_rate': 4.286319512240049e-05, 'epoch': 0.14}
{'loss': 0.0667, 'learning_rate': 3.014112531780832e-05, 'epoch': 0.4}
{'loss': 0.0783, 'learning_rate': 3.043590404952283e-05, 'epoch': 0.39}
{'loss': 0.0673, 'learning_rate': 3.4636500976454553e-06, 'epoch': 0.93}
{'loss': 0.0705, 'learning_rate': 2.3361214488374665e-05, 'epoch': 0.53}
{'loss': 0.1081, 'learning_rate': 4.1481699791854704e-05, 'epoch': 0.17}
{'loss': 0.0589, 'learning_rate': 4.346184309897033e-05, 'epoch': 0.13}
{'loss': 0.0357, 'learning_rate': 1.1364334418576078e-06, 'epoch': 0.98}
{'loss': 0.0466, 'learning_rate': 3.190979770809536e-05, 'epoch': 0.36}
{'loss': 0.0951, 'learning_rate': 4.036729355854777e-05, 'epoch': 0.19}
{'loss': 0.1073, 'learning_rate': 4.095673156624731e-05, 'epoch': 0.18}
{'loss': 0.0517, 'learning_rate': 7.033601572578168e-06, 'epoch': 0.86}
{'loss': 0.0409, 'learning_rate': 3.3162607317882017e-06, 'epoch': 0.93}
{'loss': 0.0807, 'learning_rate': 4.278951537143805e-05, 'epoch': 0.14}
{'loss': 0.0606, 'learning_rate': 2.999373595195107e-05, 'epoch': 0.4}
{'loss': 0.0591, 'learning_rate': 3.0288514683665575e-05, 'epoch': 0.39}
{'loss': 0.1101, 'learning_rate': 3.1688713659309486e-06, 'epoch': 0.94}
{'loss': 0.0727, 'learning_rate': 2.321382512251741e-05, 'epoch': 0.54}
{'loss': 0.0811, 'learning_rate': 4.338816334800789e-05, 'epoch': 0.13}
{'loss': 0.0873, 'learning_rate': 4.1408020040892266e-05, 'epoch': 0.17}
{'loss': 0.0745, 'learning_rate': 3.176240834223811e-05, 'epoch': 0.37}
{'loss': 0.0588, 'learning_rate': 8.907181030775846e-07, 'epoch': 0.98}
{'loss': 0.0969, 'learning_rate': 4.0293613807585334e-05, 'epoch': 0.19}
{'loss': 0.096, 'learning_rate': 4.0883051815284866e-05, 'epoch': 0.18}
{'loss': 0.036, 'learning_rate': 3.0214820000736945e-06, 'epoch': 0.94}
{'loss': 0.0364, 'learning_rate': 6.787886233798145e-06, 'epoch': 0.86}
{'loss': 0.0964, 'learning_rate': 4.2715835620475606e-05, 'epoch': 0.15}
{'loss': 0.1004, 'learning_rate': 2.9846346586093814e-05, 'epoch': 0.4}
{'loss': 0.0711, 'learning_rate': 2.8740926342164414e-06, 'epoch': 0.94}
{'loss': 0.0487, 'learning_rate': 3.014112531780832e-05, 'epoch': 0.4}
{'loss': 0.0446, 'learning_rate': 2.306643575666016e-05, 'epoch': 0.54}
{'loss': 0.1044, 'learning_rate': 4.3314483597045444e-05, 'epoch': 0.13}
{'loss': 0.0503, 'learning_rate': 4.133434028992982e-05, 'epoch': 0.17}
{'loss': 0.0837, 'learning_rate': 4.0219934056622896e-05, 'epoch': 0.2}
{'loss': 0.0695, 'learning_rate': 3.161501897638086e-05, 'epoch': 0.37}
{'loss': 0.0414, 'learning_rate': 6.450027642975612e-07, 'epoch': 0.99}
{'loss': 0.0738, 'learning_rate': 4.080937206432243e-05, 'epoch': 0.18}
{'loss': 0.0636, 'learning_rate': 2.726703268359188e-06, 'epoch': 0.95}
{'loss': 0.0699, 'learning_rate': 4.264215586951317e-05, 'epoch': 0.15}
{'loss': 0.0312, 'learning_rate': 6.542170895018121e-06, 'epoch': 0.87}
{'loss': 0.0735, 'learning_rate': 2.9698957220236563e-05, 'epoch': 0.41}
{'loss': 0.0346, 'learning_rate': 2.5793139025019346e-06, 'epoch': 0.95}
{'loss': 0.1221, 'learning_rate': 4.3240803846083006e-05, 'epoch': 0.14}
{'loss': 0.0686, 'learning_rate': 2.999373595195107e-05, 'epoch': 0.4}
{'loss': 0.0415, 'learning_rate': 2.2919046390802904e-05, 'epoch': 0.54}
{'loss': 0.0551, 'learning_rate': 4.126066053896738e-05, 'epoch': 0.18}
{'loss': 0.0551, 'learning_rate': 4.014625430566045e-05, 'epoch': 0.2}
{'loss': 0.0697, 'learning_rate': 3.99287425517538e-07, 'epoch': 0.99}
{'loss': 0.0493, 'learning_rate': 3.1467629610523604e-05, 'epoch': 0.37}
{'loss': 0.0686, 'learning_rate': 4.073569231335998e-05, 'epoch': 0.19}
{'loss': 0.0384, 'learning_rate': 2.431924536644681e-06, 'epoch': 0.95}
{'loss': 0.0477, 'learning_rate': 6.296455556238098e-06, 'epoch': 0.87}
{'loss': 0.0485, 'learning_rate': 4.256847611855073e-05, 'epoch': 0.15}
{'loss': 0.0473, 'learning_rate': 2.9551567854379308e-05, 'epoch': 0.41}
{'loss': 0.0534, 'learning_rate': 2.284535170787428e-06, 'epoch': 0.95}
{'loss': 0.0863, 'learning_rate': 4.316712409512057e-05, 'epoch': 0.14}
{'loss': 0.0877, 'learning_rate': 2.9846346586093814e-05, 'epoch': 0.4}
{'loss': 0.0796, 'learning_rate': 2.277165702494565e-05, 'epoch': 0.55}
{'loss': 0.0713, 'learning_rate': 4.1186980788004945e-05, 'epoch': 0.18}
{'loss': 0.074, 'learning_rate': 4.007257455469801e-05, 'epoch': 0.2}
{'loss': 0.054, 'learning_rate': 3.132024024466635e-05, 'epoch': 0.37}
{'loss': 0.0643, 'learning_rate': 1.535720867375146e-07, 'epoch': 1.0}
{'loss': 0.0526, 'learning_rate': 2.1371458049301743e-06, 'epoch': 0.96}
{'loss': 0.0599, 'learning_rate': 4.0662012562397545e-05, 'epoch': 0.19}
{'loss': 0.0799, 'learning_rate': 4.249479636758828e-05, 'epoch': 0.15}
{'loss': 0.0633, 'learning_rate': 6.0507402174580754e-06, 'epoch': 0.88}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0331, 'learning_rate': 1.989756439072921e-06, 'epoch': 0.96}
{'loss': 0.0722, 'learning_rate': 2.9404178488522056e-05, 'epoch': 0.41}
{'loss': 0.1328, 'learning_rate': 4.309344434415812e-05, 'epoch': 0.14}
{'loss': 0.0938, 'learning_rate': 2.9698957220236563e-05, 'epoch': 0.41}
{'loss': 0.0686, 'learning_rate': 2.2624267659088397e-05, 'epoch': 0.55}
{'loss': 0.0557, 'learning_rate': 3.999889480373557e-05, 'epoch': 0.2}
{'loss': 0.1141, 'learning_rate': 4.11133010370425e-05, 'epoch': 0.18}
{'loss': 0.0868, 'learning_rate': 3.1172850878809094e-05, 'epoch': 0.38}
{'loss': 0.0698, 'learning_rate': 4.058833281143511e-05, 'epoch': 0.19}
{'loss': 0.0869, 'learning_rate': 1.8423670732156675e-06, 'epoch': 0.96}
{'loss': 0.0945, 'learning_rate': 4.242111661662584e-05, 'epoch': 0.15}
{'loss': 0.0502, 'learning_rate': 5.805024878678052e-06, 'epoch': 0.88}
{'loss': 0.1189, 'learning_rate': 4.301976459319568e-05, 'epoch': 0.14}
{'loss': 0.0456, 'learning_rate': 1.6949777073584142e-06, 'epoch': 0.97}
{'loss': 0.0616, 'learning_rate': 2.92567891226648e-05, 'epoch': 0.42}
{'loss': 0.0867, 'learning_rate': 3.9925215052773124e-05, 'epoch': 0.2}
{'loss': 0.0857, 'learning_rate': 2.2476878293231142e-05, 'epoch': 0.55}
{'loss': 0.0395, 'learning_rate': 2.9551567854379308e-05, 'epoch': 0.41}
{'loss': 0.0689, 'learning_rate': 4.103962128608006e-05, 'epoch': 0.18}
{'loss': 0.0381, 'learning_rate': 3.102546151295184e-05, 'epoch': 0.38}
{'loss': 0.0926, 'learning_rate': 4.051465306047266e-05, 'epoch': 0.19}
{'loss': 0.046, 'learning_rate': 1.5475883415011608e-06, 'epoch': 0.97}
{'loss': 0.1047, 'learning_rate': 4.2347436865663395e-05, 'epoch': 0.15}
{'loss': 0.0689, 'learning_rate': 5.559309539898029e-06, 'epoch': 0.89}
{'loss': 0.0882, 'learning_rate': 4.294608484223323e-05, 'epoch': 0.14}
{'loss': 0.0466, 'learning_rate': 1.4001989756439072e-06, 'epoch': 0.97}
{'loss': 0.0599, 'learning_rate': 2.9109399756807547e-05, 'epoch': 0.42}
{'loss': 0.0469, 'learning_rate': 3.9851535301810685e-05, 'epoch': 0.2}
{'loss': 0.0305, 'learning_rate': 2.232948892737389e-05, 'epoch': 0.55}
{'loss': 0.0678, 'learning_rate': 2.9404178488522056e-05, 'epoch': 0.41}
{'loss': 0.0503, 'learning_rate': 4.096594153511761e-05, 'epoch': 0.18}
{'loss': 0.0779, 'learning_rate': 4.044097330951022e-05, 'epoch': 0.19}
{'loss': 0.0928, 'learning_rate': 3.087807214709459e-05, 'epoch': 0.38}
{'loss': 0.0515, 'learning_rate': 1.2528096097866538e-06, 'epoch': 0.98}
{'loss': 0.0676, 'learning_rate': 4.227375711470096e-05, 'epoch': 0.15}
{'loss': 0.101, 'learning_rate': 4.2872405091270795e-05, 'epoch': 0.14}
{'loss': 0.0553, 'learning_rate': 5.313594201118004e-06, 'epoch': 0.89}
{'loss': 0.0823, 'learning_rate': 3.977785555084824e-05, 'epoch': 0.2}
{'loss': 0.0614, 'learning_rate': 1.1054202439294004e-06, 'epoch': 0.98}
{'loss': 0.0515, 'learning_rate': 2.8962010390950295e-05, 'epoch': 0.42}
{'loss': 0.0474, 'learning_rate': 2.2182099561516636e-05, 'epoch': 0.56}
{'loss': 0.0621, 'learning_rate': 2.92567891226648e-05, 'epoch': 0.42}
{'loss': 0.0677, 'learning_rate': 4.089226178415517e-05, 'epoch': 0.18}
{'loss': 0.0443, 'learning_rate': 4.036729355854777e-05, 'epoch': 0.19}
{'loss': 0.0626, 'learning_rate': 3.0730682781237337e-05, 'epoch': 0.39}
{'loss': 0.0757, 'learning_rate': 9.58030878072147e-07, 'epoch': 0.98}
{'loss': 0.0855, 'learning_rate': 4.220007736373852e-05, 'epoch': 0.16}
{'loss': 0.0636, 'learning_rate': 4.279872534030836e-05, 'epoch': 0.14}
{'loss': 0.0642, 'learning_rate': 3.97041757998858e-05, 'epoch': 0.21}
{'loss': 0.0788, 'learning_rate': 8.106415122148938e-07, 'epoch': 0.98}
{'loss': 0.0466, 'learning_rate': 5.067878862337981e-06, 'epoch': 0.9}
{'loss': 0.0953, 'learning_rate': 2.881462102509304e-05, 'epoch': 0.42}
{'loss': 0.0583, 'learning_rate': 2.2034710195659385e-05, 'epoch': 0.56}
{'loss': 0.1188, 'learning_rate': 2.9109399756807547e-05, 'epoch': 0.42}
{'loss': 0.0559, 'learning_rate': 4.0818582033192734e-05, 'epoch': 0.18}
{'loss': 0.104, 'learning_rate': 4.0293613807585334e-05, 'epoch': 0.19}
{'loss': 0.0866, 'learning_rate': 3.058329341538008e-05, 'epoch': 0.39}
{'loss': 0.0605, 'learning_rate': 6.632521463576403e-07, 'epoch': 0.99}
{'loss': 0.0836, 'learning_rate': 4.2126397612776074e-05, 'epoch': 0.16}
{'loss': 0.0647, 'learning_rate': 4.272504558934591e-05, 'epoch': 0.15}
{'loss': 0.0421, 'learning_rate': 3.963049604892336e-05, 'epoch': 0.21}
{'loss': 0.0672, 'learning_rate': 4.822163523557958e-06, 'epoch': 0.9}
{'loss': 0.049, 'learning_rate': 5.158627805003869e-07, 'epoch': 0.99}
{'loss': 0.0674, 'learning_rate': 2.866723165923579e-05, 'epoch': 0.43}
{'loss': 0.0659, 'learning_rate': 2.188732082980213e-05, 'epoch': 0.56}
{'loss': 0.0721, 'learning_rate': 2.8962010390950295e-05, 'epoch': 0.42}
{'loss': 0.1006, 'learning_rate': 4.074490228223029e-05, 'epoch': 0.19}
{'loss': 0.1045, 'learning_rate': 3.043590404952283e-05, 'epoch': 0.39}
{'loss': 0.0519, 'learning_rate': 4.0219934056622896e-05, 'epoch': 0.2}
{'loss': 0.0378, 'learning_rate': 3.684734146431335e-07, 'epoch': 0.99}
{'loss': 0.0731, 'learning_rate': 4.2052717861813636e-05, 'epoch': 0.16}
{'loss': 0.0983, 'learning_rate': 4.2651365838383474e-05, 'epoch': 0.15}
{'loss': 0.0533, 'learning_rate': 3.955681629796092e-05, 'epoch': 0.21}
{'loss': 0.0509, 'learning_rate': 2.2108404878588012e-07, 'epoch': 1.0}
{'loss': 0.0533, 'learning_rate': 4.576448184777935e-06, 'epoch': 0.91}
{'loss': 0.0718, 'learning_rate': 2.8519842293378534e-05, 'epoch': 0.43}
{'loss': 0.0467, 'learning_rate': 2.1739931463944875e-05, 'epoch': 0.57}
{'loss': 0.0426, 'learning_rate': 2.881462102509304e-05, 'epoch': 0.42}
{'loss': 0.0347, 'learning_rate': 4.067122253126785e-05, 'epoch': 0.19}
{'loss': 0.0477, 'learning_rate': 3.0288514683665575e-05, 'epoch': 0.39}
{'loss': 0.088, 'learning_rate': 4.014625430566045e-05, 'epoch': 0.2}
{'loss': 0.0731, 'learning_rate': 7.36946829286267e-08, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1193, 'learning_rate': 4.257768608742103e-05, 'epoch': 0.15}
{'loss': 0.081, 'learning_rate': 4.1979038110851185e-05, 'epoch': 0.16}
{'loss': 0.0778, 'learning_rate': 3.9483136546998475e-05, 'epoch': 0.21}
{'loss': 0.0498, 'learning_rate': 2.8372452927521283e-05, 'epoch': 0.43}
{'loss': 0.0426, 'learning_rate': 4.330732845997911e-06, 'epoch': 0.91}
{'loss': 0.0583, 'learning_rate': 2.1592542098087623e-05, 'epoch': 0.57}
{'loss': 0.0546, 'learning_rate': 2.866723165923579e-05, 'epoch': 0.43}
{'loss': 0.0825, 'learning_rate': 4.0597542780305406e-05, 'epoch': 0.19}
{'loss': 0.0269, 'learning_rate': 3.014112531780832e-05, 'epoch': 0.4}
{'loss': 0.08, 'learning_rate': 4.007257455469801e-05, 'epoch': 0.2}
{'loss': 0.0901, 'learning_rate': 4.2504006336458585e-05, 'epoch': 0.15}
{'loss': 0.0737, 'learning_rate': 3.940945679603603e-05, 'epoch': 0.21}
{'loss': 0.0865, 'learning_rate': 4.1905358359888746e-05, 'epoch': 0.16}
{'loss': 0.0873, 'learning_rate': 2.8225063561664028e-05, 'epoch': 0.44}
{'loss': 0.0645, 'learning_rate': 2.144515273223037e-05, 'epoch': 0.57}
{'loss': 0.0758, 'learning_rate': 4.085017507217888e-06, 'epoch': 0.92}
{'loss': 0.0778, 'learning_rate': 2.8519842293378534e-05, 'epoch': 0.43}
{'loss': 0.0953, 'learning_rate': 2.999373595195107e-05, 'epoch': 0.4}
{'loss': 0.0791, 'learning_rate': 4.052386302934297e-05, 'epoch': 0.19}
{'loss': 0.0831, 'learning_rate': 3.999889480373557e-05, 'epoch': 0.2}
{'loss': 0.0474, 'learning_rate': 4.2430326585496146e-05, 'epoch': 0.15}
{'loss': 0.0457, 'learning_rate': 3.933577704507359e-05, 'epoch': 0.21}
{'loss': 0.0836, 'learning_rate': 4.183167860892631e-05, 'epoch': 0.16}
{'loss': 0.0592, 'learning_rate': 2.8077674195806776e-05, 'epoch': 0.44}
{'loss': 0.0775, 'learning_rate': 2.1297763366373117e-05, 'epoch': 0.57}
{'loss': 0.0766, 'learning_rate': 2.8372452927521283e-05, 'epoch': 0.43}
{'loss': 0.07, 'learning_rate': 3.839302168437864e-06, 'epoch': 0.92}
{'loss': 0.0687, 'learning_rate': 2.9846346586093814e-05, 'epoch': 0.4}
{'loss': 0.0729, 'learning_rate': 4.0450183278380524e-05, 'epoch': 0.19}
{'loss': 0.0861, 'learning_rate': 3.9925215052773124e-05, 'epoch': 0.2}
{'loss': 0.0981, 'learning_rate': 4.23566468345337e-05, 'epoch': 0.15}
{'loss': 0.0841, 'learning_rate': 3.926209729411115e-05, 'epoch': 0.22}
{'loss': 0.0702, 'learning_rate': 4.1757998857963864e-05, 'epoch': 0.17}
{'loss': 0.1033, 'learning_rate': 2.793028482994952e-05, 'epoch': 0.44}
{'loss': 0.0486, 'learning_rate': 2.1150374000515862e-05, 'epoch': 0.58}
{'loss': 0.0792, 'learning_rate': 2.8225063561664028e-05, 'epoch': 0.44}
{'loss': 0.0386, 'learning_rate': 3.593586829657841e-06, 'epoch': 0.93}
{'loss': 0.0573, 'learning_rate': 2.9698957220236563e-05, 'epoch': 0.41}
{'loss': 0.0608, 'learning_rate': 4.037650352741808e-05, 'epoch': 0.19}
{'loss': 0.0905, 'learning_rate': 3.9851535301810685e-05, 'epoch': 0.2}
{'loss': 0.0789, 'learning_rate': 4.2282967083571264e-05, 'epoch': 0.15}
{'loss': 0.0988, 'learning_rate': 3.918841754314871e-05, 'epoch': 0.22}
{'loss': 0.0712, 'learning_rate': 4.1684319107001426e-05, 'epoch': 0.17}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0737, 'learning_rate': 2.7782895464092267e-05, 'epoch': 0.44}
{'loss': 0.0802, 'learning_rate': 2.100298463465861e-05, 'epoch': 0.58}
{'loss': 0.0342, 'learning_rate': 2.8077674195806776e-05, 'epoch': 0.44}
{'loss': 0.1196, 'learning_rate': 2.9551567854379308e-05, 'epoch': 0.41}
{'loss': 0.0785, 'learning_rate': 3.911473779218627e-05, 'epoch': 0.22}
{'loss': 0.0826, 'learning_rate': 4.220928733260882e-05, 'epoch': 0.16}
{'loss': 0.0667, 'learning_rate': 3.347871490877818e-06, 'epoch': 0.93}
{'loss': 0.0753, 'learning_rate': 4.030282377645564e-05, 'epoch': 0.19}
{'loss': 0.0697, 'learning_rate': 3.977785555084824e-05, 'epoch': 0.2}
{'loss': 0.0699, 'learning_rate': 4.161063935603898e-05, 'epoch': 0.17}
{'loss': 0.0259, 'learning_rate': 2.7635506098235015e-05, 'epoch': 0.45}
{'loss': 0.0506, 'learning_rate': 2.0855595268801356e-05, 'epoch': 0.58}
{'loss': 0.0681, 'learning_rate': 2.793028482994952e-05, 'epoch': 0.44}
{'loss': 0.067, 'learning_rate': 3.9041058041223826e-05, 'epoch': 0.22}
{'loss': 0.0667, 'learning_rate': 4.213560758164638e-05, 'epoch': 0.16}
{'loss': 0.0582, 'learning_rate': 2.9404178488522056e-05, 'epoch': 0.41}
{'loss': 0.0766, 'learning_rate': 3.97041757998858e-05, 'epoch': 0.21}
{'loss': 0.0846, 'learning_rate': 4.0229144025493196e-05, 'epoch': 0.2}
{'loss': 0.0617, 'learning_rate': 3.1021561520977945e-06, 'epoch': 0.94}
{'loss': 0.0842, 'learning_rate': 4.153695960507654e-05, 'epoch': 0.17}
{'loss': 0.0562, 'learning_rate': 2.748811673237776e-05, 'epoch': 0.45}
{'loss': 0.0486, 'learning_rate': 2.0708205902944104e-05, 'epoch': 0.59}
{'loss': 0.1127, 'learning_rate': 3.896737829026139e-05, 'epoch': 0.22}
{'loss': 0.0624, 'learning_rate': 4.206192783068394e-05, 'epoch': 0.16}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0595, 'learning_rate': 2.7782895464092267e-05, 'epoch': 0.44}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0669, 'learning_rate': 2.92567891226648e-05, 'epoch': 0.42}
{'loss': 0.072, 'learning_rate': 4.015546427453076e-05, 'epoch': 0.2}
{'loss': 0.0772, 'learning_rate': 3.963049604892336e-05, 'epoch': 0.21}
{'loss': 0.0238, 'learning_rate': 2.856440813317771e-06, 'epoch': 0.94}
{'loss': 0.1467, 'learning_rate': 4.14632798541141e-05, 'epoch': 0.17}
{'loss': 0.0686, 'learning_rate': 3.8893698539298936e-05, 'epoch': 0.22}
{'loss': 0.0667, 'learning_rate': 2.734072736652051e-05, 'epoch': 0.45}
{'loss': 0.0603, 'learning_rate': 4.198824807972149e-05, 'epoch': 0.16}
{'loss': 0.0508, 'learning_rate': 2.056081653708685e-05, 'epoch': 0.59}
{'loss': 0.0605, 'learning_rate': 2.7635506098235015e-05, 'epoch': 0.45}
{'loss': 0.0563, 'learning_rate': 2.9109399756807547e-05, 'epoch': 0.42}
{'loss': 0.0435, 'learning_rate': 3.955681629796092e-05, 'epoch': 0.21}
{'loss': 0.0912, 'learning_rate': 4.008178452356832e-05, 'epoch': 0.2}
{'loss': 0.0763, 'learning_rate': 2.610725474537748e-06, 'epoch': 0.95}
{'loss': 0.0606, 'learning_rate': 4.138960010315165e-05, 'epoch': 0.17}
{'loss': 0.0649, 'learning_rate': 3.88200187883365e-05, 'epoch': 0.22}
{'loss': 0.0757, 'learning_rate': 4.191456832875905e-05, 'epoch': 0.16}
{'loss': 0.0948, 'learning_rate': 2.7193338000663254e-05, 'epoch': 0.46}
{'loss': 0.0537, 'learning_rate': 2.0413427171229595e-05, 'epoch': 0.59}
{'loss': 0.052, 'learning_rate': 2.748811673237776e-05, 'epoch': 0.45}
{'loss': 0.1316, 'learning_rate': 2.8962010390950295e-05, 'epoch': 0.42}
{'loss': 0.0614, 'learning_rate': 3.9483136546998475e-05, 'epoch': 0.21}
{'loss': 0.0691, 'learning_rate': 4.0008104772605875e-05, 'epoch': 0.2}
{'loss': 0.0718, 'learning_rate': 4.1315920352189215e-05, 'epoch': 0.17}
{'loss': 0.0666, 'learning_rate': 2.3650101357577246e-06, 'epoch': 0.95}
{'eval_loss': 0.05052286013960838, 'eval_accuracy': 0.9836309009911248, 'eval_f1': 0.9836110919023582, 'eval_precision': 0.988223851043874, 'eval_recall': 0.979041194892797, 'eval_runtime': 367.4814, 'eval_samples_per_second': 157.597, 'eval_steps_per_second': 19.702, 'epoch': 1.0}
{'loss': 0.0796, 'learning_rate': 3.874633903737406e-05, 'epoch': 0.23}
{'train_runtime': 5148.7292, 'train_samples_per_second': 20.247, 'train_steps_per_second': 0.316, 'train_loss': 0.07930482879592247, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0654, 'learning_rate': 4.184088857779661e-05, 'epoch': 0.16}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0986, 'learning_rate': 2.7045948634806002e-05, 'epoch': 0.46}
{'eval_loss': 0.045663002878427505, 'eval_accuracy': 0.9844251821666609, 'eval_f1': 0.9844111852338322, 'eval_precision': 0.9887172365908696, 'eval_recall': 0.9801424785765909, 'eval_runtime': 220.3077, 'eval_samples_per_second': 262.878, 'eval_steps_per_second': 32.863, 'epoch': 1.0}
{'loss': 0.0298, 'learning_rate': 2.0266037805372343e-05, 'epoch': 0.6}
{'train_runtime': 4150.6398, 'train_samples_per_second': 41.859, 'train_steps_per_second': 0.654, 'train_loss': 0.0737169425681991, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0652, 'learning_rate': 2.734072736652051e-05, 'epoch': 0.45}
{'loss': 0.0569, 'learning_rate': 2.881462102509304e-05, 'epoch': 0.42}
{'loss': 0.0464, 'learning_rate': 3.940945679603603e-05, 'epoch': 0.21}
{'loss': 0.0967, 'learning_rate': 3.993442502164343e-05, 'epoch': 0.2}
{'loss': 0.071, 'learning_rate': 4.124224060122677e-05, 'epoch': 0.18}
{'loss': 0.0512, 'learning_rate': 2.1192947969777013e-06, 'epoch': 0.96}
{'loss': 0.1064, 'learning_rate': 3.8672659286411615e-05, 'epoch': 0.23}
{'loss': 0.0434, 'learning_rate': 4.176720882683417e-05, 'epoch': 0.17}
{'loss': 0.0596, 'learning_rate': 2.6898559268948748e-05, 'epoch': 0.46}
{'loss': 0.0835, 'learning_rate': 2.011864843951509e-05, 'epoch': 0.6}
{'loss': 0.0524, 'learning_rate': 2.7193338000663254e-05, 'epoch': 0.46}
{'loss': 0.0785, 'learning_rate': 2.866723165923579e-05, 'epoch': 0.43}
{'loss': 0.0872, 'learning_rate': 3.9860745270680985e-05, 'epoch': 0.2}
{'loss': 0.0674, 'learning_rate': 3.933577704507359e-05, 'epoch': 0.21}
{'loss': 0.0951, 'learning_rate': 4.116856085026433e-05, 'epoch': 0.18}
{'loss': 0.0626, 'learning_rate': 1.873579458197678e-06, 'epoch': 0.96}
{'loss': 0.0542, 'learning_rate': 3.859897953544918e-05, 'epoch': 0.23}
{'loss': 0.077, 'learning_rate': 4.169352907587173e-05, 'epoch': 0.17}
{'loss': 0.0602, 'learning_rate': 2.6751169903091493e-05, 'epoch': 0.47}
{'loss': 0.0656, 'learning_rate': 1.9971259073657837e-05, 'epoch': 0.6}
{'loss': 0.0621, 'learning_rate': 2.7045948634806002e-05, 'epoch': 0.46}
{'loss': 0.0658, 'learning_rate': 2.8519842293378534e-05, 'epoch': 0.43}
{'loss': 0.0678, 'learning_rate': 3.978706551971855e-05, 'epoch': 0.2}
{'loss': 0.0674, 'learning_rate': 3.926209729411115e-05, 'epoch': 0.22}
{'loss': 0.1073, 'learning_rate': 4.1094881099301894e-05, 'epoch': 0.18}
{'loss': 0.0406, 'learning_rate': 1.6278641194176547e-06, 'epoch': 0.97}
{'loss': 0.0369, 'learning_rate': 3.852529978448673e-05, 'epoch': 0.23}
{'loss': 0.0519, 'learning_rate': 4.161984932490929e-05, 'epoch': 0.17}
{'loss': 0.0676, 'learning_rate': 2.660378053723424e-05, 'epoch': 0.47}
{'loss': 0.0524, 'learning_rate': 1.9823869707800582e-05, 'epoch': 0.6}
{'loss': 0.0739, 'learning_rate': 2.6898559268948748e-05, 'epoch': 0.46}
{'loss': 0.0825, 'learning_rate': 2.8372452927521283e-05, 'epoch': 0.43}
{'loss': 0.0661, 'learning_rate': 3.971338576875611e-05, 'epoch': 0.21}
{'loss': 0.066, 'learning_rate': 3.918841754314871e-05, 'epoch': 0.22}
{'loss': 0.083, 'learning_rate': 4.102120134833945e-05, 'epoch': 0.18}
{'loss': 0.102, 'learning_rate': 3.8451620033524294e-05, 'epoch': 0.23}
{'loss': 0.0541, 'learning_rate': 1.3821487806376311e-06, 'epoch': 0.97}
{'loss': 0.0593, 'learning_rate': 4.154616957394685e-05, 'epoch': 0.17}
{'loss': 0.021, 'learning_rate': 2.6456391171376986e-05, 'epoch': 0.47}
{'loss': 0.0562, 'learning_rate': 1.967648034194333e-05, 'epoch': 0.61}
{'loss': 0.0799, 'learning_rate': 2.6751169903091493e-05, 'epoch': 0.47}
{'loss': 0.0628, 'learning_rate': 2.8225063561664028e-05, 'epoch': 0.44}
{'loss': 0.0518, 'learning_rate': 3.911473779218627e-05, 'epoch': 0.22}
{'loss': 0.0809, 'learning_rate': 3.9639706017793664e-05, 'epoch': 0.21}
{'loss': 0.0423, 'learning_rate': 4.0947521597377004e-05, 'epoch': 0.18}
{'loss': 0.0637, 'learning_rate': 3.837794028256185e-05, 'epoch': 0.23}
{'loss': 0.0542, 'learning_rate': 4.14724898229844e-05, 'epoch': 0.17}
{'loss': 0.0361, 'learning_rate': 1.1364334418576078e-06, 'epoch': 0.98}
{'loss': 0.0431, 'learning_rate': 2.6309001805519735e-05, 'epoch': 0.47}
{'loss': 0.053, 'learning_rate': 1.9529090976086076e-05, 'epoch': 0.61}
{'loss': 0.0449, 'learning_rate': 2.660378053723424e-05, 'epoch': 0.47}
{'loss': 0.0458, 'learning_rate': 2.8077674195806776e-05, 'epoch': 0.44}
{'loss': 0.0432, 'learning_rate': 3.9041058041223826e-05, 'epoch': 0.22}
{'loss': 0.1047, 'learning_rate': 3.9566026266831226e-05, 'epoch': 0.21}
{'loss': 0.0634, 'learning_rate': 4.087384184641456e-05, 'epoch': 0.18}
{'loss': 0.0587, 'learning_rate': 3.8304260531599405e-05, 'epoch': 0.23}
{'loss': 0.1054, 'learning_rate': 4.139881007202196e-05, 'epoch': 0.17}
{'loss': 0.0456, 'learning_rate': 8.907181030775846e-07, 'epoch': 0.98}
{'loss': 0.0665, 'learning_rate': 2.616161243966248e-05, 'epoch': 0.48}
{'loss': 0.0425, 'learning_rate': 1.938170161022882e-05, 'epoch': 0.61}
{'loss': 0.0589, 'learning_rate': 2.6456391171376986e-05, 'epoch': 0.47}
{'loss': 0.07, 'learning_rate': 2.793028482994952e-05, 'epoch': 0.44}
{'loss': 0.0789, 'learning_rate': 3.896737829026139e-05, 'epoch': 0.22}
{'loss': 0.0803, 'learning_rate': 3.949234651586878e-05, 'epoch': 0.21}
{'loss': 0.0328, 'learning_rate': 4.080016209545212e-05, 'epoch': 0.18}
{'loss': 0.066, 'learning_rate': 3.823058078063697e-05, 'epoch': 0.24}
{'loss': 0.1135, 'learning_rate': 4.132513032105952e-05, 'epoch': 0.17}
{'loss': 0.0265, 'learning_rate': 6.450027642975612e-07, 'epoch': 0.99}
{'loss': 0.0835, 'learning_rate': 2.601422307380523e-05, 'epoch': 0.48}
{'loss': 0.1032, 'learning_rate': 1.923431224437157e-05, 'epoch': 0.62}
{'loss': 0.1136, 'learning_rate': 2.7782895464092267e-05, 'epoch': 0.44}
{'loss': 0.0716, 'learning_rate': 2.6309001805519735e-05, 'epoch': 0.47}
{'loss': 0.0666, 'learning_rate': 3.8893698539298936e-05, 'epoch': 0.22}
{'loss': 0.0464, 'learning_rate': 3.9418666764906336e-05, 'epoch': 0.21}
{'loss': 0.1491, 'learning_rate': 4.072648234448968e-05, 'epoch': 0.19}
{'loss': 0.078, 'learning_rate': 3.815690102967452e-05, 'epoch': 0.24}
{'loss': 0.0522, 'learning_rate': 4.1251450570097076e-05, 'epoch': 0.18}
{'loss': 0.0447, 'learning_rate': 3.99287425517538e-07, 'epoch': 0.99}
{'loss': 0.0782, 'learning_rate': 2.5866833707947974e-05, 'epoch': 0.48}
{'loss': 0.0644, 'learning_rate': 1.9086922878514315e-05, 'epoch': 0.62}
{'loss': 0.0523, 'learning_rate': 2.7635506098235015e-05, 'epoch': 0.45}
{'loss': 0.0529, 'learning_rate': 2.616161243966248e-05, 'epoch': 0.48}
{'loss': 0.0746, 'learning_rate': 3.8083221278712084e-05, 'epoch': 0.24}
{'loss': 0.0361, 'learning_rate': 3.88200187883365e-05, 'epoch': 0.22}
{'loss': 0.1092, 'learning_rate': 4.065280259352724e-05, 'epoch': 0.19}
{'loss': 0.0926, 'learning_rate': 3.93449870139439e-05, 'epoch': 0.21}
{'loss': 0.0507, 'learning_rate': 4.117777081913464e-05, 'epoch': 0.18}
{'loss': 0.0431, 'learning_rate': 1.535720867375146e-07, 'epoch': 1.0}
{'loss': 0.0502, 'learning_rate': 2.748811673237776e-05, 'epoch': 0.45}
{'loss': 0.0517, 'learning_rate': 2.5719444342090722e-05, 'epoch': 0.49}
{'loss': 0.0461, 'learning_rate': 1.8939533512657063e-05, 'epoch': 0.62}
{'loss': 0.0639, 'learning_rate': 3.8009541527749646e-05, 'epoch': 0.24}
{'loss': 0.0806, 'learning_rate': 2.601422307380523e-05, 'epoch': 0.48}
{'loss': 0.0845, 'learning_rate': 3.874633903737406e-05, 'epoch': 0.23}
{'loss': 0.0505, 'learning_rate': 4.05791228425648e-05, 'epoch': 0.19}
{'loss': 0.0638, 'learning_rate': 3.9271307262981454e-05, 'epoch': 0.22}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0958, 'learning_rate': 4.1104091068172194e-05, 'epoch': 0.18}
{'loss': 0.0648, 'learning_rate': 2.734072736652051e-05, 'epoch': 0.45}
{'loss': 0.0406, 'learning_rate': 2.5572054976233467e-05, 'epoch': 0.49}
{'loss': 0.0428, 'learning_rate': 1.8792144146799808e-05, 'epoch': 0.62}
{'loss': 0.0716, 'learning_rate': 3.79358617767872e-05, 'epoch': 0.24}
{'loss': 0.0685, 'learning_rate': 2.5866833707947974e-05, 'epoch': 0.48}
{'loss': 0.0845, 'learning_rate': 3.8672659286411615e-05, 'epoch': 0.23}
{'loss': 0.0947, 'learning_rate': 4.0505443091602356e-05, 'epoch': 0.19}
{'loss': 0.0636, 'learning_rate': 4.1030411317209756e-05, 'epoch': 0.18}
{'loss': 0.0523, 'learning_rate': 3.9197627512019015e-05, 'epoch': 0.22}
{'loss': 0.0674, 'learning_rate': 2.7193338000663254e-05, 'epoch': 0.46}
{'loss': 0.0728, 'learning_rate': 2.5424665610376212e-05, 'epoch': 0.49}
{'loss': 0.0868, 'learning_rate': 3.7862182025824756e-05, 'epoch': 0.24}
{'loss': 0.0406, 'learning_rate': 1.8644754780942557e-05, 'epoch': 0.63}
{'loss': 0.0588, 'learning_rate': 4.043176334063991e-05, 'epoch': 0.19}
{'loss': 0.0634, 'learning_rate': 4.095673156624731e-05, 'epoch': 0.18}
{'loss': 0.043, 'learning_rate': 2.5719444342090722e-05, 'epoch': 0.49}
{'loss': 0.047, 'learning_rate': 3.859897953544918e-05, 'epoch': 0.23}
{'loss': 0.0548, 'learning_rate': 3.912394776105657e-05, 'epoch': 0.22}
{'loss': 0.0741, 'learning_rate': 3.778850227486231e-05, 'epoch': 0.24}
{'loss': 0.0585, 'learning_rate': 2.7045948634806002e-05, 'epoch': 0.46}
{'loss': 0.0945, 'learning_rate': 2.527727624451896e-05, 'epoch': 0.5}
{'loss': 0.0404, 'learning_rate': 1.8497365415085302e-05, 'epoch': 0.63}
{'loss': 0.0791, 'learning_rate': 4.0883051815284866e-05, 'epoch': 0.18}
{'loss': 0.0921, 'learning_rate': 4.035808358967747e-05, 'epoch': 0.19}
{'loss': 0.0552, 'learning_rate': 2.5572054976233467e-05, 'epoch': 0.49}
{'loss': 0.0873, 'learning_rate': 3.852529978448673e-05, 'epoch': 0.23}
{'loss': 0.0305, 'learning_rate': 3.905026801009413e-05, 'epoch': 0.22}
{'loss': 0.0843, 'learning_rate': 3.771482252389987e-05, 'epoch': 0.25}
{'loss': 0.0424, 'learning_rate': 2.6898559268948748e-05, 'epoch': 0.46}
{'loss': 0.058, 'learning_rate': 2.5129886878661706e-05, 'epoch': 0.5}
{'loss': 0.0255, 'learning_rate': 1.8349976049228047e-05, 'epoch': 0.63}
{'loss': 0.0731, 'learning_rate': 4.080937206432243e-05, 'epoch': 0.18}
{'loss': 0.0772, 'learning_rate': 4.028440383871503e-05, 'epoch': 0.19}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0686, 'learning_rate': 2.5424665610376212e-05, 'epoch': 0.49}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0617, 'learning_rate': 3.8451620033524294e-05, 'epoch': 0.23}
{'loss': 0.0932, 'learning_rate': 3.897658825913169e-05, 'epoch': 0.22}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0542, 'learning_rate': 3.7641142772937435e-05, 'epoch': 0.25}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0472, 'learning_rate': 2.6751169903091493e-05, 'epoch': 0.47}
{'loss': 0.0863, 'learning_rate': 2.498249751280445e-05, 'epoch': 0.5}
{'loss': 0.0545, 'learning_rate': 1.8202586683370796e-05, 'epoch': 0.64}
{'loss': 0.0683, 'learning_rate': 4.073569231335998e-05, 'epoch': 0.19}
{'loss': 0.0656, 'learning_rate': 4.021072408775259e-05, 'epoch': 0.2}
{'loss': 0.0519, 'learning_rate': 2.527727624451896e-05, 'epoch': 0.5}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0694, 'learning_rate': 3.837794028256185e-05, 'epoch': 0.23}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0839, 'learning_rate': 3.890290850816924e-05, 'epoch': 0.22}
{'loss': 0.0869, 'learning_rate': 3.756746302197499e-05, 'epoch': 0.25}
{'loss': 0.0459, 'learning_rate': 2.660378053723424e-05, 'epoch': 0.47}
{'loss': 0.0821, 'learning_rate': 4.0662012562397545e-05, 'epoch': 0.19}
{'loss': 0.0324, 'learning_rate': 2.4835108146947196e-05, 'epoch': 0.5}
{'loss': 0.059, 'learning_rate': 1.805519731751354e-05, 'epoch': 0.64}
{'loss': 0.0379, 'learning_rate': 4.0137044336790145e-05, 'epoch': 0.2}
{'loss': 0.0546, 'learning_rate': 2.5129886878661706e-05, 'epoch': 0.5}
{'loss': 0.0545, 'learning_rate': 3.8304260531599405e-05, 'epoch': 0.23}
{'loss': 0.0664, 'learning_rate': 3.8829228757206805e-05, 'epoch': 0.22}
{'loss': 0.0739, 'learning_rate': 3.749378327101255e-05, 'epoch': 0.25}
{'loss': 0.1066, 'learning_rate': 4.058833281143511e-05, 'epoch': 0.19}
{'loss': 0.0831, 'learning_rate': 2.6456391171376986e-05, 'epoch': 0.47}
{'loss': 0.0947, 'learning_rate': 2.4687718781089945e-05, 'epoch': 0.51}
{'loss': 0.0202, 'learning_rate': 1.790780795165629e-05, 'epoch': 0.64}
{'loss': 0.068, 'learning_rate': 4.006336458582771e-05, 'epoch': 0.2}
{'loss': 0.0672, 'learning_rate': 2.498249751280445e-05, 'epoch': 0.5}
{'loss': 0.0256, 'learning_rate': 3.823058078063697e-05, 'epoch': 0.24}
{'loss': 0.039, 'learning_rate': 3.875554900624436e-05, 'epoch': 0.23}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0681, 'learning_rate': 3.74201035200501e-05, 'epoch': 0.25}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0778, 'learning_rate': 4.051465306047266e-05, 'epoch': 0.19}
{'loss': 0.0766, 'learning_rate': 2.6309001805519735e-05, 'epoch': 0.47}
{'loss': 0.0662, 'learning_rate': 2.454032941523269e-05, 'epoch': 0.51}
{'loss': 0.0209, 'learning_rate': 1.7760418585799034e-05, 'epoch': 0.65}
{'loss': 0.1074, 'learning_rate': 3.998968483486526e-05, 'epoch': 0.2}
{'loss': 0.095, 'learning_rate': 2.4835108146947196e-05, 'epoch': 0.5}
{'loss': 0.0468, 'learning_rate': 3.815690102967452e-05, 'epoch': 0.24}
{'loss': 0.112, 'learning_rate': 3.868186925528192e-05, 'epoch': 0.23}
{'loss': 0.0553, 'learning_rate': 3.734642376908766e-05, 'epoch': 0.25}
{'loss': 0.0583, 'learning_rate': 4.044097330951022e-05, 'epoch': 0.19}
{'loss': 0.0861, 'learning_rate': 2.616161243966248e-05, 'epoch': 0.48}
{'loss': 0.0316, 'learning_rate': 2.439294004937544e-05, 'epoch': 0.51}
{'loss': 0.0383, 'learning_rate': 1.7613029219941783e-05, 'epoch': 0.65}
{'loss': 0.0636, 'learning_rate': 3.991600508390282e-05, 'epoch': 0.2}
{'loss': 0.0673, 'learning_rate': 2.4687718781089945e-05, 'epoch': 0.51}
{'loss': 0.0584, 'learning_rate': 3.8083221278712084e-05, 'epoch': 0.24}
{'loss': 0.0938, 'learning_rate': 3.8608189504319484e-05, 'epoch': 0.23}
{'loss': 0.0661, 'learning_rate': 3.7272744018125224e-05, 'epoch': 0.25}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0405, 'learning_rate': 2.601422307380523e-05, 'epoch': 0.48}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0959, 'learning_rate': 2.4245550683518184e-05, 'epoch': 0.52}
{'loss': 0.0729, 'learning_rate': 1.7465639854084528e-05, 'epoch': 0.65}
{'loss': 0.0823, 'learning_rate': 3.984232533294038e-05, 'epoch': 0.2}
{'loss': 0.0568, 'learning_rate': 4.036729355854777e-05, 'epoch': 0.19}
{'loss': 0.1022, 'learning_rate': 3.8009541527749646e-05, 'epoch': 0.24}
{'loss': 0.068, 'learning_rate': 2.454032941523269e-05, 'epoch': 0.51}
{'loss': 0.0643, 'learning_rate': 3.853450975335704e-05, 'epoch': 0.23}
{'loss': 0.0814, 'learning_rate': 3.719906426716278e-05, 'epoch': 0.26}
{'loss': 0.0272, 'learning_rate': 2.5866833707947974e-05, 'epoch': 0.48}
{'loss': 0.0691, 'learning_rate': 4.0293613807585334e-05, 'epoch': 0.19}
{'loss': 0.0789, 'learning_rate': 1.7318250488227277e-05, 'epoch': 0.65}
{'loss': 0.0456, 'learning_rate': 2.409816131766093e-05, 'epoch': 0.52}
{'loss': 0.0573, 'learning_rate': 3.9768645581977934e-05, 'epoch': 0.2}
{'loss': 0.0472, 'learning_rate': 2.439294004937544e-05, 'epoch': 0.51}
{'loss': 0.0679, 'learning_rate': 3.79358617767872e-05, 'epoch': 0.24}
{'loss': 0.1335, 'learning_rate': 3.84608300023946e-05, 'epoch': 0.23}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0643, 'learning_rate': 3.712538451620034e-05, 'epoch': 0.26}
{'loss': 0.101, 'learning_rate': 2.5719444342090722e-05, 'epoch': 0.49}
{'loss': 0.0736, 'learning_rate': 4.0219934056622896e-05, 'epoch': 0.2}
{'loss': 0.0599, 'learning_rate': 1.717086112237002e-05, 'epoch': 0.66}
{'loss': 0.0735, 'learning_rate': 2.3950771951803677e-05, 'epoch': 0.52}
{'loss': 0.0534, 'learning_rate': 3.9694965831015496e-05, 'epoch': 0.21}
{'loss': 0.0621, 'learning_rate': 2.4245550683518184e-05, 'epoch': 0.52}
{'loss': 0.0726, 'learning_rate': 3.7862182025824756e-05, 'epoch': 0.24}
{'loss': 0.0771, 'learning_rate': 3.838715025143215e-05, 'epoch': 0.23}
{'loss': 0.0843, 'learning_rate': 3.70517047652379e-05, 'epoch': 0.26}
{'loss': 0.0596, 'learning_rate': 4.014625430566045e-05, 'epoch': 0.2}
{'loss': 0.0589, 'learning_rate': 2.5572054976233467e-05, 'epoch': 0.49}
{'loss': 0.0471, 'learning_rate': 1.7023471756512767e-05, 'epoch': 0.66}
{'loss': 0.0571, 'learning_rate': 2.3803382585946423e-05, 'epoch': 0.52}
{'loss': 0.0773, 'learning_rate': 3.962128608005306e-05, 'epoch': 0.21}
{'loss': 0.0474, 'learning_rate': 2.409816131766093e-05, 'epoch': 0.52}
{'loss': 0.059, 'learning_rate': 3.778850227486231e-05, 'epoch': 0.24}
{'loss': 0.0221, 'learning_rate': 3.831347050046971e-05, 'epoch': 0.23}
{'loss': 0.0631, 'learning_rate': 3.697802501427546e-05, 'epoch': 0.26}
{'loss': 0.074, 'learning_rate': 4.007257455469801e-05, 'epoch': 0.2}
{'loss': 0.0592, 'learning_rate': 2.5424665610376212e-05, 'epoch': 0.49}
{'loss': 0.0807, 'learning_rate': 1.6876082390655515e-05, 'epoch': 0.66}
{'loss': 0.0685, 'learning_rate': 2.365599322008917e-05, 'epoch': 0.53}
{'loss': 0.1202, 'learning_rate': 3.954760632909061e-05, 'epoch': 0.21}
{'loss': 0.0671, 'learning_rate': 2.3950771951803677e-05, 'epoch': 0.52}
{'loss': 0.0852, 'learning_rate': 3.771482252389987e-05, 'epoch': 0.25}
{'loss': 0.0584, 'learning_rate': 3.823979074950727e-05, 'epoch': 0.24}
{'loss': 0.1014, 'learning_rate': 3.6904345263313014e-05, 'epoch': 0.26}
{'loss': 0.0482, 'learning_rate': 3.999889480373557e-05, 'epoch': 0.2}
{'loss': 0.0393, 'learning_rate': 2.527727624451896e-05, 'epoch': 0.5}
{'loss': 0.0505, 'learning_rate': 1.672869302479826e-05, 'epoch': 0.67}
{'loss': 0.0695, 'learning_rate': 2.3508603854231916e-05, 'epoch': 0.53}
{'loss': 0.0603, 'learning_rate': 3.9473926578128175e-05, 'epoch': 0.21}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0715, 'learning_rate': 2.3803382585946423e-05, 'epoch': 0.52}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0781, 'learning_rate': 3.7641142772937435e-05, 'epoch': 0.25}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0773, 'learning_rate': 3.683066551235057e-05, 'epoch': 0.26}
{'loss': 0.1267, 'learning_rate': 3.816611099854483e-05, 'epoch': 0.24}
{'loss': 0.0637, 'learning_rate': 3.9925215052773124e-05, 'epoch': 0.2}
{'loss': 0.0553, 'learning_rate': 2.5129886878661706e-05, 'epoch': 0.5}
{'loss': 0.0651, 'learning_rate': 1.658130365894101e-05, 'epoch': 0.67}
{'loss': 0.05, 'learning_rate': 2.3361214488374665e-05, 'epoch': 0.53}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0869, 'learning_rate': 3.9400246827165724e-05, 'epoch': 0.21}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0391, 'learning_rate': 2.365599322008917e-05, 'epoch': 0.53}
{'loss': 0.0594, 'learning_rate': 3.756746302197499e-05, 'epoch': 0.25}
{'loss': 0.0551, 'learning_rate': 3.675698576138813e-05, 'epoch': 0.27}
{'loss': 0.0874, 'learning_rate': 3.809243124758239e-05, 'epoch': 0.24}
{'loss': 0.0717, 'learning_rate': 3.9851535301810685e-05, 'epoch': 0.2}
{'eval_loss': 0.05055956169962883, 'eval_accuracy': 0.9824394792278206, 'eval_f1': 0.9824136678828961, 'eval_precision': 0.9872792993187821, 'eval_recall': 0.9775957600578173, 'eval_runtime': 388.7846, 'eval_samples_per_second': 148.962, 'eval_steps_per_second': 18.622, 'epoch': 1.0}
{'loss': 0.0728, 'learning_rate': 2.498249751280445e-05, 'epoch': 0.5}
{'loss': 0.0256, 'learning_rate': 1.6433914293083754e-05, 'epoch': 0.67}
{'loss': 0.0558, 'learning_rate': 2.321382512251741e-05, 'epoch': 0.54}
{'loss': 0.0949, 'learning_rate': 3.9326567076203285e-05, 'epoch': 0.21}
{'train_runtime': 5328.9537, 'train_samples_per_second': 19.562, 'train_steps_per_second': 0.306, 'train_loss': 0.08019527833204011, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.08, 'learning_rate': 3.6683306010425686e-05, 'epoch': 0.27}
{'loss': 0.0539, 'learning_rate': 2.3508603854231916e-05, 'epoch': 0.53}
{'loss': 0.088, 'learning_rate': 3.749378327101255e-05, 'epoch': 0.25}
{'loss': 0.0634, 'learning_rate': 3.8018751496619945e-05, 'epoch': 0.24}
{'loss': 0.0338, 'learning_rate': 3.977785555084824e-05, 'epoch': 0.2}
{'loss': 0.0466, 'learning_rate': 2.4835108146947196e-05, 'epoch': 0.5}
{'loss': 0.0727, 'learning_rate': 1.6286524927226503e-05, 'epoch': 0.67}
{'loss': 0.0605, 'learning_rate': 2.306643575666016e-05, 'epoch': 0.54}
{'loss': 0.0753, 'learning_rate': 3.925288732524085e-05, 'epoch': 0.22}
{'loss': 0.0339, 'learning_rate': 3.660962625946325e-05, 'epoch': 0.27}
{'loss': 0.0652, 'learning_rate': 2.3361214488374665e-05, 'epoch': 0.53}
{'loss': 0.0744, 'learning_rate': 3.74201035200501e-05, 'epoch': 0.25}
{'loss': 0.0592, 'learning_rate': 3.794507174565751e-05, 'epoch': 0.24}
{'loss': 0.0792, 'learning_rate': 3.97041757998858e-05, 'epoch': 0.21}
{'loss': 0.053, 'learning_rate': 2.4687718781089945e-05, 'epoch': 0.51}
{'loss': 0.0232, 'learning_rate': 1.6139135561369248e-05, 'epoch': 0.68}
{'loss': 0.0774, 'learning_rate': 2.2919046390802904e-05, 'epoch': 0.54}
{'loss': 0.0736, 'learning_rate': 3.91792075742784e-05, 'epoch': 0.22}
{'loss': 0.0766, 'learning_rate': 3.653594650850081e-05, 'epoch': 0.27}
{'loss': 0.0584, 'learning_rate': 2.321382512251741e-05, 'epoch': 0.54}
{'loss': 0.0624, 'learning_rate': 3.734642376908766e-05, 'epoch': 0.25}
{'loss': 0.081, 'learning_rate': 3.963049604892336e-05, 'epoch': 0.21}
{'loss': 0.0418, 'learning_rate': 3.787139199469506e-05, 'epoch': 0.24}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0672, 'learning_rate': 2.454032941523269e-05, 'epoch': 0.51}
{'loss': 0.0722, 'learning_rate': 1.5991746195511993e-05, 'epoch': 0.68}
{'loss': 0.0504, 'learning_rate': 2.277165702494565e-05, 'epoch': 0.55}
{'loss': 0.0711, 'learning_rate': 3.9105527823315965e-05, 'epoch': 0.22}
{'loss': 0.0619, 'learning_rate': 3.6462266757538365e-05, 'epoch': 0.27}
{'loss': 0.0538, 'learning_rate': 2.306643575666016e-05, 'epoch': 0.54}
{'loss': 0.0913, 'learning_rate': 3.7272744018125224e-05, 'epoch': 0.25}
{'loss': 0.0784, 'learning_rate': 3.955681629796092e-05, 'epoch': 0.21}
{'loss': 0.0775, 'learning_rate': 3.779771224373262e-05, 'epoch': 0.24}
{'loss': 0.0711, 'learning_rate': 2.439294004937544e-05, 'epoch': 0.51}
{'loss': 0.052, 'learning_rate': 1.584435682965474e-05, 'epoch': 0.68}
{'loss': 0.0561, 'learning_rate': 2.2624267659088397e-05, 'epoch': 0.55}
{'loss': 0.0897, 'learning_rate': 3.903184807235352e-05, 'epoch': 0.22}
{'loss': 0.0729, 'learning_rate': 3.638858700657592e-05, 'epoch': 0.27}
{'loss': 0.0611, 'learning_rate': 2.2919046390802904e-05, 'epoch': 0.54}
{'loss': 0.0866, 'learning_rate': 3.719906426716278e-05, 'epoch': 0.26}
{'loss': 0.0504, 'learning_rate': 3.9483136546998475e-05, 'epoch': 0.21}
{'loss': 0.0852, 'learning_rate': 3.772403249277018e-05, 'epoch': 0.25}
{'loss': 0.065, 'learning_rate': 2.4245550683518184e-05, 'epoch': 0.52}
{'loss': 0.1, 'learning_rate': 3.6314907255613475e-05, 'epoch': 0.27}
{'loss': 0.0442, 'learning_rate': 1.5696967463797487e-05, 'epoch': 0.69}
{'loss': 0.081, 'learning_rate': 2.2476878293231142e-05, 'epoch': 0.55}
{'loss': 0.0686, 'learning_rate': 3.895816832139108e-05, 'epoch': 0.22}
{'loss': 0.0655, 'learning_rate': 2.277165702494565e-05, 'epoch': 0.55}
{'loss': 0.0717, 'learning_rate': 3.940945679603603e-05, 'epoch': 0.21}
{'loss': 0.077, 'learning_rate': 3.712538451620034e-05, 'epoch': 0.26}
{'loss': 0.0563, 'learning_rate': 3.7650352741807735e-05, 'epoch': 0.25}
{'loss': 0.0713, 'learning_rate': 3.624122750465104e-05, 'epoch': 0.28}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0637, 'learning_rate': 2.409816131766093e-05, 'epoch': 0.52}
{'loss': 0.0712, 'learning_rate': 1.5549578097940235e-05, 'epoch': 0.69}
{'loss': 0.1011, 'learning_rate': 3.888448857042864e-05, 'epoch': 0.22}
{'loss': 0.0325, 'learning_rate': 2.232948892737389e-05, 'epoch': 0.55}
{'loss': 0.0663, 'learning_rate': 3.933577704507359e-05, 'epoch': 0.21}
{'loss': 0.0449, 'learning_rate': 2.2624267659088397e-05, 'epoch': 0.55}
{'loss': 0.0591, 'learning_rate': 3.70517047652379e-05, 'epoch': 0.26}
{'loss': 0.0528, 'learning_rate': 3.75766729908453e-05, 'epoch': 0.25}
{'loss': 0.0669, 'learning_rate': 3.61675477536886e-05, 'epoch': 0.28}
{'loss': 0.0731, 'learning_rate': 2.3950771951803677e-05, 'epoch': 0.52}
{'loss': 0.0684, 'learning_rate': 1.540218873208298e-05, 'epoch': 0.69}
{'loss': 0.0716, 'learning_rate': 3.881080881946619e-05, 'epoch': 0.22}
{'loss': 0.0265, 'learning_rate': 2.2182099561516636e-05, 'epoch': 0.56}
{'loss': 0.0579, 'learning_rate': 3.926209729411115e-05, 'epoch': 0.22}
{'loss': 0.0606, 'learning_rate': 2.2476878293231142e-05, 'epoch': 0.55}
{'loss': 0.0687, 'learning_rate': 3.697802501427546e-05, 'epoch': 0.26}
{'loss': 0.0766, 'learning_rate': 3.750299323988286e-05, 'epoch': 0.25}
{'loss': 0.059, 'learning_rate': 3.6093868002726154e-05, 'epoch': 0.28}
{'loss': 0.0642, 'learning_rate': 2.3803382585946423e-05, 'epoch': 0.52}
{'loss': 0.0479, 'learning_rate': 3.8737129068503754e-05, 'epoch': 0.23}
{'loss': 0.0567, 'learning_rate': 1.5254799366225727e-05, 'epoch': 0.7}
{'loss': 0.1049, 'learning_rate': 3.918841754314871e-05, 'epoch': 0.22}
{'loss': 0.0671, 'learning_rate': 2.2034710195659385e-05, 'epoch': 0.56}
{'loss': 0.0683, 'learning_rate': 2.232948892737389e-05, 'epoch': 0.55}
{'loss': 0.0658, 'learning_rate': 3.6904345263313014e-05, 'epoch': 0.26}
{'loss': 0.0648, 'learning_rate': 3.6020188251763716e-05, 'epoch': 0.28}
{'loss': 0.09, 'learning_rate': 3.7429313488920414e-05, 'epoch': 0.25}
{'loss': 0.0692, 'learning_rate': 2.365599322008917e-05, 'epoch': 0.53}
{'loss': 0.0515, 'learning_rate': 3.911473779218627e-05, 'epoch': 0.22}
{'loss': 0.069, 'learning_rate': 3.866344931754131e-05, 'epoch': 0.23}
{'loss': 0.0416, 'learning_rate': 1.5107410000368474e-05, 'epoch': 0.7}
{'loss': 0.0444, 'learning_rate': 2.188732082980213e-05, 'epoch': 0.56}
{'loss': 0.0698, 'learning_rate': 2.2182099561516636e-05, 'epoch': 0.56}
{'loss': 0.0401, 'learning_rate': 3.683066551235057e-05, 'epoch': 0.26}
{'loss': 0.0903, 'learning_rate': 3.594650850080127e-05, 'epoch': 0.28}
{'loss': 0.1118, 'learning_rate': 3.735563373795797e-05, 'epoch': 0.25}
{'loss': 0.0645, 'learning_rate': 2.3508603854231916e-05, 'epoch': 0.53}
{'loss': 0.067, 'learning_rate': 3.9041058041223826e-05, 'epoch': 0.22}
{'loss': 0.0618, 'learning_rate': 3.858976956657887e-05, 'epoch': 0.23}
{'loss': 0.0582, 'learning_rate': 1.496002063451122e-05, 'epoch': 0.7}
{'loss': 0.0698, 'learning_rate': 2.1739931463944875e-05, 'epoch': 0.57}
{'loss': 0.0641, 'learning_rate': 2.2034710195659385e-05, 'epoch': 0.56}
{'loss': 0.0619, 'learning_rate': 3.675698576138813e-05, 'epoch': 0.27}
{'loss': 0.0555, 'learning_rate': 3.587282874983883e-05, 'epoch': 0.28}
{'loss': 0.0985, 'learning_rate': 3.7281953986995524e-05, 'epoch': 0.25}
{'loss': 0.0574, 'learning_rate': 3.896737829026139e-05, 'epoch': 0.22}
{'loss': 0.0948, 'learning_rate': 2.3361214488374665e-05, 'epoch': 0.53}
{'loss': 0.0551, 'learning_rate': 3.851608981561643e-05, 'epoch': 0.23}
{'loss': 0.066, 'learning_rate': 1.4812631268653968e-05, 'epoch': 0.7}
{'loss': 0.0363, 'learning_rate': 2.1592542098087623e-05, 'epoch': 0.57}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0692, 'learning_rate': 2.188732082980213e-05, 'epoch': 0.56}
{'loss': 0.0617, 'learning_rate': 3.579914899887639e-05, 'epoch': 0.28}
{'loss': 0.0691, 'learning_rate': 3.6683306010425686e-05, 'epoch': 0.27}
{'loss': 0.0854, 'learning_rate': 3.7208274236033086e-05, 'epoch': 0.26}
{'loss': 0.058, 'learning_rate': 3.8893698539298936e-05, 'epoch': 0.22}
{'loss': 0.0387, 'learning_rate': 2.321382512251741e-05, 'epoch': 0.54}
{'loss': 0.0752, 'learning_rate': 3.844241006465399e-05, 'epoch': 0.23}
{'loss': 0.0388, 'learning_rate': 1.4665241902796714e-05, 'epoch': 0.71}
{'loss': 0.043, 'learning_rate': 2.144515273223037e-05, 'epoch': 0.57}
{'loss': 0.0546, 'learning_rate': 2.1739931463944875e-05, 'epoch': 0.57}
{'loss': 0.0651, 'learning_rate': 3.5725469247913944e-05, 'epoch': 0.29}
{'loss': 0.0645, 'learning_rate': 3.660962625946325e-05, 'epoch': 0.27}
{'loss': 0.0554, 'learning_rate': 3.713459448507065e-05, 'epoch': 0.26}
{'loss': 0.0696, 'learning_rate': 3.88200187883365e-05, 'epoch': 0.22}
{'loss': 0.048, 'learning_rate': 2.306643575666016e-05, 'epoch': 0.54}
{'loss': 0.06, 'learning_rate': 3.836873031369154e-05, 'epoch': 0.23}
{'loss': 0.0422, 'learning_rate': 2.1297763366373117e-05, 'epoch': 0.57}
{'loss': 0.0519, 'learning_rate': 1.4517852536939461e-05, 'epoch': 0.71}
{'loss': 0.0311, 'learning_rate': 3.5651789496951506e-05, 'epoch': 0.29}
{'loss': 0.0272, 'learning_rate': 2.1592542098087623e-05, 'epoch': 0.57}
{'loss': 0.0768, 'learning_rate': 3.653594650850081e-05, 'epoch': 0.27}
{'loss': 0.0715, 'learning_rate': 3.874633903737406e-05, 'epoch': 0.23}
{'loss': 0.0461, 'learning_rate': 3.70609147341082e-05, 'epoch': 0.26}
{'loss': 0.0914, 'learning_rate': 2.2919046390802904e-05, 'epoch': 0.54}
{'loss': 0.0684, 'learning_rate': 3.82950505627291e-05, 'epoch': 0.23}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0792, 'learning_rate': 2.1150374000515862e-05, 'epoch': 0.58}
{'loss': 0.043, 'learning_rate': 1.4370463171082208e-05, 'epoch': 0.71}
{'loss': 0.1298, 'learning_rate': 3.557810974598906e-05, 'epoch': 0.29}
{'loss': 0.0317, 'learning_rate': 2.144515273223037e-05, 'epoch': 0.57}
{'loss': 0.052, 'learning_rate': 3.6462266757538365e-05, 'epoch': 0.27}
{'loss': 0.1049, 'learning_rate': 3.8672659286411615e-05, 'epoch': 0.23}
{'loss': 0.0619, 'learning_rate': 3.6987234983145765e-05, 'epoch': 0.26}
{'loss': 0.0521, 'learning_rate': 2.277165702494565e-05, 'epoch': 0.55}
{'loss': 0.0664, 'learning_rate': 3.822137081176666e-05, 'epoch': 0.24}
{'loss': 0.0362, 'learning_rate': 1.4223073805224953e-05, 'epoch': 0.72}
{'loss': 0.059, 'learning_rate': 2.100298463465861e-05, 'epoch': 0.58}
{'loss': 0.1013, 'learning_rate': 3.550442999502662e-05, 'epoch': 0.29}
{'loss': 0.0947, 'learning_rate': 2.1297763366373117e-05, 'epoch': 0.57}
{'loss': 0.053, 'learning_rate': 3.638858700657592e-05, 'epoch': 0.27}
{'loss': 0.0662, 'learning_rate': 3.859897953544918e-05, 'epoch': 0.23}
{'loss': 0.0413, 'learning_rate': 3.691355523218332e-05, 'epoch': 0.26}
{'loss': 0.0499, 'learning_rate': 2.2624267659088397e-05, 'epoch': 0.55}
{'loss': 0.0719, 'learning_rate': 3.814769106080422e-05, 'epoch': 0.24}
{'loss': 0.0941, 'learning_rate': 3.5430750244064185e-05, 'epoch': 0.29}
{'loss': 0.0988, 'learning_rate': 1.40756844393677e-05, 'epoch': 0.72}
{'loss': 0.0608, 'learning_rate': 2.0855595268801356e-05, 'epoch': 0.58}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0569, 'learning_rate': 2.1150374000515862e-05, 'epoch': 0.58}
{'loss': 0.1073, 'learning_rate': 3.6314907255613475e-05, 'epoch': 0.27}
{'loss': 0.0596, 'learning_rate': 3.852529978448673e-05, 'epoch': 0.23}
{'loss': 0.0466, 'learning_rate': 3.6839875481220875e-05, 'epoch': 0.26}
{'loss': 0.0434, 'learning_rate': 2.2476878293231142e-05, 'epoch': 0.55}
{'loss': 0.0744, 'learning_rate': 3.535707049310173e-05, 'epoch': 0.29}
{'loss': 0.0611, 'learning_rate': 3.807401130984178e-05, 'epoch': 0.24}
{'loss': 0.0373, 'learning_rate': 1.3928295073510447e-05, 'epoch': 0.72}
{'loss': 0.0524, 'learning_rate': 2.0708205902944104e-05, 'epoch': 0.59}
{'loss': 0.0578, 'learning_rate': 2.100298463465861e-05, 'epoch': 0.58}
{'loss': 0.0736, 'learning_rate': 3.8451620033524294e-05, 'epoch': 0.23}
{'loss': 0.1046, 'learning_rate': 3.624122750465104e-05, 'epoch': 0.28}
{'loss': 0.0449, 'learning_rate': 3.676619573025844e-05, 'epoch': 0.27}
{'loss': 0.0492, 'learning_rate': 2.232948892737389e-05, 'epoch': 0.55}
{'loss': 0.0662, 'learning_rate': 3.5283390742139295e-05, 'epoch': 0.29}
{'loss': 0.0486, 'learning_rate': 3.800033155887934e-05, 'epoch': 0.24}
{'loss': 0.0504, 'learning_rate': 2.056081653708685e-05, 'epoch': 0.59}
{'loss': 0.0468, 'learning_rate': 1.3780905707653194e-05, 'epoch': 0.72}
{'loss': 0.0627, 'learning_rate': 2.0855595268801356e-05, 'epoch': 0.58}
{'loss': 0.0718, 'learning_rate': 3.837794028256185e-05, 'epoch': 0.23}
{'loss': 0.0799, 'learning_rate': 3.61675477536886e-05, 'epoch': 0.28}
{'loss': 0.0511, 'learning_rate': 3.669251597929599e-05, 'epoch': 0.27}
{'loss': 0.0653, 'learning_rate': 3.520971099117685e-05, 'epoch': 0.3}
{'loss': 0.0456, 'learning_rate': 2.2182099561516636e-05, 'epoch': 0.56}
{'loss': 0.0429, 'learning_rate': 3.792665180791689e-05, 'epoch': 0.24}
{'loss': 0.0891, 'learning_rate': 2.0413427171229595e-05, 'epoch': 0.59}
{'loss': 0.0498, 'learning_rate': 1.363351634179594e-05, 'epoch': 0.73}
{'loss': 0.0517, 'learning_rate': 2.0708205902944104e-05, 'epoch': 0.59}
{'loss': 0.1049, 'learning_rate': 3.8304260531599405e-05, 'epoch': 0.23}
{'loss': 0.0548, 'learning_rate': 3.6093868002726154e-05, 'epoch': 0.28}
{'loss': 0.073, 'learning_rate': 3.6618836228333554e-05, 'epoch': 0.27}
{'loss': 0.0534, 'learning_rate': 3.513603124021441e-05, 'epoch': 0.3}
{'loss': 0.0409, 'learning_rate': 2.2034710195659385e-05, 'epoch': 0.56}
{'loss': 0.0645, 'learning_rate': 3.785297205695445e-05, 'epoch': 0.24}
{'loss': 0.0558, 'learning_rate': 2.0266037805372343e-05, 'epoch': 0.6}
{'loss': 0.0394, 'learning_rate': 1.3486126975938687e-05, 'epoch': 0.73}
{'loss': 0.0524, 'learning_rate': 2.056081653708685e-05, 'epoch': 0.59}
{'loss': 0.0567, 'learning_rate': 3.823058078063697e-05, 'epoch': 0.24}
{'loss': 0.0994, 'learning_rate': 3.6020188251763716e-05, 'epoch': 0.28}
{'loss': 0.073, 'learning_rate': 3.5062351489251974e-05, 'epoch': 0.3}
{'loss': 0.0732, 'learning_rate': 3.654515647737111e-05, 'epoch': 0.27}
{'loss': 0.0677, 'learning_rate': 2.188732082980213e-05, 'epoch': 0.56}
{'loss': 0.0746, 'learning_rate': 3.777929230599201e-05, 'epoch': 0.24}
{'loss': 0.0248, 'learning_rate': 2.011864843951509e-05, 'epoch': 0.6}
{'loss': 0.0526, 'learning_rate': 1.3338737610081434e-05, 'epoch': 0.73}
{'loss': 0.0476, 'learning_rate': 3.815690102967452e-05, 'epoch': 0.24}
{'loss': 0.0394, 'learning_rate': 2.0413427171229595e-05, 'epoch': 0.59}
{'loss': 0.0726, 'learning_rate': 3.594650850080127e-05, 'epoch': 0.28}
{'loss': 0.0858, 'learning_rate': 3.498867173828953e-05, 'epoch': 0.3}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0977, 'learning_rate': 3.647147672640867e-05, 'epoch': 0.27}
{'loss': 0.0275, 'learning_rate': 2.1739931463944875e-05, 'epoch': 0.57}
{'loss': 0.0724, 'learning_rate': 3.770561255502957e-05, 'epoch': 0.25}
{'loss': 0.1018, 'learning_rate': 1.9971259073657837e-05, 'epoch': 0.6}
{'loss': 0.037, 'learning_rate': 1.3191348244224181e-05, 'epoch': 0.74}
{'loss': 0.0861, 'learning_rate': 3.8083221278712084e-05, 'epoch': 0.24}
{'loss': 0.0677, 'learning_rate': 2.0266037805372343e-05, 'epoch': 0.6}
{'loss': 0.0902, 'learning_rate': 3.491499198732709e-05, 'epoch': 0.3}
{'loss': 0.0703, 'learning_rate': 3.587282874983883e-05, 'epoch': 0.28}
{'loss': 0.0669, 'learning_rate': 3.639779697544623e-05, 'epoch': 0.27}
{'loss': 0.07, 'learning_rate': 2.1592542098087623e-05, 'epoch': 0.57}
{'loss': 0.0627, 'learning_rate': 3.763193280406713e-05, 'epoch': 0.25}
{'loss': 0.087, 'learning_rate': 3.8009541527749646e-05, 'epoch': 0.24}
{'loss': 0.0456, 'learning_rate': 1.3043958878366926e-05, 'epoch': 0.74}
{'loss': 0.0698, 'learning_rate': 1.9823869707800582e-05, 'epoch': 0.6}
{'loss': 0.0414, 'learning_rate': 2.011864843951509e-05, 'epoch': 0.6}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0705, 'learning_rate': 3.484131223636464e-05, 'epoch': 0.3}
{'loss': 0.0664, 'learning_rate': 3.579914899887639e-05, 'epoch': 0.28}
{'loss': 0.0924, 'learning_rate': 3.632411722448378e-05, 'epoch': 0.27}
{'loss': 0.0918, 'learning_rate': 2.144515273223037e-05, 'epoch': 0.57}
{'loss': 0.0923, 'learning_rate': 3.7558253053104684e-05, 'epoch': 0.25}
{'loss': 0.0696, 'learning_rate': 3.79358617767872e-05, 'epoch': 0.24}
{'loss': 0.0562, 'learning_rate': 1.2896569512509673e-05, 'epoch': 0.74}
{'loss': 0.0517, 'learning_rate': 1.967648034194333e-05, 'epoch': 0.61}
{'loss': 0.0486, 'learning_rate': 1.9971259073657837e-05, 'epoch': 0.6}
{'loss': 0.0512, 'learning_rate': 3.47676324854022e-05, 'epoch': 0.31}
{'loss': 0.045, 'learning_rate': 3.5725469247913944e-05, 'epoch': 0.29}
{'loss': 0.0692, 'learning_rate': 3.6250437473521344e-05, 'epoch': 0.28}
{'loss': 0.0421, 'learning_rate': 2.1297763366373117e-05, 'epoch': 0.57}
{'loss': 0.0614, 'learning_rate': 3.7484573302142246e-05, 'epoch': 0.25}
{'loss': 0.0608, 'learning_rate': 3.7862182025824756e-05, 'epoch': 0.24}
{'loss': 0.0505, 'learning_rate': 1.274918014665242e-05, 'epoch': 0.75}
{'loss': 0.0667, 'learning_rate': 1.9529090976086076e-05, 'epoch': 0.61}
{'loss': 0.0523, 'learning_rate': 3.4693952734439763e-05, 'epoch': 0.31}
{'loss': 0.0582, 'learning_rate': 1.9823869707800582e-05, 'epoch': 0.6}
{'loss': 0.0304, 'learning_rate': 3.5651789496951506e-05, 'epoch': 0.29}
{'loss': 0.0475, 'learning_rate': 3.61767577225589e-05, 'epoch': 0.28}
{'loss': 0.0648, 'learning_rate': 2.1150374000515862e-05, 'epoch': 0.58}
{'loss': 0.0529, 'learning_rate': 3.778850227486231e-05, 'epoch': 0.24}
{'loss': 0.0335, 'learning_rate': 3.74108935511798e-05, 'epoch': 0.25}
{'loss': 0.0711, 'learning_rate': 1.2601790780795167e-05, 'epoch': 0.75}
{'loss': 0.0772, 'learning_rate': 1.938170161022882e-05, 'epoch': 0.61}
{'loss': 0.0661, 'learning_rate': 3.462027298347732e-05, 'epoch': 0.31}
{'loss': 0.0629, 'learning_rate': 1.967648034194333e-05, 'epoch': 0.61}
{'loss': 0.0493, 'learning_rate': 3.557810974598906e-05, 'epoch': 0.29}
{'loss': 0.0731, 'learning_rate': 3.771482252389987e-05, 'epoch': 0.25}
{'loss': 0.0623, 'learning_rate': 3.610307797159646e-05, 'epoch': 0.28}
{'loss': 0.0735, 'learning_rate': 2.100298463465861e-05, 'epoch': 0.58}
{'loss': 0.0937, 'learning_rate': 3.7337213800217356e-05, 'epoch': 0.25}
{'loss': 0.0598, 'learning_rate': 3.454659323251488e-05, 'epoch': 0.31}
{'loss': 0.0482, 'learning_rate': 1.2454401414937912e-05, 'epoch': 0.75}
{'loss': 0.0523, 'learning_rate': 1.923431224437157e-05, 'epoch': 0.62}
{'loss': 0.0695, 'learning_rate': 1.9529090976086076e-05, 'epoch': 0.61}
{'loss': 0.0613, 'learning_rate': 3.550442999502662e-05, 'epoch': 0.29}
{'loss': 0.066, 'learning_rate': 3.7641142772937435e-05, 'epoch': 0.25}
{'loss': 0.0594, 'learning_rate': 3.602939822063402e-05, 'epoch': 0.28}
{'loss': 0.0504, 'learning_rate': 2.0855595268801356e-05, 'epoch': 0.58}
{'loss': 0.0924, 'learning_rate': 3.726353404925492e-05, 'epoch': 0.25}
{'loss': 0.0994, 'learning_rate': 3.4472913481552436e-05, 'epoch': 0.31}
{'loss': 0.0453, 'learning_rate': 1.2307012049080659e-05, 'epoch': 0.75}
{'loss': 0.031, 'learning_rate': 1.9086922878514315e-05, 'epoch': 0.62}
{'loss': 0.0429, 'learning_rate': 1.938170161022882e-05, 'epoch': 0.61}
{'loss': 0.055, 'learning_rate': 3.5430750244064185e-05, 'epoch': 0.29}
{'loss': 0.06, 'learning_rate': 3.756746302197499e-05, 'epoch': 0.25}
{'loss': 0.0664, 'learning_rate': 2.0708205902944104e-05, 'epoch': 0.59}
{'loss': 0.0361, 'learning_rate': 3.595571846967158e-05, 'epoch': 0.28}
{'loss': 0.0675, 'learning_rate': 3.439923373059e-05, 'epoch': 0.31}
{'loss': 0.0561, 'learning_rate': 3.718985429829247e-05, 'epoch': 0.26}
{'loss': 0.0516, 'learning_rate': 1.8939533512657063e-05, 'epoch': 0.62}
{'loss': 0.0663, 'learning_rate': 1.2159622683223406e-05, 'epoch': 0.76}
{'loss': 0.0681, 'learning_rate': 1.923431224437157e-05, 'epoch': 0.62}
{'loss': 0.0347, 'learning_rate': 3.535707049310173e-05, 'epoch': 0.29}
{'loss': 0.0628, 'learning_rate': 3.749378327101255e-05, 'epoch': 0.25}
{'loss': 0.0907, 'learning_rate': 3.432555397962755e-05, 'epoch': 0.31}
{'loss': 0.0461, 'learning_rate': 2.056081653708685e-05, 'epoch': 0.59}
{'loss': 0.0687, 'learning_rate': 3.588203871870913e-05, 'epoch': 0.28}
{'loss': 0.0702, 'learning_rate': 3.7116174547330035e-05, 'epoch': 0.26}
{'loss': 0.0662, 'learning_rate': 1.8792144146799808e-05, 'epoch': 0.62}
{'loss': 0.0571, 'learning_rate': 1.2012233317366152e-05, 'epoch': 0.76}
{'loss': 0.0754, 'learning_rate': 1.9086922878514315e-05, 'epoch': 0.62}
{'loss': 0.0345, 'learning_rate': 3.74201035200501e-05, 'epoch': 0.25}
{'loss': 0.0962, 'learning_rate': 3.5283390742139295e-05, 'epoch': 0.29}
{'loss': 0.0533, 'learning_rate': 3.425187422866511e-05, 'epoch': 0.32}
{'loss': 0.0376, 'learning_rate': 2.0413427171229595e-05, 'epoch': 0.59}
{'loss': 0.0809, 'learning_rate': 3.580835896774669e-05, 'epoch': 0.28}
{'loss': 0.0689, 'learning_rate': 3.70424947963676e-05, 'epoch': 0.26}
{'loss': 0.0686, 'learning_rate': 1.8644754780942557e-05, 'epoch': 0.63}
{'loss': 0.0532, 'learning_rate': 1.18648439515089e-05, 'epoch': 0.76}
{'loss': 0.0418, 'learning_rate': 1.8939533512657063e-05, 'epoch': 0.62}
{'loss': 0.0572, 'learning_rate': 3.734642376908766e-05, 'epoch': 0.25}
{'loss': 0.0697, 'learning_rate': 3.520971099117685e-05, 'epoch': 0.3}
{'loss': 0.0589, 'learning_rate': 3.417819447770267e-05, 'epoch': 0.32}
{'loss': 0.0871, 'learning_rate': 2.0266037805372343e-05, 'epoch': 0.6}
{'loss': 0.0741, 'learning_rate': 3.696881504540515e-05, 'epoch': 0.26}
{'loss': 0.0627, 'learning_rate': 3.573467921678425e-05, 'epoch': 0.29}
{'loss': 0.0723, 'learning_rate': 1.8497365415085302e-05, 'epoch': 0.63}
{'loss': 0.052, 'learning_rate': 1.1717454585651644e-05, 'epoch': 0.77}
{'loss': 0.038, 'learning_rate': 1.8792144146799808e-05, 'epoch': 0.62}
{'loss': 0.0747, 'learning_rate': 3.7272744018125224e-05, 'epoch': 0.25}
{'loss': 0.0547, 'learning_rate': 3.513603124021441e-05, 'epoch': 0.3}
{'loss': 0.0526, 'learning_rate': 3.4104514726740225e-05, 'epoch': 0.32}
{'loss': 0.0349, 'learning_rate': 2.011864843951509e-05, 'epoch': 0.6}
{'loss': 0.0484, 'learning_rate': 3.689513529444271e-05, 'epoch': 0.26}
{'loss': 0.0562, 'learning_rate': 3.566099946582181e-05, 'epoch': 0.29}
{'loss': 0.0595, 'learning_rate': 1.8349976049228047e-05, 'epoch': 0.63}
{'loss': 0.0554, 'learning_rate': 1.1570065219794391e-05, 'epoch': 0.77}
{'loss': 0.0598, 'learning_rate': 1.8644754780942557e-05, 'epoch': 0.63}
{'loss': 0.0554, 'learning_rate': 3.719906426716278e-05, 'epoch': 0.26}
{'loss': 0.0819, 'learning_rate': 3.403083497577779e-05, 'epoch': 0.32}
{'loss': 0.0669, 'learning_rate': 3.5062351489251974e-05, 'epoch': 0.3}
{'loss': 0.1041, 'learning_rate': 1.9971259073657837e-05, 'epoch': 0.6}
{'loss': 0.0807, 'learning_rate': 3.682145554348026e-05, 'epoch': 0.26}
{'loss': 0.0788, 'learning_rate': 3.558731971485937e-05, 'epoch': 0.29}
{'loss': 0.0685, 'learning_rate': 1.8202586683370796e-05, 'epoch': 0.64}
{'loss': 0.0342, 'learning_rate': 1.1422675853937138e-05, 'epoch': 0.77}
{'loss': 0.0308, 'learning_rate': 1.8497365415085302e-05, 'epoch': 0.63}
{'loss': 0.0562, 'learning_rate': 3.712538451620034e-05, 'epoch': 0.26}
{'loss': 0.0605, 'learning_rate': 3.395715522481535e-05, 'epoch': 0.32}
{'loss': 0.0783, 'learning_rate': 3.498867173828953e-05, 'epoch': 0.3}
{'loss': 0.0563, 'learning_rate': 1.9823869707800582e-05, 'epoch': 0.6}
{'loss': 0.0999, 'learning_rate': 3.6747775792517824e-05, 'epoch': 0.27}
{'loss': 0.0594, 'learning_rate': 3.551363996389693e-05, 'epoch': 0.29}
{'loss': 0.0654, 'learning_rate': 1.805519731751354e-05, 'epoch': 0.64}
{'loss': 0.0664, 'learning_rate': 1.1275286488079885e-05, 'epoch': 0.78}
{'loss': 0.0531, 'learning_rate': 3.70517047652379e-05, 'epoch': 0.26}
{'loss': 0.0593, 'learning_rate': 1.8349976049228047e-05, 'epoch': 0.63}
{'loss': 0.0534, 'learning_rate': 3.3883475473852904e-05, 'epoch': 0.32}
{'loss': 0.0908, 'learning_rate': 3.491499198732709e-05, 'epoch': 0.3}
{'loss': 0.0746, 'learning_rate': 1.967648034194333e-05, 'epoch': 0.61}
{'loss': 0.0549, 'learning_rate': 3.6674096041555386e-05, 'epoch': 0.27}
{'loss': 0.0572, 'learning_rate': 3.5439960212934484e-05, 'epoch': 0.29}
{'loss': 0.0472, 'learning_rate': 1.790780795165629e-05, 'epoch': 0.64}
{'loss': 0.0562, 'learning_rate': 1.1127897122222632e-05, 'epoch': 0.78}
{'loss': 0.0567, 'learning_rate': 3.697802501427546e-05, 'epoch': 0.26}
{'loss': 0.0703, 'learning_rate': 1.8202586683370796e-05, 'epoch': 0.64}
{'loss': 0.0921, 'learning_rate': 3.380979572289046e-05, 'epoch': 0.32}
{'loss': 0.0815, 'learning_rate': 3.484131223636464e-05, 'epoch': 0.3}
{'loss': 0.0556, 'learning_rate': 1.9529090976086076e-05, 'epoch': 0.61}
{'loss': 0.0985, 'learning_rate': 3.660041629059294e-05, 'epoch': 0.27}
{'loss': 0.0603, 'learning_rate': 3.536628046197204e-05, 'epoch': 0.29}
{'loss': 0.0564, 'learning_rate': 1.7760418585799034e-05, 'epoch': 0.65}
{'loss': 0.0425, 'learning_rate': 1.0980507756365379e-05, 'epoch': 0.78}
{'loss': 0.0679, 'learning_rate': 3.6904345263313014e-05, 'epoch': 0.26}
{'loss': 0.061, 'learning_rate': 3.3736115971928014e-05, 'epoch': 0.33}
{'loss': 0.0532, 'learning_rate': 1.805519731751354e-05, 'epoch': 0.64}
{'loss': 0.0605, 'learning_rate': 3.47676324854022e-05, 'epoch': 0.31}
{'loss': 0.0731, 'learning_rate': 3.6526736539630504e-05, 'epoch': 0.27}
{'loss': 0.0385, 'learning_rate': 1.938170161022882e-05, 'epoch': 0.61}
{'loss': 0.0503, 'learning_rate': 3.52926007110096e-05, 'epoch': 0.29}
{'loss': 0.0524, 'learning_rate': 3.683066551235057e-05, 'epoch': 0.26}
{'loss': 0.0496, 'learning_rate': 1.7613029219941783e-05, 'epoch': 0.65}
{'loss': 0.0285, 'learning_rate': 1.0833118390508125e-05, 'epoch': 0.78}
{'loss': 0.0801, 'learning_rate': 3.3662436220965576e-05, 'epoch': 0.33}
{'loss': 0.0405, 'learning_rate': 1.790780795165629e-05, 'epoch': 0.64}
{'loss': 0.0524, 'learning_rate': 3.4693952734439763e-05, 'epoch': 0.31}
{'loss': 0.1114, 'learning_rate': 3.645305678866806e-05, 'epoch': 0.27}
{'loss': 0.0798, 'learning_rate': 1.923431224437157e-05, 'epoch': 0.62}
{'loss': 0.1134, 'learning_rate': 3.521892096004716e-05, 'epoch': 0.3}
{'loss': 0.0626, 'learning_rate': 3.675698576138813e-05, 'epoch': 0.27}
{'loss': 0.0571, 'learning_rate': 1.7465639854084528e-05, 'epoch': 0.65}
{'loss': 0.0627, 'learning_rate': 3.358875647000314e-05, 'epoch': 0.33}
{'loss': 0.0613, 'learning_rate': 1.0685729024650872e-05, 'epoch': 0.79}
{'loss': 0.0544, 'learning_rate': 1.7760418585799034e-05, 'epoch': 0.65}
{'loss': 0.0571, 'learning_rate': 3.462027298347732e-05, 'epoch': 0.31}
{'loss': 0.0621, 'learning_rate': 3.6379377037705614e-05, 'epoch': 0.27}
{'loss': 0.0373, 'learning_rate': 1.9086922878514315e-05, 'epoch': 0.62}
{'loss': 0.072, 'learning_rate': 3.514524120908472e-05, 'epoch': 0.3}
{'loss': 0.0766, 'learning_rate': 3.6683306010425686e-05, 'epoch': 0.27}
{'loss': 0.0829, 'learning_rate': 3.3515076719040693e-05, 'epoch': 0.33}
{'loss': 0.0589, 'learning_rate': 1.7318250488227277e-05, 'epoch': 0.65}
{'loss': 0.0446, 'learning_rate': 1.0538339658793617e-05, 'epoch': 0.79}
{'loss': 0.0653, 'learning_rate': 1.7613029219941783e-05, 'epoch': 0.65}
{'loss': 0.0401, 'learning_rate': 3.454659323251488e-05, 'epoch': 0.31}
{'loss': 0.0879, 'learning_rate': 3.6305697286743176e-05, 'epoch': 0.27}
{'loss': 0.0735, 'learning_rate': 1.8939533512657063e-05, 'epoch': 0.62}
{'loss': 0.0985, 'learning_rate': 3.660962625946325e-05, 'epoch': 0.27}
{'loss': 0.0428, 'learning_rate': 3.5071561458122274e-05, 'epoch': 0.3}
{'loss': 0.0708, 'learning_rate': 3.3441396968078255e-05, 'epoch': 0.33}
{'loss': 0.0835, 'learning_rate': 1.717086112237002e-05, 'epoch': 0.66}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0694, 'learning_rate': 1.0390950292936364e-05, 'epoch': 0.79}
{'loss': 0.0688, 'learning_rate': 1.7465639854084528e-05, 'epoch': 0.65}
{'loss': 0.0863, 'learning_rate': 3.4472913481552436e-05, 'epoch': 0.31}
{'loss': 0.0633, 'learning_rate': 3.623201753578073e-05, 'epoch': 0.28}
{'loss': 0.048, 'learning_rate': 1.8792144146799808e-05, 'epoch': 0.62}
{'loss': 0.0562, 'learning_rate': 3.336771721711581e-05, 'epoch': 0.33}
{'loss': 0.0587, 'learning_rate': 3.653594650850081e-05, 'epoch': 0.27}
{'loss': 0.065, 'learning_rate': 3.4997881707159836e-05, 'epoch': 0.3}
{'loss': 0.0707, 'learning_rate': 1.7023471756512767e-05, 'epoch': 0.66}
{'loss': 0.0289, 'learning_rate': 1.0243560927079111e-05, 'epoch': 0.8}
{'loss': 0.0594, 'learning_rate': 1.7318250488227277e-05, 'epoch': 0.65}
{'loss': 0.0682, 'learning_rate': 3.439923373059e-05, 'epoch': 0.31}
{'loss': 0.0505, 'learning_rate': 3.615833778481829e-05, 'epoch': 0.28}
{'loss': 0.0486, 'learning_rate': 1.8644754780942557e-05, 'epoch': 0.63}
{'loss': 0.0639, 'learning_rate': 3.3294037466153366e-05, 'epoch': 0.33}
{'loss': 0.0562, 'learning_rate': 3.6462266757538365e-05, 'epoch': 0.27}
{'loss': 0.0691, 'learning_rate': 3.49242019561974e-05, 'epoch': 0.3}
{'loss': 0.0564, 'learning_rate': 1.6876082390655515e-05, 'epoch': 0.66}
{'loss': 0.0277, 'learning_rate': 1.0096171561221858e-05, 'epoch': 0.8}
{'loss': 0.0492, 'learning_rate': 1.717086112237002e-05, 'epoch': 0.66}
{'loss': 0.0516, 'learning_rate': 3.432555397962755e-05, 'epoch': 0.31}
{'loss': 0.0232, 'learning_rate': 3.608465803385585e-05, 'epoch': 0.28}
{'loss': 0.0657, 'learning_rate': 3.322035771519093e-05, 'epoch': 0.34}
{'loss': 0.0733, 'learning_rate': 1.8497365415085302e-05, 'epoch': 0.63}
{'loss': 0.1032, 'learning_rate': 3.638858700657592e-05, 'epoch': 0.27}
{'loss': 0.0684, 'learning_rate': 3.4850522205234946e-05, 'epoch': 0.3}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0633, 'learning_rate': 1.672869302479826e-05, 'epoch': 0.67}
{'loss': 0.0677, 'learning_rate': 9.948782195364605e-06, 'epoch': 0.8}
{'loss': 0.0451, 'learning_rate': 1.7023471756512767e-05, 'epoch': 0.66}
{'loss': 0.0754, 'learning_rate': 3.425187422866511e-05, 'epoch': 0.32}
{'loss': 0.0323, 'learning_rate': 3.314667796422848e-05, 'epoch': 0.34}
{'loss': 0.0991, 'learning_rate': 3.601097828289341e-05, 'epoch': 0.28}
{'loss': 0.0706, 'learning_rate': 3.6314907255613475e-05, 'epoch': 0.27}
{'loss': 0.0769, 'learning_rate': 1.8349976049228047e-05, 'epoch': 0.63}
{'loss': 0.0429, 'learning_rate': 3.477684245427251e-05, 'epoch': 0.31}
{'loss': 0.0569, 'learning_rate': 1.658130365894101e-05, 'epoch': 0.67}
{'loss': 0.0608, 'learning_rate': 9.801392829507352e-06, 'epoch': 0.8}
{'loss': 0.0872, 'learning_rate': 1.6876082390655515e-05, 'epoch': 0.66}
{'loss': 0.0468, 'learning_rate': 3.417819447770267e-05, 'epoch': 0.32}
{'loss': 0.065, 'learning_rate': 3.593729853193097e-05, 'epoch': 0.28}
{'loss': 0.0546, 'learning_rate': 1.8202586683370796e-05, 'epoch': 0.64}
{'loss': 0.053, 'learning_rate': 3.3072998213266045e-05, 'epoch': 0.34}
{'loss': 0.0712, 'learning_rate': 3.624122750465104e-05, 'epoch': 0.28}
{'loss': 0.0766, 'learning_rate': 3.470316270331006e-05, 'epoch': 0.31}
{'loss': 0.0566, 'learning_rate': 1.6433914293083754e-05, 'epoch': 0.67}
{'loss': 0.0328, 'learning_rate': 9.654003463650098e-06, 'epoch': 0.81}
{'loss': 0.0632, 'learning_rate': 1.672869302479826e-05, 'epoch': 0.67}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0721, 'learning_rate': 3.4104514726740225e-05, 'epoch': 0.32}
{'loss': 0.0816, 'learning_rate': 3.586361878096852e-05, 'epoch': 0.28}
{'loss': 0.0557, 'learning_rate': 1.805519731751354e-05, 'epoch': 0.64}
{'loss': 0.0558, 'learning_rate': 3.29993184623036e-05, 'epoch': 0.34}
{'loss': 0.0725, 'learning_rate': 3.61675477536886e-05, 'epoch': 0.28}
{'loss': 0.0531, 'learning_rate': 3.4629482952347625e-05, 'epoch': 0.31}
{'loss': 0.0688, 'learning_rate': 1.6286524927226503e-05, 'epoch': 0.67}
{'loss': 0.0294, 'learning_rate': 9.506614097792845e-06, 'epoch': 0.81}
{'loss': 0.0406, 'learning_rate': 1.658130365894101e-05, 'epoch': 0.67}
{'loss': 0.0535, 'learning_rate': 3.403083497577779e-05, 'epoch': 0.32}
{'loss': 0.1015, 'learning_rate': 3.578993903000608e-05, 'epoch': 0.28}
{'loss': 0.0638, 'learning_rate': 3.292563871134116e-05, 'epoch': 0.34}
{'loss': 0.028, 'learning_rate': 1.790780795165629e-05, 'epoch': 0.64}
{'loss': 0.0725, 'learning_rate': 3.6093868002726154e-05, 'epoch': 0.28}
{'loss': 0.0638, 'learning_rate': 1.6139135561369248e-05, 'epoch': 0.68}
{'loss': 0.064, 'learning_rate': 3.455580320138519e-05, 'epoch': 0.31}
{'loss': 0.0544, 'learning_rate': 9.35922473193559e-06, 'epoch': 0.81}
{'loss': 0.044, 'learning_rate': 1.6433914293083754e-05, 'epoch': 0.67}
{'loss': 0.0558, 'learning_rate': 3.395715522481535e-05, 'epoch': 0.32}
{'loss': 0.0668, 'learning_rate': 3.571625927904364e-05, 'epoch': 0.29}
{'loss': 0.071, 'learning_rate': 3.285195896037872e-05, 'epoch': 0.34}
{'loss': 0.0556, 'learning_rate': 1.7760418585799034e-05, 'epoch': 0.65}
{'loss': 0.0682, 'learning_rate': 3.6020188251763716e-05, 'epoch': 0.28}
{'loss': 0.0788, 'learning_rate': 1.5991746195511993e-05, 'epoch': 0.68}
{'loss': 0.0421, 'learning_rate': 3.448212345042274e-05, 'epoch': 0.31}
{'loss': 0.0365, 'learning_rate': 9.211835366078337e-06, 'epoch': 0.82}
{'loss': 0.0941, 'learning_rate': 1.6286524927226503e-05, 'epoch': 0.67}
{'loss': 0.0609, 'learning_rate': 3.3883475473852904e-05, 'epoch': 0.32}
{'loss': 0.0651, 'learning_rate': 3.277827920941627e-05, 'epoch': 0.34}
{'loss': 0.0656, 'learning_rate': 3.56425795280812e-05, 'epoch': 0.29}
{'loss': 0.0617, 'learning_rate': 1.7613029219941783e-05, 'epoch': 0.65}
{'loss': 0.0868, 'learning_rate': 3.594650850080127e-05, 'epoch': 0.28}
{'loss': 0.0552, 'learning_rate': 1.584435682965474e-05, 'epoch': 0.68}
{'loss': 0.0657, 'learning_rate': 3.4408443699460304e-05, 'epoch': 0.31}
{'loss': 0.0624, 'learning_rate': 9.064446000221084e-06, 'epoch': 0.82}
{'loss': 0.0608, 'learning_rate': 1.6139135561369248e-05, 'epoch': 0.68}
{'loss': 0.0748, 'learning_rate': 3.2704599458453834e-05, 'epoch': 0.35}
{'loss': 0.0568, 'learning_rate': 3.556889977711876e-05, 'epoch': 0.29}
{'loss': 0.0793, 'learning_rate': 3.380979572289046e-05, 'epoch': 0.32}
{'loss': 0.0524, 'learning_rate': 1.7465639854084528e-05, 'epoch': 0.65}
{'loss': 0.0709, 'learning_rate': 3.587282874983883e-05, 'epoch': 0.28}
{'loss': 0.063, 'learning_rate': 1.5696967463797487e-05, 'epoch': 0.69}
{'loss': 0.13, 'learning_rate': 3.433476394849785e-05, 'epoch': 0.31}
{'loss': 0.0281, 'learning_rate': 8.917056634363831e-06, 'epoch': 0.82}
{'loss': 0.0401, 'learning_rate': 1.5991746195511993e-05, 'epoch': 0.68}
{'loss': 0.0967, 'learning_rate': 3.263091970749139e-05, 'epoch': 0.35}
{'loss': 0.0825, 'learning_rate': 3.5495220026156316e-05, 'epoch': 0.29}
{'loss': 0.0703, 'learning_rate': 3.3736115971928014e-05, 'epoch': 0.33}
{'loss': 0.0667, 'learning_rate': 3.579914899887639e-05, 'epoch': 0.28}
{'loss': 0.046, 'learning_rate': 1.7318250488227277e-05, 'epoch': 0.65}
{'loss': 0.0346, 'learning_rate': 1.5549578097940235e-05, 'epoch': 0.69}
{'loss': 0.1009, 'learning_rate': 3.4261084197535414e-05, 'epoch': 0.32}
{'loss': 0.0541, 'learning_rate': 8.769667268506578e-06, 'epoch': 0.83}
{'loss': 0.0267, 'learning_rate': 1.584435682965474e-05, 'epoch': 0.68}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0594, 'learning_rate': 3.255723995652895e-05, 'epoch': 0.35}
{'loss': 0.0477, 'learning_rate': 3.5725469247913944e-05, 'epoch': 0.29}
{'loss': 0.0556, 'learning_rate': 3.542154027519388e-05, 'epoch': 0.29}
{'loss': 0.0717, 'learning_rate': 3.3662436220965576e-05, 'epoch': 0.33}
{'loss': 0.0313, 'learning_rate': 1.717086112237002e-05, 'epoch': 0.66}
{'loss': 0.0396, 'learning_rate': 1.540218873208298e-05, 'epoch': 0.69}
{'loss': 0.065, 'learning_rate': 3.4187404446572976e-05, 'epoch': 0.32}
{'loss': 0.0574, 'learning_rate': 8.622277902649325e-06, 'epoch': 0.83}
{'loss': 0.0677, 'learning_rate': 1.5696967463797487e-05, 'epoch': 0.69}
{'loss': 0.0601, 'learning_rate': 3.248356020556651e-05, 'epoch': 0.35}
{'loss': 0.1021, 'learning_rate': 3.5651789496951506e-05, 'epoch': 0.29}
{'loss': 0.0617, 'learning_rate': 3.534786052423143e-05, 'epoch': 0.29}
{'loss': 0.0842, 'learning_rate': 3.358875647000314e-05, 'epoch': 0.33}
{'loss': 0.0318, 'learning_rate': 1.7023471756512767e-05, 'epoch': 0.66}
{'loss': 0.0631, 'learning_rate': 3.411372469561053e-05, 'epoch': 0.32}
{'loss': 0.0471, 'learning_rate': 1.5254799366225727e-05, 'epoch': 0.7}
{'loss': 0.0291, 'learning_rate': 8.474888536792071e-06, 'epoch': 0.83}
{'loss': 0.0742, 'learning_rate': 3.240988045460407e-05, 'epoch': 0.35}
{'loss': 0.0524, 'learning_rate': 1.5549578097940235e-05, 'epoch': 0.69}
{'loss': 0.0818, 'learning_rate': 3.557810974598906e-05, 'epoch': 0.29}
{'loss': 0.0742, 'learning_rate': 3.527418077326899e-05, 'epoch': 0.29}
{'loss': 0.0552, 'learning_rate': 3.3515076719040693e-05, 'epoch': 0.33}
{'loss': 0.0469, 'learning_rate': 1.6876082390655515e-05, 'epoch': 0.66}
{'loss': 0.0707, 'learning_rate': 3.4040044944648093e-05, 'epoch': 0.32}
{'loss': 0.0707, 'learning_rate': 1.5107410000368474e-05, 'epoch': 0.7}
{'loss': 0.078, 'learning_rate': 3.233620070364163e-05, 'epoch': 0.35}
{'loss': 0.0445, 'learning_rate': 8.327499170934818e-06, 'epoch': 0.83}
{'loss': 0.0568, 'learning_rate': 1.540218873208298e-05, 'epoch': 0.69}
{'loss': 0.0526, 'learning_rate': 3.550442999502662e-05, 'epoch': 0.29}
{'loss': 0.0625, 'learning_rate': 3.520050102230655e-05, 'epoch': 0.3}
{'loss': 0.0459, 'learning_rate': 3.3441396968078255e-05, 'epoch': 0.33}
{'loss': 0.0895, 'learning_rate': 1.672869302479826e-05, 'epoch': 0.67}
{'loss': 0.0617, 'learning_rate': 3.226252095267918e-05, 'epoch': 0.36}
{'loss': 0.0786, 'learning_rate': 3.396636519368565e-05, 'epoch': 0.32}
{'loss': 0.047, 'learning_rate': 1.496002063451122e-05, 'epoch': 0.7}
{'loss': 0.0348, 'learning_rate': 8.180109805077563e-06, 'epoch': 0.84}
{'loss': 0.071, 'learning_rate': 3.5430750244064185e-05, 'epoch': 0.29}
{'loss': 0.0354, 'learning_rate': 1.5254799366225727e-05, 'epoch': 0.7}
{'loss': 0.0922, 'learning_rate': 3.5126821271344106e-05, 'epoch': 0.3}
{'loss': 0.0601, 'learning_rate': 3.336771721711581e-05, 'epoch': 0.33}
{'loss': 0.0795, 'learning_rate': 1.658130365894101e-05, 'epoch': 0.67}
{'loss': 0.0581, 'learning_rate': 3.218884120171674e-05, 'epoch': 0.36}
{'loss': 0.0668, 'learning_rate': 1.4812631268653968e-05, 'epoch': 0.7}
{'loss': 0.087, 'learning_rate': 3.389268544272321e-05, 'epoch': 0.32}
{'loss': 0.076, 'learning_rate': 3.535707049310173e-05, 'epoch': 0.29}
{'loss': 0.0518, 'learning_rate': 8.03272043922031e-06, 'epoch': 0.84}
{'loss': 0.0627, 'learning_rate': 1.5107410000368474e-05, 'epoch': 0.7}
{'loss': 0.0516, 'learning_rate': 3.505314152038167e-05, 'epoch': 0.3}
{'loss': 0.1076, 'learning_rate': 3.3294037466153366e-05, 'epoch': 0.33}
{'loss': 0.0317, 'learning_rate': 1.6433914293083754e-05, 'epoch': 0.67}
{'loss': 0.0462, 'learning_rate': 3.21151614507543e-05, 'epoch': 0.36}
{'loss': 0.0298, 'learning_rate': 1.4665241902796714e-05, 'epoch': 0.71}
{'loss': 0.0546, 'learning_rate': 3.3819005691760766e-05, 'epoch': 0.32}
{'loss': 0.1015, 'learning_rate': 3.5283390742139295e-05, 'epoch': 0.29}
{'loss': 0.0961, 'learning_rate': 7.885331073363057e-06, 'epoch': 0.84}
{'loss': 0.0427, 'learning_rate': 1.496002063451122e-05, 'epoch': 0.7}
{'loss': 0.0539, 'learning_rate': 3.497946176941922e-05, 'epoch': 0.3}
{'loss': 0.05, 'learning_rate': 3.322035771519093e-05, 'epoch': 0.34}
{'loss': 0.0618, 'learning_rate': 1.6286524927226503e-05, 'epoch': 0.67}
{'loss': 0.055, 'learning_rate': 3.204148169979186e-05, 'epoch': 0.36}
{'loss': 0.0919, 'learning_rate': 1.4517852536939461e-05, 'epoch': 0.71}
{'loss': 0.0869, 'learning_rate': 3.520971099117685e-05, 'epoch': 0.3}
{'loss': 0.0654, 'learning_rate': 3.374532594079832e-05, 'epoch': 0.33}
{'loss': 0.0491, 'learning_rate': 7.737941707505804e-06, 'epoch': 0.85}
{'loss': 0.0849, 'learning_rate': 1.4812631268653968e-05, 'epoch': 0.7}
{'loss': 0.0509, 'learning_rate': 3.4905782018456785e-05, 'epoch': 0.3}
{'loss': 0.0735, 'learning_rate': 3.196780194882942e-05, 'epoch': 0.36}
{'loss': 0.0391, 'learning_rate': 1.6139135561369248e-05, 'epoch': 0.68}
{'loss': 0.0455, 'learning_rate': 3.314667796422848e-05, 'epoch': 0.34}
{'loss': 0.0554, 'learning_rate': 3.513603124021441e-05, 'epoch': 0.3}
{'loss': 0.0699, 'learning_rate': 1.4370463171082208e-05, 'epoch': 0.71}
{'loss': 0.0829, 'learning_rate': 3.367164618983588e-05, 'epoch': 0.33}
{'loss': 0.0199, 'learning_rate': 7.590552341648551e-06, 'epoch': 0.85}
{'loss': 0.0417, 'learning_rate': 1.4665241902796714e-05, 'epoch': 0.71}
{'loss': 0.0524, 'learning_rate': 3.1894122197866975e-05, 'epoch': 0.36}
{'loss': 0.0448, 'learning_rate': 3.483210226749434e-05, 'epoch': 0.3}
{'loss': 0.0523, 'learning_rate': 1.5991746195511993e-05, 'epoch': 0.68}
{'loss': 0.068, 'learning_rate': 3.3072998213266045e-05, 'epoch': 0.34}
{'loss': 0.0699, 'learning_rate': 3.5062351489251974e-05, 'epoch': 0.3}
{'loss': 0.0527, 'learning_rate': 1.4223073805224953e-05, 'epoch': 0.72}
{'loss': 0.0585, 'learning_rate': 3.359796643887344e-05, 'epoch': 0.33}
{'loss': 0.0687, 'learning_rate': 7.4431629757912975e-06, 'epoch': 0.85}
{'loss': 0.0793, 'learning_rate': 1.4517852536939461e-05, 'epoch': 0.71}
{'loss': 0.0456, 'learning_rate': 3.182044244690453e-05, 'epoch': 0.36}
{'loss': 0.0899, 'learning_rate': 3.4758422516531895e-05, 'epoch': 0.31}
{'loss': 0.0493, 'learning_rate': 1.584435682965474e-05, 'epoch': 0.68}
{'loss': 0.0639, 'learning_rate': 3.29993184623036e-05, 'epoch': 0.34}
{'loss': 0.0529, 'learning_rate': 3.498867173828953e-05, 'epoch': 0.3}
{'loss': 0.071, 'learning_rate': 1.40756844393677e-05, 'epoch': 0.72}
{'loss': 0.0658, 'learning_rate': 3.3524286687911e-05, 'epoch': 0.33}
{'loss': 0.0525, 'learning_rate': 7.2957736099340435e-06, 'epoch': 0.85}
{'loss': 0.0721, 'learning_rate': 1.4370463171082208e-05, 'epoch': 0.71}
{'loss': 0.0364, 'learning_rate': 3.174676269594209e-05, 'epoch': 0.37}
{'loss': 0.0707, 'learning_rate': 3.468474276556946e-05, 'epoch': 0.31}
{'loss': 0.0542, 'learning_rate': 1.5696967463797487e-05, 'epoch': 0.69}
{'loss': 0.0655, 'learning_rate': 3.292563871134116e-05, 'epoch': 0.34}
{'loss': 0.0809, 'learning_rate': 3.491499198732709e-05, 'epoch': 0.3}
{'loss': 0.0304, 'learning_rate': 1.3928295073510447e-05, 'epoch': 0.72}
{'loss': 0.0609, 'learning_rate': 3.345060693694856e-05, 'epoch': 0.33}
{'loss': 0.0281, 'learning_rate': 7.14838424407679e-06, 'epoch': 0.86}
{'loss': 0.0253, 'learning_rate': 1.4223073805224953e-05, 'epoch': 0.72}
{'loss': 0.056, 'learning_rate': 3.167308294497965e-05, 'epoch': 0.37}
{'loss': 0.095, 'learning_rate': 3.461106301460701e-05, 'epoch': 0.31}
{'loss': 0.0478, 'learning_rate': 1.5549578097940235e-05, 'epoch': 0.69}
{'loss': 0.0564, 'learning_rate': 3.285195896037872e-05, 'epoch': 0.34}
{'loss': 0.0814, 'learning_rate': 3.484131223636464e-05, 'epoch': 0.3}
{'loss': 0.0258, 'learning_rate': 1.3780905707653194e-05, 'epoch': 0.72}
{'loss': 0.0611, 'learning_rate': 3.337692718598612e-05, 'epoch': 0.33}
{'loss': 0.046, 'learning_rate': 7.000994878219537e-06, 'epoch': 0.86}
{'loss': 0.0619, 'learning_rate': 1.40756844393677e-05, 'epoch': 0.72}
{'loss': 0.0476, 'learning_rate': 3.159940319401721e-05, 'epoch': 0.37}
{'loss': 0.0689, 'learning_rate': 3.4537383263644574e-05, 'epoch': 0.31}
{'loss': 0.0923, 'learning_rate': 1.540218873208298e-05, 'epoch': 0.69}
{'loss': 0.0507, 'learning_rate': 3.47676324854022e-05, 'epoch': 0.31}
{'loss': 0.0946, 'learning_rate': 3.277827920941627e-05, 'epoch': 0.34}
{'loss': 0.0413, 'learning_rate': 1.363351634179594e-05, 'epoch': 0.73}
{'loss': 0.0526, 'learning_rate': 3.330324743502367e-05, 'epoch': 0.33}
{'loss': 0.0358, 'learning_rate': 6.853605512362283e-06, 'epoch': 0.86}
{'loss': 0.0575, 'learning_rate': 1.3928295073510447e-05, 'epoch': 0.72}
{'loss': 0.0809, 'learning_rate': 3.1525723443054764e-05, 'epoch': 0.37}
{'loss': 0.0704, 'learning_rate': 3.4463703512682136e-05, 'epoch': 0.31}
{'loss': 0.0704, 'learning_rate': 1.5254799366225727e-05, 'epoch': 0.7}
{'loss': 0.0488, 'learning_rate': 3.2704599458453834e-05, 'epoch': 0.35}
{'loss': 0.0604, 'learning_rate': 3.4693952734439763e-05, 'epoch': 0.31}
{'loss': 0.0357, 'learning_rate': 1.3486126975938687e-05, 'epoch': 0.73}
{'loss': 0.0477, 'learning_rate': 3.322956768406123e-05, 'epoch': 0.34}
{'loss': 0.0602, 'learning_rate': 6.70621614650503e-06, 'epoch': 0.87}
{'loss': 0.0206, 'learning_rate': 1.3780905707653194e-05, 'epoch': 0.72}
{'loss': 0.044, 'learning_rate': 3.1452043692092326e-05, 'epoch': 0.37}
{'loss': 0.0819, 'learning_rate': 3.439002376171969e-05, 'epoch': 0.31}
{'loss': 0.048, 'learning_rate': 1.5107410000368474e-05, 'epoch': 0.7}
{'loss': 0.0538, 'learning_rate': 3.263091970749139e-05, 'epoch': 0.35}
{'loss': 0.0835, 'learning_rate': 3.462027298347732e-05, 'epoch': 0.31}
{'loss': 0.0755, 'learning_rate': 1.3338737610081434e-05, 'epoch': 0.73}
{'loss': 0.0633, 'learning_rate': 6.558826780647777e-06, 'epoch': 0.87}
{'loss': 0.073, 'learning_rate': 3.315588793309879e-05, 'epoch': 0.34}
{'loss': 0.0503, 'learning_rate': 1.363351634179594e-05, 'epoch': 0.73}
{'loss': 0.0625, 'learning_rate': 3.137836394112989e-05, 'epoch': 0.37}
{'loss': 0.043, 'learning_rate': 3.4316344010757246e-05, 'epoch': 0.31}
{'loss': 0.0398, 'learning_rate': 1.496002063451122e-05, 'epoch': 0.7}
{'loss': 0.0761, 'learning_rate': 3.255723995652895e-05, 'epoch': 0.35}
{'loss': 0.0671, 'learning_rate': 3.454659323251488e-05, 'epoch': 0.31}
{'loss': 0.0665, 'learning_rate': 1.3191348244224181e-05, 'epoch': 0.74}
{'loss': 0.0518, 'learning_rate': 6.411437414790524e-06, 'epoch': 0.87}
{'loss': 0.0448, 'learning_rate': 3.308220818213635e-05, 'epoch': 0.34}
{'loss': 0.0894, 'learning_rate': 3.1304684190167436e-05, 'epoch': 0.37}
{'loss': 0.0472, 'learning_rate': 1.3486126975938687e-05, 'epoch': 0.73}
{'loss': 0.1045, 'learning_rate': 3.42426642597948e-05, 'epoch': 0.32}
{'loss': 0.0295, 'learning_rate': 1.4812631268653968e-05, 'epoch': 0.7}
{'loss': 0.07, 'learning_rate': 3.4472913481552436e-05, 'epoch': 0.31}
{'loss': 0.098, 'learning_rate': 3.248356020556651e-05, 'epoch': 0.35}
{'loss': 0.0511, 'learning_rate': 1.3043958878366926e-05, 'epoch': 0.74}
{'loss': 0.0526, 'learning_rate': 3.1231004439205e-05, 'epoch': 0.38}
{'loss': 0.0982, 'learning_rate': 6.26404804893327e-06, 'epoch': 0.88}
{'loss': 0.0545, 'learning_rate': 3.3008528431173906e-05, 'epoch': 0.34}
{'loss': 0.0816, 'learning_rate': 1.3338737610081434e-05, 'epoch': 0.73}
{'loss': 0.0634, 'learning_rate': 3.4168984508832363e-05, 'epoch': 0.32}
{'loss': 0.0566, 'learning_rate': 1.4665241902796714e-05, 'epoch': 0.71}
{'loss': 0.042, 'learning_rate': 3.439923373059e-05, 'epoch': 0.31}
{'loss': 0.0534, 'learning_rate': 3.240988045460407e-05, 'epoch': 0.35}
{'loss': 0.037, 'learning_rate': 3.1157324688242553e-05, 'epoch': 0.38}
{'loss': 0.0989, 'learning_rate': 1.2896569512509673e-05, 'epoch': 0.74}
{'loss': 0.0552, 'learning_rate': 6.116658683076016e-06, 'epoch': 0.88}
{'loss': 0.0572, 'learning_rate': 3.293484868021147e-05, 'epoch': 0.34}
{'loss': 0.0444, 'learning_rate': 1.3191348244224181e-05, 'epoch': 0.74}
{'loss': 0.0899, 'learning_rate': 3.4095304757869925e-05, 'epoch': 0.32}
{'loss': 0.0809, 'learning_rate': 3.432555397962755e-05, 'epoch': 0.31}
{'loss': 0.0653, 'learning_rate': 1.4517852536939461e-05, 'epoch': 0.71}
{'loss': 0.0694, 'learning_rate': 3.233620070364163e-05, 'epoch': 0.35}
{'loss': 0.1212, 'learning_rate': 3.1083644937280115e-05, 'epoch': 0.38}
{'loss': 0.0531, 'learning_rate': 1.274918014665242e-05, 'epoch': 0.75}
{'loss': 0.0473, 'learning_rate': 5.9692693172187625e-06, 'epoch': 0.88}
{'loss': 0.0328, 'learning_rate': 3.2861168929249023e-05, 'epoch': 0.34}
{'loss': 0.0476, 'learning_rate': 1.3043958878366926e-05, 'epoch': 0.74}
{'loss': 0.0598, 'learning_rate': 3.425187422866511e-05, 'epoch': 0.32}
{'loss': 0.0842, 'learning_rate': 3.402162500690748e-05, 'epoch': 0.32}
{'loss': 0.0447, 'learning_rate': 1.4370463171082208e-05, 'epoch': 0.71}
{'loss': 0.0466, 'learning_rate': 3.226252095267918e-05, 'epoch': 0.36}
{'loss': 0.0583, 'learning_rate': 3.100996518631768e-05, 'epoch': 0.38}
{'loss': 0.0519, 'learning_rate': 1.2601790780795167e-05, 'epoch': 0.75}
{'loss': 0.0577, 'learning_rate': 5.821879951361509e-06, 'epoch': 0.88}
{'loss': 0.0819, 'learning_rate': 3.278748917828658e-05, 'epoch': 0.34}
{'loss': 0.0436, 'learning_rate': 1.2896569512509673e-05, 'epoch': 0.74}
{'loss': 0.0539, 'learning_rate': 3.417819447770267e-05, 'epoch': 0.32}
{'loss': 0.048, 'learning_rate': 3.394794525594504e-05, 'epoch': 0.32}
{'loss': 0.0573, 'learning_rate': 1.4223073805224953e-05, 'epoch': 0.72}
{'loss': 0.0261, 'learning_rate': 3.093628543535523e-05, 'epoch': 0.38}
{'loss': 0.0811, 'learning_rate': 3.218884120171674e-05, 'epoch': 0.36}
{'loss': 0.0564, 'learning_rate': 1.2454401414937912e-05, 'epoch': 0.75}
{'loss': 0.0241, 'learning_rate': 5.674490585504256e-06, 'epoch': 0.89}
{'loss': 0.079, 'learning_rate': 3.271380942732414e-05, 'epoch': 0.35}
{'loss': 0.0128, 'learning_rate': 1.274918014665242e-05, 'epoch': 0.75}
{'loss': 0.0422, 'learning_rate': 3.4104514726740225e-05, 'epoch': 0.32}
{'loss': 0.1023, 'learning_rate': 3.38742655049826e-05, 'epoch': 0.32}
{'loss': 0.0316, 'learning_rate': 1.40756844393677e-05, 'epoch': 0.72}
{'loss': 0.0683, 'learning_rate': 3.0862605684392794e-05, 'epoch': 0.38}
{'loss': 0.0429, 'learning_rate': 3.21151614507543e-05, 'epoch': 0.36}
{'loss': 0.046, 'learning_rate': 1.2307012049080659e-05, 'epoch': 0.75}
{'loss': 0.0904, 'learning_rate': 3.403083497577779e-05, 'epoch': 0.32}
{'loss': 0.0564, 'learning_rate': 5.527101219647002e-06, 'epoch': 0.89}
{'loss': 0.062, 'learning_rate': 3.2640129676361696e-05, 'epoch': 0.35}
{'loss': 0.0825, 'learning_rate': 1.2601790780795167e-05, 'epoch': 0.75}
{'loss': 0.064, 'learning_rate': 3.380058575402015e-05, 'epoch': 0.32}
{'loss': 0.0737, 'learning_rate': 3.078892593343034e-05, 'epoch': 0.38}
{'loss': 0.0573, 'learning_rate': 1.3928295073510447e-05, 'epoch': 0.72}
{'loss': 0.041, 'learning_rate': 3.204148169979186e-05, 'epoch': 0.36}
{'loss': 0.0285, 'learning_rate': 1.2159622683223406e-05, 'epoch': 0.76}
{'loss': 0.0701, 'learning_rate': 3.395715522481535e-05, 'epoch': 0.32}
{'loss': 0.0717, 'learning_rate': 5.379711853789749e-06, 'epoch': 0.89}
{'loss': 0.0546, 'learning_rate': 3.256644992539926e-05, 'epoch': 0.35}
{'loss': 0.0496, 'learning_rate': 1.2454401414937912e-05, 'epoch': 0.75}
{'loss': 0.0459, 'learning_rate': 3.3726906003057715e-05, 'epoch': 0.33}
{'loss': 0.0564, 'learning_rate': 3.0715246182467905e-05, 'epoch': 0.39}
{'loss': 0.0367, 'learning_rate': 1.3780905707653194e-05, 'epoch': 0.72}
{'loss': 0.0504, 'learning_rate': 3.196780194882942e-05, 'epoch': 0.36}
{'loss': 0.0558, 'learning_rate': 1.2012233317366152e-05, 'epoch': 0.76}
{'loss': 0.0628, 'learning_rate': 3.3883475473852904e-05, 'epoch': 0.32}
{'loss': 0.0512, 'learning_rate': 5.232322487932496e-06, 'epoch': 0.9}
{'loss': 0.0599, 'learning_rate': 3.249277017443681e-05, 'epoch': 0.35}
{'loss': 0.0422, 'learning_rate': 1.2307012049080659e-05, 'epoch': 0.75}
{'loss': 0.0793, 'learning_rate': 3.064156643150547e-05, 'epoch': 0.39}
{'loss': 0.0699, 'learning_rate': 3.365322625209527e-05, 'epoch': 0.33}
{'loss': 0.0681, 'learning_rate': 1.363351634179594e-05, 'epoch': 0.73}
{'loss': 0.0682, 'learning_rate': 3.1894122197866975e-05, 'epoch': 0.36}
{'loss': 0.0445, 'learning_rate': 3.380979572289046e-05, 'epoch': 0.32}
{'loss': 0.0339, 'learning_rate': 1.18648439515089e-05, 'epoch': 0.76}
{'loss': 0.0356, 'learning_rate': 5.084933122075243e-06, 'epoch': 0.9}
{'loss': 0.0705, 'learning_rate': 3.2419090423474375e-05, 'epoch': 0.35}
{'loss': 0.0499, 'learning_rate': 1.2159622683223406e-05, 'epoch': 0.76}
{'loss': 0.065, 'learning_rate': 3.056788668054302e-05, 'epoch': 0.39}
{'loss': 0.0434, 'learning_rate': 3.357954650113283e-05, 'epoch': 0.33}
{'loss': 0.0484, 'learning_rate': 1.3486126975938687e-05, 'epoch': 0.73}
{'loss': 0.0628, 'learning_rate': 3.182044244690453e-05, 'epoch': 0.36}
{'loss': 0.0603, 'learning_rate': 3.3736115971928014e-05, 'epoch': 0.33}
{'loss': 0.0371, 'learning_rate': 1.1717454585651644e-05, 'epoch': 0.77}
{'loss': 0.049, 'learning_rate': 4.937543756217989e-06, 'epoch': 0.9}
{'loss': 0.0761, 'learning_rate': 1.2012233317366152e-05, 'epoch': 0.76}
{'loss': 0.0628, 'learning_rate': 3.234541067251193e-05, 'epoch': 0.35}
{'loss': 0.0526, 'learning_rate': 3.350586675017039e-05, 'epoch': 0.33}
{'loss': 0.0576, 'learning_rate': 3.0494206929580584e-05, 'epoch': 0.39}
{'loss': 0.1029, 'learning_rate': 1.3338737610081434e-05, 'epoch': 0.73}
{'loss': 0.0312, 'learning_rate': 3.174676269594209e-05, 'epoch': 0.37}
{'loss': 0.0324, 'learning_rate': 1.1570065219794391e-05, 'epoch': 0.77}
{'loss': 0.0436, 'learning_rate': 3.3662436220965576e-05, 'epoch': 0.33}
{'loss': 0.0472, 'learning_rate': 4.7901543903607355e-06, 'epoch': 0.9}
{'loss': 0.0354, 'learning_rate': 1.18648439515089e-05, 'epoch': 0.76}
{'loss': 0.074, 'learning_rate': 3.2271730921549485e-05, 'epoch': 0.36}
{'loss': 0.0684, 'learning_rate': 3.343218699920795e-05, 'epoch': 0.33}
{'loss': 0.0426, 'learning_rate': 3.0420527178618142e-05, 'epoch': 0.39}
{'loss': 0.0888, 'learning_rate': 1.3191348244224181e-05, 'epoch': 0.74}
{'loss': 0.0832, 'learning_rate': 3.167308294497965e-05, 'epoch': 0.37}
{'loss': 0.0547, 'learning_rate': 3.358875647000314e-05, 'epoch': 0.33}
{'loss': 0.0397, 'learning_rate': 1.1422675853937138e-05, 'epoch': 0.77}
{'loss': 0.0349, 'learning_rate': 4.642765024503482e-06, 'epoch': 0.91}
{'loss': 0.06, 'learning_rate': 1.1717454585651644e-05, 'epoch': 0.77}
{'loss': 0.0916, 'learning_rate': 3.219805117058705e-05, 'epoch': 0.36}
{'loss': 0.0688, 'learning_rate': 3.03468474276557e-05, 'epoch': 0.39}
{'loss': 0.0589, 'learning_rate': 3.3358507248245504e-05, 'epoch': 0.33}
{'loss': 0.0606, 'learning_rate': 1.3043958878366926e-05, 'epoch': 0.74}
{'loss': 0.0593, 'learning_rate': 3.159940319401721e-05, 'epoch': 0.37}
{'loss': 0.0873, 'learning_rate': 3.3515076719040693e-05, 'epoch': 0.33}
{'loss': 0.0392, 'learning_rate': 1.1275286488079885e-05, 'epoch': 0.78}
{'loss': 0.0551, 'learning_rate': 4.495375658646229e-06, 'epoch': 0.91}
{'loss': 0.066, 'learning_rate': 1.1570065219794391e-05, 'epoch': 0.77}
{'loss': 0.0277, 'learning_rate': 3.0273167676693253e-05, 'epoch': 0.39}
{'loss': 0.0291, 'learning_rate': 3.21243714196246e-05, 'epoch': 0.36}
{'loss': 0.074, 'learning_rate': 3.328482749728306e-05, 'epoch': 0.33}
{'loss': 0.0421, 'learning_rate': 1.2896569512509673e-05, 'epoch': 0.74}
{'loss': 0.0706, 'learning_rate': 3.1525723443054764e-05, 'epoch': 0.37}
{'loss': 0.0808, 'learning_rate': 3.3441396968078255e-05, 'epoch': 0.33}
{'loss': 0.0425, 'learning_rate': 1.1127897122222632e-05, 'epoch': 0.78}
{'loss': 0.0538, 'learning_rate': 3.019948792573081e-05, 'epoch': 0.4}
{'loss': 0.0515, 'learning_rate': 4.347986292788975e-06, 'epoch': 0.91}
{'loss': 0.1004, 'learning_rate': 1.1422675853937138e-05, 'epoch': 0.77}
{'loss': 0.0601, 'learning_rate': 3.2050691668662164e-05, 'epoch': 0.36}
{'loss': 0.0576, 'learning_rate': 3.321114774632062e-05, 'epoch': 0.34}
{'loss': 0.0537, 'learning_rate': 1.274918014665242e-05, 'epoch': 0.75}
{'loss': 0.0444, 'learning_rate': 3.1452043692092326e-05, 'epoch': 0.37}
{'loss': 0.0703, 'learning_rate': 3.336771721711581e-05, 'epoch': 0.33}
{'loss': 0.0156, 'learning_rate': 1.0980507756365379e-05, 'epoch': 0.78}
{'loss': 0.0366, 'learning_rate': 3.0125808174768373e-05, 'epoch': 0.4}
{'loss': 0.0463, 'learning_rate': 4.200596926931722e-06, 'epoch': 0.92}
{'loss': 0.0315, 'learning_rate': 1.1275286488079885e-05, 'epoch': 0.78}
{'loss': 0.0603, 'learning_rate': 3.1977011917699726e-05, 'epoch': 0.36}
{'loss': 0.0635, 'learning_rate': 3.3137467995358176e-05, 'epoch': 0.34}
{'loss': 0.0468, 'learning_rate': 1.2601790780795167e-05, 'epoch': 0.75}
{'loss': 0.06, 'learning_rate': 3.3294037466153366e-05, 'epoch': 0.33}
{'loss': 0.0969, 'learning_rate': 3.137836394112989e-05, 'epoch': 0.37}
{'loss': 0.0753, 'learning_rate': 3.005212842380593e-05, 'epoch': 0.4}
{'loss': 0.0619, 'learning_rate': 1.0833118390508125e-05, 'epoch': 0.78}
{'loss': 0.0272, 'learning_rate': 4.053207561074469e-06, 'epoch': 0.92}
{'loss': 0.0245, 'learning_rate': 1.1127897122222632e-05, 'epoch': 0.78}
{'loss': 0.0684, 'learning_rate': 3.190333216673728e-05, 'epoch': 0.36}
{'loss': 0.0621, 'learning_rate': 3.306378824439574e-05, 'epoch': 0.34}
{'loss': 0.047, 'learning_rate': 1.2454401414937912e-05, 'epoch': 0.75}
{'loss': 0.0535, 'learning_rate': 3.322035771519093e-05, 'epoch': 0.34}
{'loss': 0.0684, 'learning_rate': 2.997844867284349e-05, 'epoch': 0.4}
{'loss': 0.0847, 'learning_rate': 3.1304684190167436e-05, 'epoch': 0.37}
{'loss': 0.0784, 'learning_rate': 1.0685729024650872e-05, 'epoch': 0.79}
{'loss': 0.0349, 'learning_rate': 3.905818195217216e-06, 'epoch': 0.92}
{'loss': 0.0502, 'learning_rate': 1.0980507756365379e-05, 'epoch': 0.78}
{'loss': 0.0611, 'learning_rate': 3.182965241577484e-05, 'epoch': 0.36}
{'loss': 0.0584, 'learning_rate': 3.29901084934333e-05, 'epoch': 0.34}
{'loss': 0.0567, 'learning_rate': 3.314667796422848e-05, 'epoch': 0.34}
{'loss': 0.0366, 'learning_rate': 1.2307012049080659e-05, 'epoch': 0.75}
{'loss': 0.0462, 'learning_rate': 2.990476892188105e-05, 'epoch': 0.4}
{'loss': 0.0455, 'learning_rate': 3.1231004439205e-05, 'epoch': 0.38}
{'loss': 0.0729, 'learning_rate': 1.0538339658793617e-05, 'epoch': 0.79}
{'loss': 0.062, 'learning_rate': 3.758428829359962e-06, 'epoch': 0.93}
{'loss': 0.034, 'learning_rate': 1.0833118390508125e-05, 'epoch': 0.78}
{'loss': 0.0728, 'learning_rate': 3.3072998213266045e-05, 'epoch': 0.34}
{'loss': 0.0745, 'learning_rate': 3.2916428742470855e-05, 'epoch': 0.34}
{'loss': 0.0826, 'learning_rate': 3.175597266481239e-05, 'epoch': 0.37}
{'loss': 0.0702, 'learning_rate': 1.2159622683223406e-05, 'epoch': 0.76}
{'loss': 0.0762, 'learning_rate': 2.9831089170918607e-05, 'epoch': 0.4}
{'loss': 0.048, 'learning_rate': 3.1157324688242553e-05, 'epoch': 0.38}
{'loss': 0.0208, 'learning_rate': 1.0390950292936364e-05, 'epoch': 0.79}
{'loss': 0.0419, 'learning_rate': 3.6110394635027085e-06, 'epoch': 0.93}
{'loss': 0.073, 'learning_rate': 3.29993184623036e-05, 'epoch': 0.34}
{'loss': 0.0568, 'learning_rate': 1.0685729024650872e-05, 'epoch': 0.79}
{'loss': 0.1004, 'learning_rate': 3.284274899150842e-05, 'epoch': 0.34}
{'loss': 0.0465, 'learning_rate': 3.1682292913849953e-05, 'epoch': 0.37}
{'loss': 0.0448, 'learning_rate': 1.2012233317366152e-05, 'epoch': 0.76}
{'loss': 0.063, 'learning_rate': 2.9757409419956162e-05, 'epoch': 0.41}
{'loss': 0.084, 'learning_rate': 3.1083644937280115e-05, 'epoch': 0.38}
{'loss': 0.0572, 'learning_rate': 1.0243560927079111e-05, 'epoch': 0.8}
{'loss': 0.0565, 'learning_rate': 3.292563871134116e-05, 'epoch': 0.34}
{'loss': 0.0565, 'learning_rate': 3.4636500976454553e-06, 'epoch': 0.93}
{'loss': 0.0518, 'learning_rate': 1.0538339658793617e-05, 'epoch': 0.79}
{'loss': 0.0845, 'learning_rate': 3.2769069240545966e-05, 'epoch': 0.34}
{'loss': 0.0576, 'learning_rate': 3.1608613162887515e-05, 'epoch': 0.37}
{'loss': 0.0677, 'learning_rate': 2.968372966899372e-05, 'epoch': 0.41}
{'loss': 0.05, 'learning_rate': 1.18648439515089e-05, 'epoch': 0.76}
{'loss': 0.0668, 'learning_rate': 3.100996518631768e-05, 'epoch': 0.38}
{'loss': 0.0457, 'learning_rate': 1.0096171561221858e-05, 'epoch': 0.8}
{'loss': 0.0627, 'learning_rate': 3.285195896037872e-05, 'epoch': 0.34}
{'loss': 0.0186, 'learning_rate': 3.3162607317882017e-06, 'epoch': 0.93}
{'loss': 0.07, 'learning_rate': 1.0390950292936364e-05, 'epoch': 0.79}
{'loss': 0.1011, 'learning_rate': 3.269538948958353e-05, 'epoch': 0.35}
{'loss': 0.0615, 'learning_rate': 2.961004991803128e-05, 'epoch': 0.41}
{'loss': 0.0407, 'learning_rate': 3.153493341192507e-05, 'epoch': 0.37}
{'loss': 0.0508, 'learning_rate': 1.1717454585651644e-05, 'epoch': 0.77}
{'loss': 0.0643, 'learning_rate': 3.093628543535523e-05, 'epoch': 0.38}
{'loss': 0.0207, 'learning_rate': 3.277827920941627e-05, 'epoch': 0.34}
{'loss': 0.0227, 'learning_rate': 9.948782195364605e-06, 'epoch': 0.8}
{'loss': 0.0371, 'learning_rate': 3.1688713659309486e-06, 'epoch': 0.94}
{'loss': 0.0746, 'learning_rate': 1.0243560927079111e-05, 'epoch': 0.8}
{'loss': 0.0265, 'learning_rate': 2.9536370167068838e-05, 'epoch': 0.41}
{'loss': 0.0526, 'learning_rate': 3.262170973862109e-05, 'epoch': 0.35}
{'loss': 0.0954, 'learning_rate': 3.146125366096263e-05, 'epoch': 0.37}
{'loss': 0.0417, 'learning_rate': 1.1570065219794391e-05, 'epoch': 0.77}
{'loss': 0.062, 'learning_rate': 3.0862605684392794e-05, 'epoch': 0.38}
{'loss': 0.1057, 'learning_rate': 3.2704599458453834e-05, 'epoch': 0.35}
{'loss': 0.0471, 'learning_rate': 9.801392829507352e-06, 'epoch': 0.8}
{'loss': 0.0498, 'learning_rate': 3.0214820000736945e-06, 'epoch': 0.94}
{'loss': 0.0709, 'learning_rate': 1.0096171561221858e-05, 'epoch': 0.8}
{'loss': 0.0433, 'learning_rate': 3.2548029987658645e-05, 'epoch': 0.35}
{'loss': 0.0413, 'learning_rate': 2.9462690416106397e-05, 'epoch': 0.41}
{'loss': 0.0687, 'learning_rate': 3.138757391000019e-05, 'epoch': 0.37}
{'loss': 0.0697, 'learning_rate': 1.1422675853937138e-05, 'epoch': 0.77}
{'loss': 0.0248, 'learning_rate': 3.078892593343034e-05, 'epoch': 0.38}
{'loss': 0.0375, 'learning_rate': 9.654003463650098e-06, 'epoch': 0.81}
{'loss': 0.0724, 'learning_rate': 3.263091970749139e-05, 'epoch': 0.35}
{'loss': 0.0676, 'learning_rate': 2.8740926342164414e-06, 'epoch': 0.94}
{'loss': 0.0588, 'learning_rate': 9.948782195364605e-06, 'epoch': 0.8}
{'loss': 0.0526, 'learning_rate': 3.247435023669621e-05, 'epoch': 0.35}
{'loss': 0.0549, 'learning_rate': 2.9389010665143955e-05, 'epoch': 0.41}
{'loss': 0.0454, 'learning_rate': 1.1275286488079885e-05, 'epoch': 0.78}
{'loss': 0.0427, 'learning_rate': 3.131389415903775e-05, 'epoch': 0.37}
{'loss': 0.0776, 'learning_rate': 3.0715246182467905e-05, 'epoch': 0.39}
{'loss': 0.0839, 'learning_rate': 3.255723995652895e-05, 'epoch': 0.35}
{'loss': 0.0723, 'learning_rate': 9.506614097792845e-06, 'epoch': 0.81}
{'loss': 0.0454, 'learning_rate': 2.726703268359188e-06, 'epoch': 0.95}
{'loss': 0.0559, 'learning_rate': 2.9315330914181517e-05, 'epoch': 0.41}
{'loss': 0.0393, 'learning_rate': 9.801392829507352e-06, 'epoch': 0.8}
{'loss': 0.0582, 'learning_rate': 3.240067048573376e-05, 'epoch': 0.35}
{'loss': 0.067, 'learning_rate': 3.1240214408075305e-05, 'epoch': 0.38}
{'loss': 0.0779, 'learning_rate': 1.1127897122222632e-05, 'epoch': 0.78}
{'loss': 0.051, 'learning_rate': 3.064156643150547e-05, 'epoch': 0.39}
{'loss': 0.0818, 'learning_rate': 3.248356020556651e-05, 'epoch': 0.35}
{'loss': 0.0436, 'learning_rate': 9.35922473193559e-06, 'epoch': 0.81}
{'loss': 0.0835, 'learning_rate': 2.924165116321907e-05, 'epoch': 0.42}
{'loss': 0.0389, 'learning_rate': 2.5793139025019346e-06, 'epoch': 0.95}
{'loss': 0.0625, 'learning_rate': 9.654003463650098e-06, 'epoch': 0.81}
{'loss': 0.0497, 'learning_rate': 3.232699073477132e-05, 'epoch': 0.35}
{'loss': 0.0554, 'learning_rate': 3.116653465711286e-05, 'epoch': 0.38}
{'loss': 0.0288, 'learning_rate': 1.0980507756365379e-05, 'epoch': 0.78}
{'loss': 0.0586, 'learning_rate': 3.056788668054302e-05, 'epoch': 0.39}
{'loss': 0.088, 'learning_rate': 3.240988045460407e-05, 'epoch': 0.35}
{'loss': 0.0424, 'learning_rate': 2.9167971412256627e-05, 'epoch': 0.42}
{'loss': 0.0456, 'learning_rate': 9.211835366078337e-06, 'epoch': 0.82}
{'loss': 0.0543, 'learning_rate': 2.431924536644681e-06, 'epoch': 0.95}
{'loss': 0.0651, 'learning_rate': 9.506614097792845e-06, 'epoch': 0.81}
{'loss': 0.0473, 'learning_rate': 3.225331098380888e-05, 'epoch': 0.36}
{'loss': 0.1176, 'learning_rate': 3.109285490615042e-05, 'epoch': 0.38}
{'loss': 0.0531, 'learning_rate': 1.0833118390508125e-05, 'epoch': 0.78}
{'loss': 0.0347, 'learning_rate': 3.0494206929580584e-05, 'epoch': 0.39}
{'loss': 0.0597, 'learning_rate': 3.233620070364163e-05, 'epoch': 0.35}
{'loss': 0.1359, 'learning_rate': 2.9094291661294186e-05, 'epoch': 0.42}
{'loss': 0.0569, 'learning_rate': 9.064446000221084e-06, 'epoch': 0.82}
{'loss': 0.0223, 'learning_rate': 2.284535170787428e-06, 'epoch': 0.95}
{'loss': 0.0471, 'learning_rate': 9.35922473193559e-06, 'epoch': 0.81}
{'loss': 0.0483, 'learning_rate': 3.2179631232846434e-05, 'epoch': 0.36}
{'loss': 0.0479, 'learning_rate': 3.101917515518798e-05, 'epoch': 0.38}
{'loss': 0.0476, 'learning_rate': 1.0685729024650872e-05, 'epoch': 0.79}
{'loss': 0.0687, 'learning_rate': 3.226252095267918e-05, 'epoch': 0.36}
{'loss': 0.0908, 'learning_rate': 3.0420527178618142e-05, 'epoch': 0.39}
{'loss': 0.0576, 'learning_rate': 2.9020611910331748e-05, 'epoch': 0.42}
{'loss': 0.0532, 'learning_rate': 8.917056634363831e-06, 'epoch': 0.82}
{'loss': 0.0446, 'learning_rate': 2.1371458049301743e-06, 'epoch': 0.96}
{'loss': 0.0687, 'learning_rate': 3.2105951481883996e-05, 'epoch': 0.36}
{'loss': 0.0305, 'learning_rate': 9.211835366078337e-06, 'epoch': 0.82}
{'loss': 0.0496, 'learning_rate': 3.094549540422554e-05, 'epoch': 0.38}
{'loss': 0.016, 'learning_rate': 1.0538339658793617e-05, 'epoch': 0.79}
{'loss': 0.0514, 'learning_rate': 3.218884120171674e-05, 'epoch': 0.36}
{'loss': 0.0454, 'learning_rate': 2.8946932159369306e-05, 'epoch': 0.42}
{'loss': 0.0484, 'learning_rate': 3.03468474276557e-05, 'epoch': 0.39}
{'loss': 0.0307, 'learning_rate': 8.769667268506578e-06, 'epoch': 0.83}
{'loss': 0.0691, 'learning_rate': 9.064446000221084e-06, 'epoch': 0.82}
{'loss': 0.054, 'learning_rate': 3.203227173092155e-05, 'epoch': 0.36}
{'loss': 0.0382, 'learning_rate': 1.989756439072921e-06, 'epoch': 0.96}
{'loss': 0.0545, 'learning_rate': 3.08718156532631e-05, 'epoch': 0.38}
{'loss': 0.0495, 'learning_rate': 1.0390950292936364e-05, 'epoch': 0.79}
{'loss': 0.0665, 'learning_rate': 3.21151614507543e-05, 'epoch': 0.36}
{'loss': 0.0761, 'learning_rate': 2.8873252408406865e-05, 'epoch': 0.42}
{'loss': 0.0535, 'learning_rate': 3.0273167676693253e-05, 'epoch': 0.39}
{'loss': 0.0417, 'learning_rate': 8.622277902649325e-06, 'epoch': 0.83}
{'loss': 0.0458, 'learning_rate': 8.917056634363831e-06, 'epoch': 0.82}
{'loss': 0.0729, 'learning_rate': 3.195859197995911e-05, 'epoch': 0.36}
{'loss': 0.0422, 'learning_rate': 1.8423670732156675e-06, 'epoch': 0.96}
{'loss': 0.0598, 'learning_rate': 3.204148169979186e-05, 'epoch': 0.36}
{'loss': 0.0692, 'learning_rate': 3.079813590230065e-05, 'epoch': 0.38}
{'loss': 0.0698, 'learning_rate': 1.0243560927079111e-05, 'epoch': 0.8}
{'loss': 0.0436, 'learning_rate': 2.8799572657444424e-05, 'epoch': 0.42}
{'loss': 0.0499, 'learning_rate': 3.019948792573081e-05, 'epoch': 0.4}
{'loss': 0.0697, 'learning_rate': 8.474888536792071e-06, 'epoch': 0.83}
{'loss': 0.0696, 'learning_rate': 8.769667268506578e-06, 'epoch': 0.83}
{'loss': 0.0325, 'learning_rate': 3.1884912228996675e-05, 'epoch': 0.36}
{'loss': 0.0552, 'learning_rate': 1.6949777073584142e-06, 'epoch': 0.97}
{'loss': 0.0797, 'learning_rate': 3.196780194882942e-05, 'epoch': 0.36}
{'loss': 0.066, 'learning_rate': 2.872589290648198e-05, 'epoch': 0.43}
{'loss': 0.0638, 'learning_rate': 1.0096171561221858e-05, 'epoch': 0.8}
{'loss': 0.051, 'learning_rate': 3.072445615133821e-05, 'epoch': 0.39}
{'loss': 0.0668, 'learning_rate': 3.0125808174768373e-05, 'epoch': 0.4}
{'loss': 0.0352, 'learning_rate': 8.327499170934818e-06, 'epoch': 0.83}
{'loss': 0.0717, 'learning_rate': 3.1811232478034223e-05, 'epoch': 0.36}
{'loss': 0.0524, 'learning_rate': 8.622277902649325e-06, 'epoch': 0.83}
{'loss': 0.0523, 'learning_rate': 3.1894122197866975e-05, 'epoch': 0.36}
{'loss': 0.08, 'learning_rate': 2.8652213155519537e-05, 'epoch': 0.43}
{'loss': 0.0463, 'learning_rate': 1.5475883415011608e-06, 'epoch': 0.97}
{'loss': 0.0544, 'learning_rate': 9.948782195364605e-06, 'epoch': 0.8}
{'loss': 0.0994, 'learning_rate': 3.0650776400375766e-05, 'epoch': 0.39}
{'loss': 0.0707, 'learning_rate': 3.005212842380593e-05, 'epoch': 0.4}
{'loss': 0.0321, 'learning_rate': 8.180109805077563e-06, 'epoch': 0.84}
{'loss': 0.0608, 'learning_rate': 3.182044244690453e-05, 'epoch': 0.36}
{'loss': 0.0721, 'learning_rate': 2.8578533404557096e-05, 'epoch': 0.43}
{'loss': 0.0351, 'learning_rate': 3.1737552727071785e-05, 'epoch': 0.37}
{'loss': 0.04, 'learning_rate': 8.474888536792071e-06, 'epoch': 0.83}
{'loss': 0.0481, 'learning_rate': 1.4001989756439072e-06, 'epoch': 0.97}
{'loss': 0.0421, 'learning_rate': 9.801392829507352e-06, 'epoch': 0.8}
{'loss': 0.0947, 'learning_rate': 3.057709664941333e-05, 'epoch': 0.39}
{'loss': 0.0575, 'learning_rate': 2.997844867284349e-05, 'epoch': 0.4}
{'loss': 0.0497, 'learning_rate': 3.174676269594209e-05, 'epoch': 0.37}
{'loss': 0.044, 'learning_rate': 2.8504853653594654e-05, 'epoch': 0.43}
{'loss': 0.0284, 'learning_rate': 8.03272043922031e-06, 'epoch': 0.84}
{'loss': 0.0531, 'learning_rate': 3.166387297610934e-05, 'epoch': 0.37}
{'loss': 0.037, 'learning_rate': 8.327499170934818e-06, 'epoch': 0.83}
{'loss': 0.0494, 'learning_rate': 1.2528096097866538e-06, 'epoch': 0.98}
{'loss': 0.0387, 'learning_rate': 9.654003463650098e-06, 'epoch': 0.81}
{'loss': 0.0485, 'learning_rate': 3.0503416898450887e-05, 'epoch': 0.39}
{'loss': 0.0474, 'learning_rate': 2.990476892188105e-05, 'epoch': 0.4}
{'loss': 0.064, 'learning_rate': 2.8431173902632213e-05, 'epoch': 0.43}
{'loss': 0.0707, 'learning_rate': 3.167308294497965e-05, 'epoch': 0.37}
{'loss': 0.0931, 'learning_rate': 7.885331073363057e-06, 'epoch': 0.84}
{'loss': 0.042, 'learning_rate': 3.15901932251469e-05, 'epoch': 0.37}
{'loss': 0.064, 'learning_rate': 8.180109805077563e-06, 'epoch': 0.84}
{'loss': 0.0531, 'learning_rate': 1.1054202439294004e-06, 'epoch': 0.98}
{'loss': 0.0295, 'learning_rate': 9.506614097792845e-06, 'epoch': 0.81}
{'loss': 0.0377, 'learning_rate': 3.0429737147488445e-05, 'epoch': 0.39}
{'loss': 0.1001, 'learning_rate': 2.9831089170918607e-05, 'epoch': 0.4}
{'loss': 0.0442, 'learning_rate': 2.835749415166977e-05, 'epoch': 0.43}
{'loss': 0.0725, 'learning_rate': 3.159940319401721e-05, 'epoch': 0.37}
{'loss': 0.0362, 'learning_rate': 7.737941707505804e-06, 'epoch': 0.85}
{'loss': 0.0472, 'learning_rate': 3.1516513474184464e-05, 'epoch': 0.37}
{'loss': 0.058, 'learning_rate': 8.03272043922031e-06, 'epoch': 0.84}
{'loss': 0.0402, 'learning_rate': 9.58030878072147e-07, 'epoch': 0.98}
{'loss': 0.0381, 'learning_rate': 9.35922473193559e-06, 'epoch': 0.81}
{'loss': 0.0712, 'learning_rate': 3.0356057396526004e-05, 'epoch': 0.39}
{'loss': 0.0354, 'learning_rate': 2.9757409419956162e-05, 'epoch': 0.41}
{'loss': 0.0563, 'learning_rate': 2.828381440070733e-05, 'epoch': 0.43}
{'loss': 0.0828, 'learning_rate': 3.1525723443054764e-05, 'epoch': 0.37}
{'loss': 0.0277, 'learning_rate': 7.590552341648551e-06, 'epoch': 0.85}
{'loss': 0.0363, 'learning_rate': 3.144283372322202e-05, 'epoch': 0.37}
{'loss': 0.0428, 'learning_rate': 7.885331073363057e-06, 'epoch': 0.84}
{'loss': 0.0355, 'learning_rate': 8.106415122148938e-07, 'epoch': 0.98}
{'loss': 0.0359, 'learning_rate': 9.211835366078337e-06, 'epoch': 0.82}
{'loss': 0.0476, 'learning_rate': 3.028237764556356e-05, 'epoch': 0.39}
{'loss': 0.0567, 'learning_rate': 2.968372966899372e-05, 'epoch': 0.41}
{'loss': 0.092, 'learning_rate': 2.8210134649744885e-05, 'epoch': 0.44}
{'loss': 0.0427, 'learning_rate': 3.1452043692092326e-05, 'epoch': 0.37}
{'loss': 0.0446, 'learning_rate': 7.4431629757912975e-06, 'epoch': 0.85}
{'loss': 0.0996, 'learning_rate': 3.136915397225958e-05, 'epoch': 0.37}
{'loss': 0.0459, 'learning_rate': 7.737941707505804e-06, 'epoch': 0.85}
{'loss': 0.0283, 'learning_rate': 6.632521463576403e-07, 'epoch': 0.99}
{'loss': 0.0403, 'learning_rate': 9.064446000221084e-06, 'epoch': 0.82}
{'loss': 0.0386, 'learning_rate': 3.0208697894601118e-05, 'epoch': 0.4}
{'loss': 0.1085, 'learning_rate': 2.961004991803128e-05, 'epoch': 0.41}
{'loss': 0.0788, 'learning_rate': 2.8136454898782444e-05, 'epoch': 0.44}
{'loss': 0.0565, 'learning_rate': 3.137836394112989e-05, 'epoch': 0.37}
{'loss': 0.0773, 'learning_rate': 7.2957736099340435e-06, 'epoch': 0.85}
{'loss': 0.0459, 'learning_rate': 3.129547422129713e-05, 'epoch': 0.37}
{'loss': 0.0185, 'learning_rate': 7.590552341648551e-06, 'epoch': 0.85}
{'loss': 0.0637, 'learning_rate': 5.158627805003869e-07, 'epoch': 0.99}
{'loss': 0.0531, 'learning_rate': 8.917056634363831e-06, 'epoch': 0.82}
{'loss': 0.1082, 'learning_rate': 3.0135018143638676e-05, 'epoch': 0.4}
{'loss': 0.0538, 'learning_rate': 2.8062775147820002e-05, 'epoch': 0.44}
{'loss': 0.0326, 'learning_rate': 3.1304684190167436e-05, 'epoch': 0.37}
{'loss': 0.0517, 'learning_rate': 2.9536370167068838e-05, 'epoch': 0.41}
{'loss': 0.0674, 'learning_rate': 3.122179447033469e-05, 'epoch': 0.38}
{'loss': 0.0695, 'learning_rate': 7.14838424407679e-06, 'epoch': 0.86}
{'loss': 0.0572, 'learning_rate': 7.4431629757912975e-06, 'epoch': 0.85}
{'loss': 0.0252, 'learning_rate': 3.684734146431335e-07, 'epoch': 0.99}
{'loss': 0.0463, 'learning_rate': 8.769667268506578e-06, 'epoch': 0.83}
{'loss': 0.0821, 'learning_rate': 3.0061338392676235e-05, 'epoch': 0.4}
{'loss': 0.0698, 'learning_rate': 2.798909539685756e-05, 'epoch': 0.44}
{'loss': 0.0703, 'learning_rate': 3.1231004439205e-05, 'epoch': 0.38}
{'loss': 0.0517, 'learning_rate': 2.9462690416106397e-05, 'epoch': 0.41}
{'loss': 0.0776, 'learning_rate': 3.1148114719372254e-05, 'epoch': 0.38}
{'loss': 0.0463, 'learning_rate': 7.000994878219537e-06, 'epoch': 0.86}
{'loss': 0.0159, 'learning_rate': 7.2957736099340435e-06, 'epoch': 0.85}
{'loss': 0.032, 'learning_rate': 2.2108404878588012e-07, 'epoch': 1.0}
{'loss': 0.06, 'learning_rate': 8.622277902649325e-06, 'epoch': 0.83}
{'loss': 0.0462, 'learning_rate': 2.9987658641713797e-05, 'epoch': 0.4}
{'loss': 0.0685, 'learning_rate': 2.7915415645895123e-05, 'epoch': 0.44}
{'loss': 0.0797, 'learning_rate': 3.1157324688242553e-05, 'epoch': 0.38}
{'loss': 0.0988, 'learning_rate': 2.9389010665143955e-05, 'epoch': 0.41}
{'loss': 0.0321, 'learning_rate': 3.107443496840981e-05, 'epoch': 0.38}
{'loss': 0.0515, 'learning_rate': 6.853605512362283e-06, 'epoch': 0.86}
{'loss': 0.0862, 'learning_rate': 7.14838424407679e-06, 'epoch': 0.86}
{'loss': 0.0389, 'learning_rate': 7.36946829286267e-08, 'epoch': 1.0}
{'loss': 0.0729, 'learning_rate': 8.474888536792071e-06, 'epoch': 0.83}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0808, 'learning_rate': 2.784173589493268e-05, 'epoch': 0.44}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0349, 'learning_rate': 3.1083644937280115e-05, 'epoch': 0.38}
{'loss': 0.0196, 'learning_rate': 2.9913978890751355e-05, 'epoch': 0.4}
{'loss': 0.0456, 'learning_rate': 2.9315330914181517e-05, 'epoch': 0.41}
{'loss': 0.0564, 'learning_rate': 3.100075521744737e-05, 'epoch': 0.38}
{'loss': 0.0633, 'learning_rate': 6.70621614650503e-06, 'epoch': 0.87}
{'loss': 0.0659, 'learning_rate': 7.000994878219537e-06, 'epoch': 0.86}
{'loss': 0.0786, 'learning_rate': 8.327499170934818e-06, 'epoch': 0.83}
{'loss': 0.0725, 'learning_rate': 2.776805614397024e-05, 'epoch': 0.44}
{'loss': 0.0911, 'learning_rate': 3.100996518631768e-05, 'epoch': 0.38}
{'loss': 0.1025, 'learning_rate': 2.9840299139788914e-05, 'epoch': 0.4}
{'loss': 0.0767, 'learning_rate': 2.924165116321907e-05, 'epoch': 0.42}
{'loss': 0.0561, 'learning_rate': 3.0927075466484926e-05, 'epoch': 0.38}
{'loss': 0.0523, 'learning_rate': 6.558826780647777e-06, 'epoch': 0.87}
{'loss': 0.047, 'learning_rate': 6.853605512362283e-06, 'epoch': 0.86}
{'loss': 0.0591, 'learning_rate': 8.180109805077563e-06, 'epoch': 0.84}
{'loss': 0.0271, 'learning_rate': 2.769437639300779e-05, 'epoch': 0.45}
{'loss': 0.0339, 'learning_rate': 3.093628543535523e-05, 'epoch': 0.38}
{'loss': 0.0522, 'learning_rate': 2.9766619388826465e-05, 'epoch': 0.41}
{'loss': 0.0288, 'learning_rate': 2.9167971412256627e-05, 'epoch': 0.42}
{'loss': 0.067, 'learning_rate': 3.085339571552249e-05, 'epoch': 0.38}
{'loss': 0.0624, 'learning_rate': 6.411437414790524e-06, 'epoch': 0.87}
{'loss': 0.0968, 'learning_rate': 6.70621614650503e-06, 'epoch': 0.87}
{'loss': 0.0931, 'learning_rate': 2.762069664204535e-05, 'epoch': 0.45}
{'loss': 0.0374, 'learning_rate': 8.03272043922031e-06, 'epoch': 0.84}
{'loss': 0.0533, 'learning_rate': 3.0862605684392794e-05, 'epoch': 0.38}
{'loss': 0.0798, 'learning_rate': 2.9692939637864024e-05, 'epoch': 0.41}
{'loss': 0.0809, 'learning_rate': 2.9094291661294186e-05, 'epoch': 0.42}
{'loss': 0.0675, 'learning_rate': 3.077971596456004e-05, 'epoch': 0.38}
{'loss': 0.0751, 'learning_rate': 6.26404804893327e-06, 'epoch': 0.88}
{'loss': 0.0571, 'learning_rate': 2.7547016891082912e-05, 'epoch': 0.45}
{'loss': 0.0374, 'learning_rate': 6.558826780647777e-06, 'epoch': 0.87}
{'loss': 0.0658, 'learning_rate': 3.078892593343034e-05, 'epoch': 0.38}
{'loss': 0.0448, 'learning_rate': 7.885331073363057e-06, 'epoch': 0.84}
{'loss': 0.0665, 'learning_rate': 2.9619259886901586e-05, 'epoch': 0.41}
{'loss': 0.044, 'learning_rate': 2.9020611910331748e-05, 'epoch': 0.42}
{'loss': 0.0477, 'learning_rate': 3.07060362135976e-05, 'epoch': 0.39}
{'loss': 0.0466, 'learning_rate': 2.747333714012047e-05, 'epoch': 0.45}
{'loss': 0.0586, 'learning_rate': 6.116658683076016e-06, 'epoch': 0.88}
{'loss': 0.0486, 'learning_rate': 3.0715246182467905e-05, 'epoch': 0.39}
{'loss': 0.0657, 'learning_rate': 6.411437414790524e-06, 'epoch': 0.87}
{'loss': 0.0247, 'learning_rate': 7.737941707505804e-06, 'epoch': 0.85}
{'loss': 0.05, 'learning_rate': 2.9545580135939145e-05, 'epoch': 0.41}
{'loss': 0.053, 'learning_rate': 2.8946932159369306e-05, 'epoch': 0.42}
{'loss': 0.0696, 'learning_rate': 2.739965738915803e-05, 'epoch': 0.45}
{'loss': 0.0973, 'learning_rate': 3.063235646263516e-05, 'epoch': 0.39}
{'loss': 0.0581, 'learning_rate': 3.064156643150547e-05, 'epoch': 0.39}
{'loss': 0.053, 'learning_rate': 5.9692693172187625e-06, 'epoch': 0.88}
{'loss': 0.061, 'learning_rate': 6.26404804893327e-06, 'epoch': 0.88}
{'loss': 0.0563, 'learning_rate': 7.590552341648551e-06, 'epoch': 0.85}
{'loss': 0.0865, 'learning_rate': 2.9471900384976703e-05, 'epoch': 0.41}
{'loss': 0.077, 'learning_rate': 2.8873252408406865e-05, 'epoch': 0.42}
{'loss': 0.0444, 'learning_rate': 2.7325977638195588e-05, 'epoch': 0.45}
{'loss': 0.0572, 'learning_rate': 3.0558676711672715e-05, 'epoch': 0.39}
{'loss': 0.0355, 'learning_rate': 3.056788668054302e-05, 'epoch': 0.39}
{'loss': 0.0609, 'learning_rate': 5.821879951361509e-06, 'epoch': 0.88}
{'loss': 0.0317, 'learning_rate': 6.116658683076016e-06, 'epoch': 0.88}
{'loss': 0.0403, 'learning_rate': 7.4431629757912975e-06, 'epoch': 0.85}
{'loss': 0.0334, 'learning_rate': 2.939822063401426e-05, 'epoch': 0.41}
{'loss': 0.0475, 'learning_rate': 2.7252297887233146e-05, 'epoch': 0.46}
{'loss': 0.073, 'learning_rate': 2.8799572657444424e-05, 'epoch': 0.42}
{'loss': 0.0678, 'learning_rate': 3.0494206929580584e-05, 'epoch': 0.39}
{'loss': 0.0645, 'learning_rate': 3.0484996960710277e-05, 'epoch': 0.39}
{'loss': 0.0665, 'learning_rate': 5.674490585504256e-06, 'epoch': 0.89}
{'loss': 0.0316, 'learning_rate': 5.9692693172187625e-06, 'epoch': 0.88}
{'loss': 0.0409, 'learning_rate': 7.2957736099340435e-06, 'epoch': 0.85}
{'loss': 0.0687, 'learning_rate': 2.932454088305182e-05, 'epoch': 0.41}
{'loss': 0.0537, 'learning_rate': 2.71786181362707e-05, 'epoch': 0.46}
{'loss': 0.0597, 'learning_rate': 2.872589290648198e-05, 'epoch': 0.43}
{'loss': 0.0654, 'learning_rate': 3.0420527178618142e-05, 'epoch': 0.39}
{'loss': 0.0566, 'learning_rate': 3.0411317209747836e-05, 'epoch': 0.39}
{'loss': 0.0655, 'learning_rate': 5.527101219647002e-06, 'epoch': 0.89}
{'loss': 0.0517, 'learning_rate': 5.821879951361509e-06, 'epoch': 0.88}
{'loss': 0.0512, 'learning_rate': 7.14838424407679e-06, 'epoch': 0.86}
{'loss': 0.0817, 'learning_rate': 2.9250861132089375e-05, 'epoch': 0.42}
{'loss': 0.0287, 'learning_rate': 2.710493838530826e-05, 'epoch': 0.46}
{'loss': 0.1101, 'learning_rate': 3.03468474276557e-05, 'epoch': 0.39}
{'loss': 0.0814, 'learning_rate': 2.8652213155519537e-05, 'epoch': 0.43}
{'loss': 0.0457, 'learning_rate': 3.0337637458785394e-05, 'epoch': 0.39}
{'loss': 0.0361, 'learning_rate': 5.379711853789749e-06, 'epoch': 0.89}
{'loss': 0.0592, 'learning_rate': 5.674490585504256e-06, 'epoch': 0.89}
{'loss': 0.0463, 'learning_rate': 7.000994878219537e-06, 'epoch': 0.86}
{'loss': 0.052, 'learning_rate': 2.9177181381126934e-05, 'epoch': 0.42}
{'loss': 0.1051, 'learning_rate': 2.703125863434582e-05, 'epoch': 0.46}
{'loss': 0.0588, 'learning_rate': 2.8578533404557096e-05, 'epoch': 0.43}
{'loss': 0.0509, 'learning_rate': 3.026395770782295e-05, 'epoch': 0.39}
{'loss': 0.0598, 'learning_rate': 3.0273167676693253e-05, 'epoch': 0.39}
{'loss': 0.0573, 'learning_rate': 5.232322487932496e-06, 'epoch': 0.9}
{'loss': 0.0354, 'learning_rate': 5.527101219647002e-06, 'epoch': 0.89}
{'loss': 0.0611, 'learning_rate': 6.853605512362283e-06, 'epoch': 0.86}
{'loss': 0.0707, 'learning_rate': 2.9103501630164492e-05, 'epoch': 0.42}
{'loss': 0.0394, 'learning_rate': 2.6957578883383377e-05, 'epoch': 0.46}
{'loss': 0.0651, 'learning_rate': 2.8504853653594654e-05, 'epoch': 0.43}
{'loss': 0.0769, 'learning_rate': 3.019948792573081e-05, 'epoch': 0.4}
{'loss': 0.0625, 'learning_rate': 3.0190277956860508e-05, 'epoch': 0.4}
{'loss': 0.0379, 'learning_rate': 5.084933122075243e-06, 'epoch': 0.9}
{'loss': 0.0285, 'learning_rate': 5.379711853789749e-06, 'epoch': 0.89}
{'loss': 0.0443, 'learning_rate': 6.70621614650503e-06, 'epoch': 0.87}
{'loss': 0.075, 'learning_rate': 2.6883899132420936e-05, 'epoch': 0.46}
{'loss': 0.054, 'learning_rate': 2.902982187920205e-05, 'epoch': 0.42}
{'loss': 0.0701, 'learning_rate': 3.0125808174768373e-05, 'epoch': 0.4}
{'loss': 0.0371, 'learning_rate': 2.8431173902632213e-05, 'epoch': 0.43}
{'loss': 0.0742, 'learning_rate': 3.0116598205898067e-05, 'epoch': 0.4}
{'loss': 0.0638, 'learning_rate': 4.937543756217989e-06, 'epoch': 0.9}
{'loss': 0.0421, 'learning_rate': 5.232322487932496e-06, 'epoch': 0.9}
{'loss': 0.0517, 'learning_rate': 6.558826780647777e-06, 'epoch': 0.87}
{'loss': 0.082, 'learning_rate': 2.6810219381458494e-05, 'epoch': 0.46}
{'loss': 0.0846, 'learning_rate': 2.895614212823961e-05, 'epoch': 0.42}
{'loss': 0.0612, 'learning_rate': 3.005212842380593e-05, 'epoch': 0.4}
{'loss': 0.0429, 'learning_rate': 2.835749415166977e-05, 'epoch': 0.43}
{'loss': 0.0601, 'learning_rate': 3.0042918454935625e-05, 'epoch': 0.4}
{'loss': 0.0601, 'learning_rate': 4.7901543903607355e-06, 'epoch': 0.9}
{'loss': 0.0357, 'learning_rate': 5.084933122075243e-06, 'epoch': 0.9}
{'loss': 0.0508, 'learning_rate': 6.411437414790524e-06, 'epoch': 0.87}
{'loss': 0.0402, 'learning_rate': 2.6736539630496056e-05, 'epoch': 0.47}
{'loss': 0.0593, 'learning_rate': 2.997844867284349e-05, 'epoch': 0.4}
{'loss': 0.0385, 'learning_rate': 2.8882462377277168e-05, 'epoch': 0.42}
{'loss': 0.0804, 'learning_rate': 2.828381440070733e-05, 'epoch': 0.43}
{'loss': 0.0716, 'learning_rate': 2.9969238703973184e-05, 'epoch': 0.4}
{'loss': 0.0775, 'learning_rate': 4.642765024503482e-06, 'epoch': 0.91}
{'loss': 0.0858, 'learning_rate': 4.937543756217989e-06, 'epoch': 0.9}
{'loss': 0.0755, 'learning_rate': 6.26404804893327e-06, 'epoch': 0.88}
{'loss': 0.0262, 'learning_rate': 2.6662859879533608e-05, 'epoch': 0.47}
{'loss': 0.0607, 'learning_rate': 2.990476892188105e-05, 'epoch': 0.4}
{'eval_loss': 0.04764551296830177, 'eval_accuracy': 0.9845978519874297, 'eval_f1': 0.9845861413513047, 'eval_precision': 0.988755075833825, 'eval_recall': 0.9804522146126579, 'eval_runtime': 353.3554, 'eval_samples_per_second': 163.897, 'eval_steps_per_second': 20.489, 'epoch': 1.0}
{'loss': 0.081, 'learning_rate': 2.880878262631473e-05, 'epoch': 0.42}
{'loss': 0.0449, 'learning_rate': 2.8210134649744885e-05, 'epoch': 0.44}
{'loss': 0.0734, 'learning_rate': 2.9895558953010742e-05, 'epoch': 0.4}
{'train_runtime': 8254.412, 'train_samples_per_second': 21.049, 'train_steps_per_second': 0.329, 'train_loss': 0.07462974595470119, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0397, 'learning_rate': 4.495375658646229e-06, 'epoch': 0.91}
{'loss': 0.0298, 'learning_rate': 4.7901543903607355e-06, 'epoch': 0.9}
{'loss': 0.0383, 'learning_rate': 6.116658683076016e-06, 'epoch': 0.88}
{'loss': 0.0294, 'learning_rate': 2.6589180128571166e-05, 'epoch': 0.47}
{'loss': 0.0458, 'learning_rate': 2.9831089170918607e-05, 'epoch': 0.4}
{'loss': 0.0799, 'learning_rate': 2.8735102875352282e-05, 'epoch': 0.43}
{'loss': 0.0613, 'learning_rate': 2.9821879202048304e-05, 'epoch': 0.4}
{'loss': 0.0666, 'learning_rate': 2.8136454898782444e-05, 'epoch': 0.44}
{'loss': 0.0577, 'learning_rate': 4.347986292788975e-06, 'epoch': 0.91}
{'loss': 0.0413, 'learning_rate': 2.6515500377608725e-05, 'epoch': 0.47}
{'loss': 0.035, 'learning_rate': 4.642765024503482e-06, 'epoch': 0.91}
{'loss': 0.0661, 'learning_rate': 5.9692693172187625e-06, 'epoch': 0.88}
{'loss': 0.1177, 'learning_rate': 2.9757409419956162e-05, 'epoch': 0.41}
{'loss': 0.0703, 'learning_rate': 2.866142312438984e-05, 'epoch': 0.43}
{'loss': 0.0795, 'learning_rate': 2.9748199451085856e-05, 'epoch': 0.41}
{'loss': 0.0643, 'learning_rate': 2.8062775147820002e-05, 'epoch': 0.44}
{'loss': 0.0842, 'learning_rate': 2.6441820626646287e-05, 'epoch': 0.47}
{'loss': 0.0311, 'learning_rate': 4.200596926931722e-06, 'epoch': 0.92}
{'loss': 0.0619, 'learning_rate': 5.821879951361509e-06, 'epoch': 0.88}
{'loss': 0.0368, 'learning_rate': 4.495375658646229e-06, 'epoch': 0.91}
{'loss': 0.0601, 'learning_rate': 2.968372966899372e-05, 'epoch': 0.41}
{'loss': 0.0564, 'learning_rate': 2.85877433734274e-05, 'epoch': 0.43}
{'loss': 0.0629, 'learning_rate': 2.9674519700123415e-05, 'epoch': 0.41}
{'loss': 0.0499, 'learning_rate': 2.798909539685756e-05, 'epoch': 0.44}
{'loss': 0.0554, 'learning_rate': 2.6368140875683845e-05, 'epoch': 0.47}
{'loss': 0.0467, 'learning_rate': 4.053207561074469e-06, 'epoch': 0.92}
{'loss': 0.0286, 'learning_rate': 5.674490585504256e-06, 'epoch': 0.89}
{'loss': 0.0449, 'learning_rate': 4.347986292788975e-06, 'epoch': 0.91}
{'loss': 0.0315, 'learning_rate': 2.961004991803128e-05, 'epoch': 0.41}
{'loss': 0.0244, 'learning_rate': 2.851406362246496e-05, 'epoch': 0.43}
{'loss': 0.0694, 'learning_rate': 2.9600839949160973e-05, 'epoch': 0.41}
{'loss': 0.041, 'learning_rate': 2.7915415645895123e-05, 'epoch': 0.44}
{'loss': 0.0501, 'learning_rate': 2.6294461124721404e-05, 'epoch': 0.47}
{'loss': 0.0296, 'learning_rate': 3.905818195217216e-06, 'epoch': 0.92}
{'loss': 0.032, 'learning_rate': 5.527101219647002e-06, 'epoch': 0.89}
{'loss': 0.0224, 'learning_rate': 4.200596926931722e-06, 'epoch': 0.92}
{'loss': 0.0654, 'learning_rate': 2.9536370167068838e-05, 'epoch': 0.41}
{'loss': 0.0493, 'learning_rate': 2.844038387150252e-05, 'epoch': 0.43}
{'loss': 0.0853, 'learning_rate': 2.9527160198198535e-05, 'epoch': 0.41}
{'loss': 0.0838, 'learning_rate': 2.784173589493268e-05, 'epoch': 0.44}
{'loss': 0.0391, 'learning_rate': 2.6220781373758956e-05, 'epoch': 0.48}
{'loss': 0.0498, 'learning_rate': 3.758428829359962e-06, 'epoch': 0.93}
{'loss': 0.0298, 'learning_rate': 5.379711853789749e-06, 'epoch': 0.89}
{'loss': 0.0415, 'learning_rate': 4.053207561074469e-06, 'epoch': 0.92}
{'loss': 0.0412, 'learning_rate': 2.9462690416106397e-05, 'epoch': 0.41}
{'loss': 0.07, 'learning_rate': 2.9453480447236094e-05, 'epoch': 0.41}
{'loss': 0.0381, 'learning_rate': 2.8366704120540078e-05, 'epoch': 0.43}
{'loss': 0.0423, 'learning_rate': 2.776805614397024e-05, 'epoch': 0.44}
{'loss': 0.0903, 'learning_rate': 2.6147101622796514e-05, 'epoch': 0.48}
{'loss': 0.075, 'learning_rate': 3.6110394635027085e-06, 'epoch': 0.93}
{'loss': 0.0289, 'learning_rate': 5.232322487932496e-06, 'epoch': 0.9}
{'loss': 0.0356, 'learning_rate': 3.905818195217216e-06, 'epoch': 0.92}
{'loss': 0.0464, 'learning_rate': 2.9389010665143955e-05, 'epoch': 0.41}
{'loss': 0.0527, 'learning_rate': 2.8293024369577636e-05, 'epoch': 0.43}
{'loss': 0.0424, 'learning_rate': 2.9379800696273652e-05, 'epoch': 0.41}
{'loss': 0.0766, 'learning_rate': 2.6073421871834076e-05, 'epoch': 0.48}
{'loss': 0.0711, 'learning_rate': 2.769437639300779e-05, 'epoch': 0.45}
{'loss': 0.0318, 'learning_rate': 3.4636500976454553e-06, 'epoch': 0.93}
{'loss': 0.0354, 'learning_rate': 5.084933122075243e-06, 'epoch': 0.9}
{'loss': 0.0701, 'learning_rate': 3.758428829359962e-06, 'epoch': 0.93}
{'loss': 0.0683, 'learning_rate': 2.9315330914181517e-05, 'epoch': 0.41}
{'loss': 0.0366, 'learning_rate': 2.821934461861519e-05, 'epoch': 0.44}
{'loss': 0.0377, 'learning_rate': 2.930612094531121e-05, 'epoch': 0.41}
{'loss': 0.057, 'learning_rate': 2.5999742120871635e-05, 'epoch': 0.48}
{'loss': 0.0429, 'learning_rate': 2.762069664204535e-05, 'epoch': 0.45}
{'loss': 0.08, 'learning_rate': 3.3162607317882017e-06, 'epoch': 0.93}
{'loss': 0.0607, 'learning_rate': 2.924165116321907e-05, 'epoch': 0.42}
{'loss': 0.0525, 'learning_rate': 4.937543756217989e-06, 'epoch': 0.9}
{'loss': 0.0744, 'learning_rate': 3.6110394635027085e-06, 'epoch': 0.93}
{'loss': 0.0368, 'learning_rate': 2.5926062369909193e-05, 'epoch': 0.48}
{'loss': 0.0536, 'learning_rate': 2.9232441194348766e-05, 'epoch': 0.42}
{'loss': 0.0614, 'learning_rate': 2.814566486765275e-05, 'epoch': 0.44}
{'loss': 0.0641, 'learning_rate': 2.7547016891082912e-05, 'epoch': 0.45}
{'loss': 0.0657, 'learning_rate': 2.9167971412256627e-05, 'epoch': 0.42}
{'loss': 0.0569, 'learning_rate': 3.1688713659309486e-06, 'epoch': 0.94}
{'loss': 0.0497, 'learning_rate': 4.7901543903607355e-06, 'epoch': 0.9}
{'loss': 0.0369, 'learning_rate': 3.4636500976454553e-06, 'epoch': 0.93}
{'loss': 0.0606, 'learning_rate': 2.5852382618946752e-05, 'epoch': 0.48}
{'loss': 0.0516, 'learning_rate': 2.9158761443386324e-05, 'epoch': 0.42}
{'loss': 0.0856, 'learning_rate': 2.807198511669031e-05, 'epoch': 0.44}
{'loss': 0.0627, 'learning_rate': 2.747333714012047e-05, 'epoch': 0.45}
{'loss': 0.0583, 'learning_rate': 2.9094291661294186e-05, 'epoch': 0.42}
{'loss': 0.0561, 'learning_rate': 3.0214820000736945e-06, 'epoch': 0.94}
{'loss': 0.0326, 'learning_rate': 4.642765024503482e-06, 'epoch': 0.91}
{'loss': 0.035, 'learning_rate': 3.3162607317882017e-06, 'epoch': 0.93}
{'loss': 0.0579, 'learning_rate': 2.577870286798431e-05, 'epoch': 0.48}
{'loss': 0.065, 'learning_rate': 2.9085081692423883e-05, 'epoch': 0.42}
{'loss': 0.0336, 'learning_rate': 2.7998305365727867e-05, 'epoch': 0.44}
{'loss': 0.049, 'learning_rate': 2.739965738915803e-05, 'epoch': 0.45}
{'loss': 0.0432, 'learning_rate': 2.9020611910331748e-05, 'epoch': 0.42}
{'loss': 0.051, 'learning_rate': 2.8740926342164414e-06, 'epoch': 0.94}
{'loss': 0.0309, 'learning_rate': 4.495375658646229e-06, 'epoch': 0.91}
{'loss': 0.037, 'learning_rate': 3.1688713659309486e-06, 'epoch': 0.94}
{'loss': 0.066, 'learning_rate': 2.5705023117021866e-05, 'epoch': 0.49}
{'loss': 0.0399, 'learning_rate': 2.901140194146144e-05, 'epoch': 0.42}
{'loss': 0.0447, 'learning_rate': 2.7924625614765426e-05, 'epoch': 0.44}
{'loss': 0.0785, 'learning_rate': 2.8946932159369306e-05, 'epoch': 0.42}
{'loss': 0.05, 'learning_rate': 2.7325977638195588e-05, 'epoch': 0.45}
{'loss': 0.0591, 'learning_rate': 2.726703268359188e-06, 'epoch': 0.95}
{'loss': 0.0824, 'learning_rate': 4.347986292788975e-06, 'epoch': 0.91}
{'loss': 0.0559, 'learning_rate': 3.0214820000736945e-06, 'epoch': 0.94}
{'loss': 0.0503, 'learning_rate': 2.5631343366059424e-05, 'epoch': 0.49}
{'loss': 0.0534, 'learning_rate': 2.8937722190499e-05, 'epoch': 0.42}
{'loss': 0.0756, 'learning_rate': 2.7850945863802984e-05, 'epoch': 0.44}
{'loss': 0.0367, 'learning_rate': 2.8873252408406865e-05, 'epoch': 0.42}
{'loss': 0.0783, 'learning_rate': 2.7252297887233146e-05, 'epoch': 0.46}
{'loss': 0.0545, 'learning_rate': 2.5793139025019346e-06, 'epoch': 0.95}
{'loss': 0.0571, 'learning_rate': 4.200596926931722e-06, 'epoch': 0.92}
{'loss': 0.0437, 'learning_rate': 2.8740926342164414e-06, 'epoch': 0.94}
{'loss': 0.045, 'learning_rate': 2.5557663615096983e-05, 'epoch': 0.49}
{'loss': 0.0239, 'learning_rate': 2.886404243953656e-05, 'epoch': 0.42}
{'loss': 0.0653, 'learning_rate': 2.8799572657444424e-05, 'epoch': 0.42}
{'loss': 0.0525, 'learning_rate': 2.7777266112840543e-05, 'epoch': 0.44}
{'loss': 0.0508, 'learning_rate': 2.71786181362707e-05, 'epoch': 0.46}
{'loss': 0.0711, 'learning_rate': 2.548398386413454e-05, 'epoch': 0.49}
{'loss': 0.0445, 'learning_rate': 2.431924536644681e-06, 'epoch': 0.95}
{'loss': 0.0363, 'learning_rate': 4.053207561074469e-06, 'epoch': 0.92}
{'loss': 0.0466, 'learning_rate': 2.726703268359188e-06, 'epoch': 0.95}
{'loss': 0.0374, 'learning_rate': 2.872589290648198e-05, 'epoch': 0.43}
{'loss': 0.0565, 'learning_rate': 2.8790362688574117e-05, 'epoch': 0.42}
{'loss': 0.0369, 'learning_rate': 2.7703586361878098e-05, 'epoch': 0.45}
{'loss': 0.0439, 'learning_rate': 2.710493838530826e-05, 'epoch': 0.46}
{'loss': 0.0373, 'learning_rate': 2.54103041131721e-05, 'epoch': 0.49}
{'loss': 0.0414, 'learning_rate': 2.284535170787428e-06, 'epoch': 0.95}
{'loss': 0.0516, 'learning_rate': 3.905818195217216e-06, 'epoch': 0.92}
{'loss': 0.0454, 'learning_rate': 2.5793139025019346e-06, 'epoch': 0.95}
{'loss': 0.0571, 'learning_rate': 2.8652213155519537e-05, 'epoch': 0.43}
{'loss': 0.0432, 'learning_rate': 2.8716682937611672e-05, 'epoch': 0.43}
{'loss': 0.059, 'learning_rate': 2.7629906610915657e-05, 'epoch': 0.45}
{'loss': 0.0739, 'learning_rate': 2.533662436220966e-05, 'epoch': 0.49}
{'loss': 0.0419, 'learning_rate': 2.703125863434582e-05, 'epoch': 0.46}
{'loss': 0.0252, 'learning_rate': 2.1371458049301743e-06, 'epoch': 0.96}
{'loss': 0.0673, 'learning_rate': 3.758428829359962e-06, 'epoch': 0.93}
{'loss': 0.0143, 'learning_rate': 2.431924536644681e-06, 'epoch': 0.95}
{'loss': 0.0486, 'learning_rate': 2.8578533404557096e-05, 'epoch': 0.43}
{'loss': 0.066, 'learning_rate': 2.864300318664923e-05, 'epoch': 0.43}
{'loss': 0.074, 'learning_rate': 2.526294461124722e-05, 'epoch': 0.5}
{'loss': 0.0514, 'learning_rate': 2.7556226859953215e-05, 'epoch': 0.45}
{'loss': 0.0424, 'learning_rate': 2.6957578883383377e-05, 'epoch': 0.46}
{'loss': 0.0296, 'learning_rate': 3.6110394635027085e-06, 'epoch': 0.93}
{'loss': 0.0609, 'learning_rate': 1.989756439072921e-06, 'epoch': 0.96}
{'loss': 0.0795, 'learning_rate': 2.284535170787428e-06, 'epoch': 0.95}
{'loss': 0.0428, 'learning_rate': 2.8504853653594654e-05, 'epoch': 0.43}
{'loss': 0.0604, 'learning_rate': 2.856932343568679e-05, 'epoch': 0.43}
{'loss': 0.0553, 'learning_rate': 2.5189264860284772e-05, 'epoch': 0.5}
{'loss': 0.0654, 'learning_rate': 2.7482547108990774e-05, 'epoch': 0.45}
{'loss': 0.0503, 'learning_rate': 2.6883899132420936e-05, 'epoch': 0.46}
{'loss': 0.03, 'learning_rate': 3.4636500976454553e-06, 'epoch': 0.93}
{'loss': 0.0406, 'learning_rate': 1.8423670732156675e-06, 'epoch': 0.96}
{'loss': 0.08, 'learning_rate': 2.8431173902632213e-05, 'epoch': 0.43}
{'loss': 0.0566, 'learning_rate': 2.1371458049301743e-06, 'epoch': 0.96}
{'loss': 0.0548, 'learning_rate': 2.8495643684724348e-05, 'epoch': 0.43}
{'loss': 0.0552, 'learning_rate': 2.511558510932233e-05, 'epoch': 0.5}
{'loss': 0.0672, 'learning_rate': 2.7408867358028336e-05, 'epoch': 0.45}
{'loss': 0.05, 'learning_rate': 2.6810219381458494e-05, 'epoch': 0.46}
{'loss': 0.034, 'learning_rate': 3.3162607317882017e-06, 'epoch': 0.93}
{'loss': 0.0316, 'learning_rate': 1.6949777073584142e-06, 'epoch': 0.97}
{'loss': 0.0994, 'learning_rate': 2.835749415166977e-05, 'epoch': 0.43}
{'loss': 0.052, 'learning_rate': 1.989756439072921e-06, 'epoch': 0.96}
{'loss': 0.0674, 'learning_rate': 2.842196393376191e-05, 'epoch': 0.43}
{'loss': 0.0299, 'learning_rate': 2.504190535835989e-05, 'epoch': 0.5}
{'loss': 0.0322, 'learning_rate': 2.7335187607065894e-05, 'epoch': 0.45}
{'loss': 0.068, 'learning_rate': 2.6736539630496056e-05, 'epoch': 0.47}
{'loss': 0.0457, 'learning_rate': 3.1688713659309486e-06, 'epoch': 0.94}
{'loss': 0.0354, 'learning_rate': 2.828381440070733e-05, 'epoch': 0.43}
{'loss': 0.0582, 'learning_rate': 1.5475883415011608e-06, 'epoch': 0.97}
{'loss': 0.0437, 'learning_rate': 1.8423670732156675e-06, 'epoch': 0.96}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0698, 'learning_rate': 2.496822560739745e-05, 'epoch': 0.5}
{'loss': 0.0501, 'learning_rate': 2.834828418279947e-05, 'epoch': 0.43}
{'loss': 0.0621, 'learning_rate': 2.7261507856103453e-05, 'epoch': 0.46}
{'loss': 0.0522, 'learning_rate': 2.6662859879533608e-05, 'epoch': 0.47}
{'loss': 0.0553, 'learning_rate': 2.8210134649744885e-05, 'epoch': 0.44}
{'loss': 0.0375, 'learning_rate': 3.0214820000736945e-06, 'epoch': 0.94}
{'loss': 0.0378, 'learning_rate': 1.4001989756439072e-06, 'epoch': 0.97}
{'loss': 0.0586, 'learning_rate': 1.6949777073584142e-06, 'epoch': 0.97}
{'loss': 0.0486, 'learning_rate': 2.8274604431837027e-05, 'epoch': 0.43}
{'loss': 0.0558, 'learning_rate': 2.489454585643501e-05, 'epoch': 0.5}
{'loss': 0.0441, 'learning_rate': 2.7187828105141004e-05, 'epoch': 0.46}
{'loss': 0.0221, 'learning_rate': 2.6589180128571166e-05, 'epoch': 0.47}
{'loss': 0.0531, 'learning_rate': 2.8740926342164414e-06, 'epoch': 0.94}
{'loss': 0.0746, 'learning_rate': 2.8136454898782444e-05, 'epoch': 0.44}
{'loss': 0.06, 'learning_rate': 1.2528096097866538e-06, 'epoch': 0.98}
{'loss': 0.0605, 'learning_rate': 1.5475883415011608e-06, 'epoch': 0.97}
{'loss': 0.0507, 'learning_rate': 2.4820866105472565e-05, 'epoch': 0.5}
{'loss': 0.0323, 'learning_rate': 2.820092468087458e-05, 'epoch': 0.44}
{'loss': 0.0375, 'learning_rate': 2.7114148354178563e-05, 'epoch': 0.46}
{'loss': 0.1059, 'learning_rate': 2.6515500377608725e-05, 'epoch': 0.47}
{'loss': 0.0531, 'learning_rate': 2.8062775147820002e-05, 'epoch': 0.44}
{'loss': 0.0141, 'learning_rate': 2.726703268359188e-06, 'epoch': 0.95}
{'loss': 0.058, 'learning_rate': 1.1054202439294004e-06, 'epoch': 0.98}
{'loss': 0.0497, 'learning_rate': 1.4001989756439072e-06, 'epoch': 0.97}
{'loss': 0.0579, 'learning_rate': 2.4747186354510123e-05, 'epoch': 0.51}
{'loss': 0.058, 'learning_rate': 2.8127244929912137e-05, 'epoch': 0.44}
{'loss': 0.0392, 'learning_rate': 2.7040468603216125e-05, 'epoch': 0.46}
{'loss': 0.1045, 'learning_rate': 2.6441820626646287e-05, 'epoch': 0.47}
{'loss': 0.0633, 'learning_rate': 2.798909539685756e-05, 'epoch': 0.44}
{'loss': 0.0374, 'learning_rate': 2.5793139025019346e-06, 'epoch': 0.95}
{'loss': 0.0463, 'learning_rate': 9.58030878072147e-07, 'epoch': 0.98}
{'loss': 0.0256, 'learning_rate': 2.4673506603547682e-05, 'epoch': 0.51}
{'loss': 0.0603, 'learning_rate': 1.2528096097866538e-06, 'epoch': 0.98}
{'loss': 0.0734, 'learning_rate': 2.80535651789497e-05, 'epoch': 0.44}
{'loss': 0.0696, 'learning_rate': 2.7915415645895123e-05, 'epoch': 0.44}
{'loss': 0.0732, 'learning_rate': 2.6368140875683845e-05, 'epoch': 0.47}
{'loss': 0.0681, 'learning_rate': 2.6966788852253684e-05, 'epoch': 0.46}
{'loss': 0.0611, 'learning_rate': 2.431924536644681e-06, 'epoch': 0.95}
{'loss': 0.0337, 'learning_rate': 8.106415122148938e-07, 'epoch': 0.98}
{'loss': 0.0837, 'learning_rate': 2.4599826852585244e-05, 'epoch': 0.51}
{'loss': 0.0923, 'learning_rate': 1.1054202439294004e-06, 'epoch': 0.98}
{'loss': 0.0596, 'learning_rate': 2.7979885427987258e-05, 'epoch': 0.44}
{'loss': 0.0655, 'learning_rate': 2.784173589493268e-05, 'epoch': 0.44}
{'loss': 0.0527, 'learning_rate': 2.6294461124721404e-05, 'epoch': 0.47}
{'loss': 0.088, 'learning_rate': 2.6893109101291242e-05, 'epoch': 0.46}
{'loss': 0.0451, 'learning_rate': 2.284535170787428e-06, 'epoch': 0.95}
{'loss': 0.0322, 'learning_rate': 6.632521463576403e-07, 'epoch': 0.99}
{'loss': 0.0313, 'learning_rate': 2.45261471016228e-05, 'epoch': 0.51}
{'loss': 0.0473, 'learning_rate': 9.58030878072147e-07, 'epoch': 0.98}
{'loss': 0.0632, 'learning_rate': 2.7906205677024816e-05, 'epoch': 0.44}
{'loss': 0.0606, 'learning_rate': 2.776805614397024e-05, 'epoch': 0.44}
{'loss': 0.0551, 'learning_rate': 2.6220781373758956e-05, 'epoch': 0.48}
{'loss': 0.0669, 'learning_rate': 2.68194293503288e-05, 'epoch': 0.46}
{'loss': 0.0637, 'learning_rate': 5.158627805003869e-07, 'epoch': 0.99}
{'loss': 0.0651, 'learning_rate': 2.4452467350660357e-05, 'epoch': 0.51}
{'loss': 0.0427, 'learning_rate': 2.1371458049301743e-06, 'epoch': 0.96}
{'loss': 0.0301, 'learning_rate': 8.106415122148938e-07, 'epoch': 0.98}
{'loss': 0.044, 'learning_rate': 2.7832525926062375e-05, 'epoch': 0.44}
{'loss': 0.0606, 'learning_rate': 2.769437639300779e-05, 'epoch': 0.45}
{'loss': 0.0927, 'learning_rate': 2.6147101622796514e-05, 'epoch': 0.48}
{'loss': 0.054, 'learning_rate': 2.674574959936636e-05, 'epoch': 0.47}
{'loss': 0.054, 'learning_rate': 2.4378787599697916e-05, 'epoch': 0.51}
{'loss': 0.039, 'learning_rate': 3.684734146431335e-07, 'epoch': 0.99}
{'loss': 0.0365, 'learning_rate': 1.989756439072921e-06, 'epoch': 0.96}
{'loss': 0.0567, 'learning_rate': 6.632521463576403e-07, 'epoch': 0.99}
{'loss': 0.065, 'learning_rate': 2.7758846175099933e-05, 'epoch': 0.44}
{'loss': 0.0666, 'learning_rate': 2.762069664204535e-05, 'epoch': 0.45}
{'loss': 0.0551, 'learning_rate': 2.6073421871834076e-05, 'epoch': 0.48}
{'loss': 0.0568, 'learning_rate': 2.6672069848403914e-05, 'epoch': 0.47}
{'loss': 0.0648, 'learning_rate': 2.430510784873547e-05, 'epoch': 0.51}
{'loss': 0.0314, 'learning_rate': 2.2108404878588012e-07, 'epoch': 1.0}
{'loss': 0.0745, 'learning_rate': 1.8423670732156675e-06, 'epoch': 0.96}
{'loss': 0.0457, 'learning_rate': 5.158627805003869e-07, 'epoch': 0.99}
{'loss': 0.0605, 'learning_rate': 2.768516642413749e-05, 'epoch': 0.45}
{'loss': 0.0378, 'learning_rate': 2.7547016891082912e-05, 'epoch': 0.45}
{'loss': 0.0446, 'learning_rate': 2.5999742120871635e-05, 'epoch': 0.48}
{'loss': 0.0344, 'learning_rate': 2.4231428097773033e-05, 'epoch': 0.52}
{'loss': 0.0784, 'learning_rate': 2.6598390097441473e-05, 'epoch': 0.47}
{'loss': 0.0584, 'learning_rate': 7.36946829286267e-08, 'epoch': 1.0}
{'loss': 0.06, 'learning_rate': 3.684734146431335e-07, 'epoch': 0.99}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0556, 'learning_rate': 2.7611486673175047e-05, 'epoch': 0.45}
{'loss': 0.0428, 'learning_rate': 1.6949777073584142e-06, 'epoch': 0.97}
{'loss': 0.0835, 'learning_rate': 2.747333714012047e-05, 'epoch': 0.45}
{'loss': 0.0495, 'learning_rate': 2.5926062369909193e-05, 'epoch': 0.48}
{'loss': 0.0557, 'learning_rate': 2.415774834681059e-05, 'epoch': 0.52}
{'loss': 0.0476, 'learning_rate': 2.652471034647903e-05, 'epoch': 0.47}
{'loss': 0.0376, 'learning_rate': 2.2108404878588012e-07, 'epoch': 1.0}
{'loss': 0.0591, 'learning_rate': 2.7537806922212606e-05, 'epoch': 0.45}
{'loss': 0.0647, 'learning_rate': 2.739965738915803e-05, 'epoch': 0.45}
{'loss': 0.0388, 'learning_rate': 1.5475883415011608e-06, 'epoch': 0.97}
{'loss': 0.0498, 'learning_rate': 2.408406859584815e-05, 'epoch': 0.52}
{'loss': 0.0704, 'learning_rate': 2.5852382618946752e-05, 'epoch': 0.48}
{'loss': 0.0605, 'learning_rate': 2.645103059551659e-05, 'epoch': 0.47}
{'loss': 0.0352, 'learning_rate': 7.36946829286267e-08, 'epoch': 1.0}
{'loss': 0.0606, 'learning_rate': 2.7325977638195588e-05, 'epoch': 0.45}
{'loss': 0.0617, 'learning_rate': 2.7464127171250164e-05, 'epoch': 0.45}
{'loss': 0.0688, 'learning_rate': 1.4001989756439072e-06, 'epoch': 0.97}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0597, 'learning_rate': 2.4010388844885705e-05, 'epoch': 0.52}
{'loss': 0.0233, 'learning_rate': 2.577870286798431e-05, 'epoch': 0.48}
{'loss': 0.0549, 'learning_rate': 2.637735084455415e-05, 'epoch': 0.47}
{'loss': 0.0582, 'learning_rate': 2.7252297887233146e-05, 'epoch': 0.46}
{'loss': 0.0658, 'learning_rate': 2.7390447420287723e-05, 'epoch': 0.45}
{'loss': 0.0678, 'learning_rate': 1.2528096097866538e-06, 'epoch': 0.98}
{'loss': 0.0546, 'learning_rate': 2.3936709093923264e-05, 'epoch': 0.52}
{'loss': 0.0713, 'learning_rate': 2.5705023117021866e-05, 'epoch': 0.49}
{'loss': 0.0493, 'learning_rate': 2.6303671093591707e-05, 'epoch': 0.47}
{'loss': 0.0427, 'learning_rate': 2.71786181362707e-05, 'epoch': 0.46}
{'loss': 0.0448, 'learning_rate': 2.731676766932528e-05, 'epoch': 0.45}
{'loss': 0.0463, 'learning_rate': 1.1054202439294004e-06, 'epoch': 0.98}
{'loss': 0.0753, 'learning_rate': 2.3863029342960826e-05, 'epoch': 0.52}
{'loss': 0.1028, 'learning_rate': 2.5631343366059424e-05, 'epoch': 0.49}
{'loss': 0.0295, 'learning_rate': 2.622999134262927e-05, 'epoch': 0.48}
{'loss': 0.0364, 'learning_rate': 2.710493838530826e-05, 'epoch': 0.46}
{'loss': 0.0674, 'learning_rate': 2.7243087918362843e-05, 'epoch': 0.46}
{'loss': 0.0524, 'learning_rate': 2.378934959199838e-05, 'epoch': 0.52}
{'loss': 0.03, 'learning_rate': 9.58030878072147e-07, 'epoch': 0.98}
{'loss': 0.0602, 'learning_rate': 2.5557663615096983e-05, 'epoch': 0.49}
{'loss': 0.0437, 'learning_rate': 2.615631159166682e-05, 'epoch': 0.48}
{'loss': 0.0972, 'learning_rate': 2.703125863434582e-05, 'epoch': 0.46}
{'loss': 0.0418, 'learning_rate': 2.7169408167400395e-05, 'epoch': 0.46}
{'loss': 0.0387, 'learning_rate': 2.371566984103594e-05, 'epoch': 0.53}
{'loss': 0.0443, 'learning_rate': 8.106415122148938e-07, 'epoch': 0.98}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0589, 'learning_rate': 2.548398386413454e-05, 'epoch': 0.49}
{'loss': 0.0652, 'learning_rate': 2.608263184070438e-05, 'epoch': 0.48}
{'loss': 0.0798, 'learning_rate': 2.6957578883383377e-05, 'epoch': 0.46}
{'loss': 0.0757, 'learning_rate': 2.7095728416437954e-05, 'epoch': 0.46}
{'loss': 0.0417, 'learning_rate': 2.3641990090073498e-05, 'epoch': 0.53}
{'loss': 0.0531, 'learning_rate': 6.632521463576403e-07, 'epoch': 0.99}
{'loss': 0.0409, 'learning_rate': 2.54103041131721e-05, 'epoch': 0.49}
{'loss': 0.0685, 'learning_rate': 2.6008952089741938e-05, 'epoch': 0.48}
{'loss': 0.0248, 'learning_rate': 2.6883899132420936e-05, 'epoch': 0.46}
{'loss': 0.0296, 'learning_rate': 2.3568310339111057e-05, 'epoch': 0.53}
{'loss': 0.0658, 'learning_rate': 2.7022048665475512e-05, 'epoch': 0.46}
{'loss': 0.0633, 'learning_rate': 5.158627805003869e-07, 'epoch': 0.99}
{'loss': 0.0554, 'learning_rate': 2.533662436220966e-05, 'epoch': 0.49}
{'loss': 0.0929, 'learning_rate': 2.6810219381458494e-05, 'epoch': 0.46}
{'loss': 0.091, 'learning_rate': 2.3494630588148615e-05, 'epoch': 0.53}
{'loss': 0.0705, 'learning_rate': 2.59352723387795e-05, 'epoch': 0.48}
{'loss': 0.0752, 'learning_rate': 2.6948368914513074e-05, 'epoch': 0.46}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0477, 'learning_rate': 3.684734146431335e-07, 'epoch': 0.99}
{'loss': 0.0369, 'learning_rate': 2.526294461124722e-05, 'epoch': 0.5}
{'loss': 0.0508, 'learning_rate': 2.6736539630496056e-05, 'epoch': 0.47}
{'loss': 0.0552, 'learning_rate': 2.3420950837186174e-05, 'epoch': 0.53}
{'loss': 0.0539, 'learning_rate': 2.586159258781706e-05, 'epoch': 0.48}
{'loss': 0.0419, 'learning_rate': 2.6874689163550633e-05, 'epoch': 0.46}
{'loss': 0.0507, 'learning_rate': 2.2108404878588012e-07, 'epoch': 1.0}
{'loss': 0.0924, 'learning_rate': 2.6662859879533608e-05, 'epoch': 0.47}
{'loss': 0.0588, 'learning_rate': 2.5189264860284772e-05, 'epoch': 0.5}
{'loss': 0.0422, 'learning_rate': 2.3347271086223732e-05, 'epoch': 0.53}
{'loss': 0.0525, 'learning_rate': 2.5787912836854617e-05, 'epoch': 0.48}
{'loss': 0.0374, 'learning_rate': 2.680100941258819e-05, 'epoch': 0.46}
{'loss': 0.0577, 'learning_rate': 7.36946829286267e-08, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0253, 'learning_rate': 2.6589180128571166e-05, 'epoch': 0.47}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0894, 'learning_rate': 2.511558510932233e-05, 'epoch': 0.5}
{'loss': 0.0612, 'learning_rate': 2.3273591335261287e-05, 'epoch': 0.53}
{'loss': 0.0659, 'learning_rate': 2.5714233085892175e-05, 'epoch': 0.49}
{'loss': 0.0942, 'learning_rate': 2.6736539630496056e-05, 'epoch': 0.47}
{'loss': 0.0595, 'learning_rate': 2.6515500377608725e-05, 'epoch': 0.47}
{'loss': 0.0531, 'learning_rate': 2.3199911584298846e-05, 'epoch': 0.54}
{'loss': 0.0519, 'learning_rate': 2.504190535835989e-05, 'epoch': 0.5}
{'loss': 0.0605, 'learning_rate': 2.564055333492973e-05, 'epoch': 0.49}
{'loss': 0.0555, 'learning_rate': 2.6662859879533608e-05, 'epoch': 0.47}
{'loss': 0.0494, 'learning_rate': 2.6441820626646287e-05, 'epoch': 0.47}
{'loss': 0.0369, 'learning_rate': 2.3126231833336408e-05, 'epoch': 0.54}
{'loss': 0.0503, 'learning_rate': 2.496822560739745e-05, 'epoch': 0.5}
{'loss': 0.0642, 'learning_rate': 2.556687358396729e-05, 'epoch': 0.49}
{'loss': 0.0436, 'learning_rate': 2.6589180128571166e-05, 'epoch': 0.47}
{'loss': 0.0461, 'learning_rate': 2.6368140875683845e-05, 'epoch': 0.47}
{'loss': 0.0713, 'learning_rate': 2.3052552082373967e-05, 'epoch': 0.54}
{'loss': 0.082, 'learning_rate': 2.489454585643501e-05, 'epoch': 0.5}
{'loss': 0.0501, 'learning_rate': 2.5493193833004848e-05, 'epoch': 0.49}
{'loss': 0.0803, 'learning_rate': 2.6515500377608725e-05, 'epoch': 0.47}
{'eval_loss': 0.0452265664935112, 'eval_accuracy': 0.9843561142383535, 'eval_f1': 0.984367451169853, 'eval_precision': 0.9870583757223433, 'eval_recall': 0.981691158756926, 'eval_runtime': 352.4569, 'eval_samples_per_second': 164.315, 'eval_steps_per_second': 20.542, 'epoch': 1.0}
{'train_runtime': 8259.6099, 'train_samples_per_second': 21.035, 'train_steps_per_second': 0.329, 'train_loss': 0.07323451891442652, 'epoch': 1.0}
{'loss': 0.0724, 'learning_rate': 2.6294461124721404e-05, 'epoch': 0.47}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0416, 'learning_rate': 2.297887233141152e-05, 'epoch': 0.54}
{'loss': 0.0571, 'learning_rate': 2.4820866105472565e-05, 'epoch': 0.5}
{'loss': 0.0367, 'learning_rate': 2.5419514082042406e-05, 'epoch': 0.49}
{'loss': 0.0652, 'learning_rate': 2.6441820626646287e-05, 'epoch': 0.47}
{'loss': 0.065, 'learning_rate': 2.6220781373758956e-05, 'epoch': 0.48}
{'loss': 0.023, 'learning_rate': 2.290519258044908e-05, 'epoch': 0.54}
{'loss': 0.0507, 'learning_rate': 2.4747186354510123e-05, 'epoch': 0.51}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0591, 'learning_rate': 2.5345834331079965e-05, 'epoch': 0.49}
{'loss': 0.0657, 'learning_rate': 2.6368140875683845e-05, 'epoch': 0.47}
{'loss': 0.0338, 'learning_rate': 2.6147101622796514e-05, 'epoch': 0.48}
{'loss': 0.0595, 'learning_rate': 2.283151282948664e-05, 'epoch': 0.54}
{'eval_loss': 0.04747277498245239, 'eval_accuracy': 0.984546051041199, 'eval_f1': 0.9845425813025681, 'eval_precision': 0.9881777839412009, 'eval_recall': 0.9809340262243177, 'eval_runtime': 353.4825, 'eval_samples_per_second': 163.838, 'eval_steps_per_second': 20.482, 'epoch': 1.0}
{'train_runtime': 8280.3268, 'train_samples_per_second': 20.983, 'train_steps_per_second': 0.328, 'train_loss': 0.07228035861921381, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0387, 'learning_rate': 2.4673506603547682e-05, 'epoch': 0.51}
{'loss': 0.0694, 'learning_rate': 2.5272154580117523e-05, 'epoch': 0.5}
{'loss': 0.0392, 'learning_rate': 2.6294461124721404e-05, 'epoch': 0.47}
{'loss': 0.0581, 'learning_rate': 2.6073421871834076e-05, 'epoch': 0.48}
{'loss': 0.0472, 'learning_rate': 2.2757833078524197e-05, 'epoch': 0.55}
{'loss': 0.0552, 'learning_rate': 2.4599826852585244e-05, 'epoch': 0.51}
{'loss': 0.0316, 'learning_rate': 2.6220781373758956e-05, 'epoch': 0.48}
{'loss': 0.0534, 'learning_rate': 2.519847482915508e-05, 'epoch': 0.5}
{'loss': 0.0542, 'learning_rate': 2.5999742120871635e-05, 'epoch': 0.48}
{'loss': 0.0635, 'learning_rate': 2.2684153327561756e-05, 'epoch': 0.55}
{'loss': 0.0546, 'learning_rate': 2.45261471016228e-05, 'epoch': 0.51}
{'loss': 0.0454, 'learning_rate': 2.6147101622796514e-05, 'epoch': 0.48}
{'loss': 0.0551, 'learning_rate': 2.5124795078192637e-05, 'epoch': 0.5}
{'loss': 0.0651, 'learning_rate': 2.5926062369909193e-05, 'epoch': 0.48}
{'loss': 0.0754, 'learning_rate': 2.2610473576599314e-05, 'epoch': 0.55}
{'loss': 0.0612, 'learning_rate': 2.4452467350660357e-05, 'epoch': 0.51}
{'loss': 0.0348, 'learning_rate': 2.6073421871834076e-05, 'epoch': 0.48}
{'loss': 0.0536, 'learning_rate': 2.5051115327230196e-05, 'epoch': 0.5}
{'loss': 0.0866, 'learning_rate': 2.5852382618946752e-05, 'epoch': 0.48}
{'loss': 0.0696, 'learning_rate': 2.2536793825636873e-05, 'epoch': 0.55}
{'loss': 0.0979, 'learning_rate': 2.4378787599697916e-05, 'epoch': 0.51}
{'loss': 0.0864, 'learning_rate': 2.5999742120871635e-05, 'epoch': 0.48}
{'loss': 0.0618, 'learning_rate': 2.4977435576267754e-05, 'epoch': 0.5}
{'loss': 0.0621, 'learning_rate': 2.577870286798431e-05, 'epoch': 0.48}
{'loss': 0.0336, 'learning_rate': 2.2463114074674428e-05, 'epoch': 0.55}
{'loss': 0.0408, 'learning_rate': 2.430510784873547e-05, 'epoch': 0.51}
{'loss': 0.0512, 'learning_rate': 2.5926062369909193e-05, 'epoch': 0.48}
{'loss': 0.0459, 'learning_rate': 2.5705023117021866e-05, 'epoch': 0.49}
{'loss': 0.0469, 'learning_rate': 2.238943432371199e-05, 'epoch': 0.55}
{'loss': 0.0348, 'learning_rate': 2.4903755825305313e-05, 'epoch': 0.5}
{'loss': 0.0429, 'learning_rate': 2.4231428097773033e-05, 'epoch': 0.52}
{'loss': 0.0635, 'learning_rate': 2.5852382618946752e-05, 'epoch': 0.48}
{'loss': 0.0453, 'learning_rate': 2.231575457274955e-05, 'epoch': 0.55}
{'loss': 0.0458, 'learning_rate': 2.5631343366059424e-05, 'epoch': 0.49}
{'loss': 0.1045, 'learning_rate': 2.483007607434287e-05, 'epoch': 0.5}
{'loss': 0.0524, 'learning_rate': 2.415774834681059e-05, 'epoch': 0.52}
{'loss': 0.0302, 'learning_rate': 2.577870286798431e-05, 'epoch': 0.48}
{'loss': 0.0397, 'learning_rate': 2.2242074821787104e-05, 'epoch': 0.56}
{'loss': 0.0546, 'learning_rate': 2.5557663615096983e-05, 'epoch': 0.49}
{'loss': 0.0981, 'learning_rate': 2.475639632338043e-05, 'epoch': 0.51}
{'loss': 0.0558, 'learning_rate': 2.408406859584815e-05, 'epoch': 0.52}
{'loss': 0.1002, 'learning_rate': 2.2168395070824662e-05, 'epoch': 0.56}
{'loss': 0.0226, 'learning_rate': 2.548398386413454e-05, 'epoch': 0.49}
{'loss': 0.036, 'learning_rate': 2.5705023117021866e-05, 'epoch': 0.49}
{'loss': 0.0604, 'learning_rate': 2.468271657241799e-05, 'epoch': 0.51}
{'eval_loss': 0.047125522047281265, 'eval_accuracy': 0.9842352453638153, 'eval_f1': 0.984212073527123, 'eval_precision': 0.9890866119838733, 'eval_recall': 0.9793853460439825, 'eval_runtime': 342.0989, 'eval_samples_per_second': 169.29, 'eval_steps_per_second': 21.163, 'epoch': 1.0}
{'train_runtime': 8231.1216, 'train_samples_per_second': 21.108, 'train_steps_per_second': 0.33, 'train_loss': 0.07169914313961938, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0445, 'learning_rate': 2.4010388844885705e-05, 'epoch': 0.52}
{'loss': 0.0517, 'learning_rate': 2.209471531986222e-05, 'epoch': 0.56}
{'loss': 0.0681, 'learning_rate': 2.54103041131721e-05, 'epoch': 0.49}
{'loss': 0.1106, 'learning_rate': 2.5631343366059424e-05, 'epoch': 0.49}
{'loss': 0.0332, 'learning_rate': 2.4609036821455547e-05, 'epoch': 0.51}
{'loss': 0.0836, 'learning_rate': 2.3936709093923264e-05, 'epoch': 0.52}
{'loss': 0.0475, 'learning_rate': 2.202103556889978e-05, 'epoch': 0.56}
{'loss': 0.0714, 'learning_rate': 2.533662436220966e-05, 'epoch': 0.49}
{'loss': 0.06, 'learning_rate': 2.4535357070493102e-05, 'epoch': 0.51}
{'loss': 0.0865, 'learning_rate': 2.5557663615096983e-05, 'epoch': 0.49}
{'loss': 0.0777, 'learning_rate': 2.3863029342960826e-05, 'epoch': 0.52}
{'loss': 0.0399, 'learning_rate': 2.1947355817937338e-05, 'epoch': 0.56}
{'loss': 0.0202, 'learning_rate': 2.526294461124722e-05, 'epoch': 0.5}
{'loss': 0.0529, 'learning_rate': 2.4461677319530664e-05, 'epoch': 0.51}
{'loss': 0.0726, 'learning_rate': 2.548398386413454e-05, 'epoch': 0.49}
{'loss': 0.0504, 'learning_rate': 2.378934959199838e-05, 'epoch': 0.52}
{'loss': 0.0518, 'learning_rate': 2.1873676066974896e-05, 'epoch': 0.56}
{'loss': 0.0638, 'learning_rate': 2.5189264860284772e-05, 'epoch': 0.5}
{'loss': 0.0278, 'learning_rate': 2.4387997568568222e-05, 'epoch': 0.51}
{'loss': 0.0782, 'learning_rate': 2.54103041131721e-05, 'epoch': 0.49}
{'loss': 0.0558, 'learning_rate': 2.371566984103594e-05, 'epoch': 0.53}
{'loss': 0.0493, 'learning_rate': 2.1799996316012455e-05, 'epoch': 0.56}
{'loss': 0.0491, 'learning_rate': 2.511558510932233e-05, 'epoch': 0.5}
{'loss': 0.0564, 'learning_rate': 2.431431781760578e-05, 'epoch': 0.51}
{'loss': 0.0382, 'learning_rate': 2.533662436220966e-05, 'epoch': 0.49}
{'loss': 0.0596, 'learning_rate': 2.172631656505001e-05, 'epoch': 0.57}
{'loss': 0.0761, 'learning_rate': 2.3641990090073498e-05, 'epoch': 0.53}
{'loss': 0.0458, 'learning_rate': 2.504190535835989e-05, 'epoch': 0.5}
{'loss': 0.0854, 'learning_rate': 2.4240638066643336e-05, 'epoch': 0.52}
{'loss': 0.0617, 'learning_rate': 2.1652636814087572e-05, 'epoch': 0.57}
{'loss': 0.0653, 'learning_rate': 2.526294461124722e-05, 'epoch': 0.5}
{'loss': 0.0734, 'learning_rate': 2.3568310339111057e-05, 'epoch': 0.53}
{'loss': 0.0355, 'learning_rate': 2.496822560739745e-05, 'epoch': 0.5}
{'loss': 0.0447, 'learning_rate': 2.4166958315680895e-05, 'epoch': 0.52}
{'loss': 0.0532, 'learning_rate': 2.157895706312513e-05, 'epoch': 0.57}
{'loss': 0.0415, 'learning_rate': 2.5189264860284772e-05, 'epoch': 0.5}
{'loss': 0.0325, 'learning_rate': 2.3494630588148615e-05, 'epoch': 0.53}
{'loss': 0.0626, 'learning_rate': 2.489454585643501e-05, 'epoch': 0.5}
{'loss': 0.0268, 'learning_rate': 2.4093278564718457e-05, 'epoch': 0.52}
{'loss': 0.0679, 'learning_rate': 2.1505277312162686e-05, 'epoch': 0.57}
{'loss': 0.0764, 'learning_rate': 2.511558510932233e-05, 'epoch': 0.5}
{'loss': 0.0396, 'learning_rate': 2.3420950837186174e-05, 'epoch': 0.53}
{'loss': 0.0604, 'learning_rate': 2.4820866105472565e-05, 'epoch': 0.5}
{'loss': 0.0893, 'learning_rate': 2.4019598813756012e-05, 'epoch': 0.52}
{'loss': 0.0764, 'learning_rate': 2.1431597561200244e-05, 'epoch': 0.57}
{'loss': 0.0859, 'learning_rate': 2.504190535835989e-05, 'epoch': 0.5}
{'loss': 0.06, 'learning_rate': 2.4747186354510123e-05, 'epoch': 0.51}
{'loss': 0.0785, 'learning_rate': 2.3347271086223732e-05, 'epoch': 0.53}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0697, 'learning_rate': 2.394591906279357e-05, 'epoch': 0.52}
{'loss': 0.0475, 'learning_rate': 2.1357917810237803e-05, 'epoch': 0.57}
{'loss': 0.0698, 'learning_rate': 2.496822560739745e-05, 'epoch': 0.5}
{'loss': 0.0637, 'learning_rate': 2.4673506603547682e-05, 'epoch': 0.51}
{'loss': 0.0655, 'learning_rate': 2.3273591335261287e-05, 'epoch': 0.53}
{'loss': 0.0452, 'learning_rate': 2.1284238059275365e-05, 'epoch': 0.57}
{'loss': 0.0261, 'learning_rate': 2.387223931183113e-05, 'epoch': 0.52}
{'loss': 0.0893, 'learning_rate': 2.489454585643501e-05, 'epoch': 0.5}
{'loss': 0.0668, 'learning_rate': 2.4599826852585244e-05, 'epoch': 0.51}
{'loss': 0.0591, 'learning_rate': 2.3199911584298846e-05, 'epoch': 0.54}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0728, 'learning_rate': 2.121055830831292e-05, 'epoch': 0.58}
{'loss': 0.0352, 'learning_rate': 2.3798559560868684e-05, 'epoch': 0.52}
{'loss': 0.0569, 'learning_rate': 2.4820866105472565e-05, 'epoch': 0.5}
{'loss': 0.0362, 'learning_rate': 2.45261471016228e-05, 'epoch': 0.51}
{'loss': 0.0239, 'learning_rate': 2.3126231833336408e-05, 'epoch': 0.54}
{'loss': 0.0684, 'learning_rate': 2.113687855735048e-05, 'epoch': 0.58}
{'loss': 0.0275, 'learning_rate': 2.3724879809906246e-05, 'epoch': 0.53}
{'loss': 0.037, 'learning_rate': 2.4452467350660357e-05, 'epoch': 0.51}
{'loss': 0.0612, 'learning_rate': 2.4747186354510123e-05, 'epoch': 0.51}
{'loss': 0.045, 'learning_rate': 2.3052552082373967e-05, 'epoch': 0.54}
{'loss': 0.0759, 'learning_rate': 2.1063198806388037e-05, 'epoch': 0.58}
{'loss': 0.06, 'learning_rate': 2.3651200058943805e-05, 'epoch': 0.53}
{'loss': 0.0192, 'learning_rate': 2.4378787599697916e-05, 'epoch': 0.51}
{'loss': 0.0355, 'learning_rate': 2.4673506603547682e-05, 'epoch': 0.51}
{'loss': 0.0632, 'learning_rate': 2.297887233141152e-05, 'epoch': 0.54}
{'loss': 0.0488, 'learning_rate': 2.0989519055425592e-05, 'epoch': 0.58}
{'loss': 0.124, 'learning_rate': 2.3577520307981363e-05, 'epoch': 0.53}
{'loss': 0.018, 'learning_rate': 2.430510784873547e-05, 'epoch': 0.51}
{'loss': 0.0377, 'learning_rate': 2.4599826852585244e-05, 'epoch': 0.51}
{'loss': 0.028, 'learning_rate': 2.290519258044908e-05, 'epoch': 0.54}
{'loss': 0.0494, 'learning_rate': 2.0915839304463154e-05, 'epoch': 0.58}
{'loss': 0.1146, 'learning_rate': 2.4231428097773033e-05, 'epoch': 0.52}
{'loss': 0.1308, 'learning_rate': 2.3503840557018918e-05, 'epoch': 0.53}
{'loss': 0.0255, 'learning_rate': 2.45261471016228e-05, 'epoch': 0.51}
{'loss': 0.0755, 'learning_rate': 2.283151282948664e-05, 'epoch': 0.54}
{'loss': 0.0436, 'learning_rate': 2.0842159553500713e-05, 'epoch': 0.58}
{'loss': 0.0382, 'learning_rate': 2.415774834681059e-05, 'epoch': 0.52}
{'loss': 0.0632, 'learning_rate': 2.3430160806056477e-05, 'epoch': 0.53}
{'loss': 0.0489, 'learning_rate': 2.4452467350660357e-05, 'epoch': 0.51}
{'loss': 0.0666, 'learning_rate': 2.2757833078524197e-05, 'epoch': 0.55}
{'loss': 0.0413, 'learning_rate': 2.076847980253827e-05, 'epoch': 0.58}
{'loss': 0.0812, 'learning_rate': 2.408406859584815e-05, 'epoch': 0.52}
{'loss': 0.0506, 'learning_rate': 2.335648105509404e-05, 'epoch': 0.53}
{'loss': 0.0732, 'learning_rate': 2.4378787599697916e-05, 'epoch': 0.51}
{'loss': 0.0795, 'learning_rate': 2.2684153327561756e-05, 'epoch': 0.55}
{'loss': 0.05, 'learning_rate': 2.0694800051575826e-05, 'epoch': 0.59}
{'loss': 0.0565, 'learning_rate': 2.4010388844885705e-05, 'epoch': 0.52}
{'loss': 0.0332, 'learning_rate': 2.3282801304131594e-05, 'epoch': 0.53}
{'loss': 0.069, 'learning_rate': 2.430510784873547e-05, 'epoch': 0.51}
{'loss': 0.045, 'learning_rate': 2.2610473576599314e-05, 'epoch': 0.55}
{'loss': 0.0433, 'learning_rate': 2.0621120300613385e-05, 'epoch': 0.59}
{'loss': 0.0532, 'learning_rate': 2.3936709093923264e-05, 'epoch': 0.52}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0851, 'learning_rate': 2.3209121553169152e-05, 'epoch': 0.54}
{'loss': 0.0827, 'learning_rate': 2.4231428097773033e-05, 'epoch': 0.52}
{'loss': 0.0465, 'learning_rate': 2.2536793825636873e-05, 'epoch': 0.55}
{'loss': 0.0542, 'learning_rate': 2.0547440549650947e-05, 'epoch': 0.59}
{'loss': 0.0373, 'learning_rate': 2.3863029342960826e-05, 'epoch': 0.52}
{'loss': 0.0728, 'learning_rate': 2.313544180220671e-05, 'epoch': 0.54}
{'loss': 0.0443, 'learning_rate': 2.415774834681059e-05, 'epoch': 0.52}
{'loss': 0.0633, 'learning_rate': 2.2463114074674428e-05, 'epoch': 0.55}
{'loss': 0.0691, 'learning_rate': 2.0473760798688502e-05, 'epoch': 0.59}
{'loss': 0.0524, 'learning_rate': 2.378934959199838e-05, 'epoch': 0.52}
{'loss': 0.0564, 'learning_rate': 2.306176205124427e-05, 'epoch': 0.54}
{'loss': 0.0454, 'learning_rate': 2.040008104772606e-05, 'epoch': 0.59}
{'loss': 0.0544, 'learning_rate': 2.408406859584815e-05, 'epoch': 0.52}
{'loss': 0.0556, 'learning_rate': 2.238943432371199e-05, 'epoch': 0.55}
{'loss': 0.0347, 'learning_rate': 2.371566984103594e-05, 'epoch': 0.53}
{'loss': 0.0823, 'learning_rate': 2.032640129676362e-05, 'epoch': 0.59}
{'loss': 0.0369, 'learning_rate': 2.2988082300281828e-05, 'epoch': 0.54}
{'loss': 0.045, 'learning_rate': 2.4010388844885705e-05, 'epoch': 0.52}
{'loss': 0.0112, 'learning_rate': 2.231575457274955e-05, 'epoch': 0.55}
{'loss': 0.1066, 'learning_rate': 2.3641990090073498e-05, 'epoch': 0.53}
{'loss': 0.0386, 'learning_rate': 2.0252721545801178e-05, 'epoch': 0.6}
{'loss': 0.0719, 'learning_rate': 2.2914402549319387e-05, 'epoch': 0.54}
{'loss': 0.0748, 'learning_rate': 2.3936709093923264e-05, 'epoch': 0.52}
{'loss': 0.1066, 'learning_rate': 2.2242074821787104e-05, 'epoch': 0.56}
{'loss': 0.0711, 'learning_rate': 2.3568310339111057e-05, 'epoch': 0.53}
{'loss': 0.0429, 'learning_rate': 2.0179041794838736e-05, 'epoch': 0.6}
{'loss': 0.0336, 'learning_rate': 2.3863029342960826e-05, 'epoch': 0.52}
{'loss': 0.0513, 'learning_rate': 2.2840722798356945e-05, 'epoch': 0.54}
{'loss': 0.0185, 'learning_rate': 2.2168395070824662e-05, 'epoch': 0.56}
{'loss': 0.0272, 'learning_rate': 2.3494630588148615e-05, 'epoch': 0.53}
{'loss': 0.0843, 'learning_rate': 2.0105362043876295e-05, 'epoch': 0.6}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0674, 'learning_rate': 2.378934959199838e-05, 'epoch': 0.52}
{'loss': 0.0529, 'learning_rate': 2.27670430473945e-05, 'epoch': 0.55}
{'loss': 0.0255, 'learning_rate': 2.209471531986222e-05, 'epoch': 0.56}
{'loss': 0.0465, 'learning_rate': 2.3420950837186174e-05, 'epoch': 0.53}
{'loss': 0.0478, 'learning_rate': 2.0031682292913853e-05, 'epoch': 0.6}
{'loss': 0.0339, 'learning_rate': 2.371566984103594e-05, 'epoch': 0.53}
{'loss': 0.0291, 'learning_rate': 2.202103556889978e-05, 'epoch': 0.56}
{'loss': 0.0409, 'learning_rate': 2.269336329643206e-05, 'epoch': 0.55}
{'loss': 0.0581, 'learning_rate': 2.3347271086223732e-05, 'epoch': 0.53}
{'loss': 0.0606, 'learning_rate': 1.995800254195141e-05, 'epoch': 0.6}
{'loss': 0.0497, 'learning_rate': 2.3641990090073498e-05, 'epoch': 0.53}
{'loss': 0.0392, 'learning_rate': 2.261968354546962e-05, 'epoch': 0.55}
{'loss': 0.0502, 'learning_rate': 2.1947355817937338e-05, 'epoch': 0.56}
{'loss': 0.0676, 'learning_rate': 2.3273591335261287e-05, 'epoch': 0.53}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0555, 'learning_rate': 1.9884322790988967e-05, 'epoch': 0.6}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0337, 'learning_rate': 2.254600379450718e-05, 'epoch': 0.55}
{'loss': 0.0498, 'learning_rate': 2.3568310339111057e-05, 'epoch': 0.53}
{'loss': 0.0376, 'learning_rate': 2.1873676066974896e-05, 'epoch': 0.56}
{'loss': 0.0499, 'learning_rate': 2.3199911584298846e-05, 'epoch': 0.54}
{'loss': 0.0215, 'learning_rate': 1.981064304002653e-05, 'epoch': 0.6}
{'loss': 0.039, 'learning_rate': 2.1799996316012455e-05, 'epoch': 0.56}
{'loss': 0.0862, 'learning_rate': 2.2472324043544735e-05, 'epoch': 0.55}
{'loss': 0.0508, 'learning_rate': 2.3494630588148615e-05, 'epoch': 0.53}
{'loss': 0.0484, 'learning_rate': 2.3126231833336408e-05, 'epoch': 0.54}
{'loss': 0.0517, 'learning_rate': 1.9736963289064088e-05, 'epoch': 0.61}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0897, 'learning_rate': 2.172631656505001e-05, 'epoch': 0.57}
{'loss': 0.0611, 'learning_rate': 2.3420950837186174e-05, 'epoch': 0.53}
{'loss': 0.0622, 'learning_rate': 2.2398644292582293e-05, 'epoch': 0.55}
{'loss': 0.075, 'learning_rate': 2.3052552082373967e-05, 'epoch': 0.54}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0374, 'learning_rate': 1.9663283538101643e-05, 'epoch': 0.61}
{'loss': 0.0679, 'learning_rate': 2.3347271086223732e-05, 'epoch': 0.53}
{'loss': 0.0605, 'learning_rate': 2.1652636814087572e-05, 'epoch': 0.57}
{'loss': 0.052, 'learning_rate': 2.232496454161985e-05, 'epoch': 0.55}
{'loss': 0.0715, 'learning_rate': 2.297887233141152e-05, 'epoch': 0.54}
{'loss': 0.0553, 'learning_rate': 1.95896037871392e-05, 'epoch': 0.61}
{'loss': 0.0449, 'learning_rate': 2.3273591335261287e-05, 'epoch': 0.53}
{'loss': 0.0221, 'learning_rate': 2.290519258044908e-05, 'epoch': 0.54}
{'loss': 0.0752, 'learning_rate': 2.157895706312513e-05, 'epoch': 0.57}
{'loss': 0.058, 'learning_rate': 2.225128479065741e-05, 'epoch': 0.56}
{'loss': 0.0806, 'learning_rate': 1.951592403617676e-05, 'epoch': 0.61}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.069, 'learning_rate': 2.283151282948664e-05, 'epoch': 0.54}
{'loss': 0.0318, 'learning_rate': 2.3199911584298846e-05, 'epoch': 0.54}
{'loss': 0.0681, 'learning_rate': 2.1505277312162686e-05, 'epoch': 0.57}
{'loss': 0.0589, 'learning_rate': 2.217760503969497e-05, 'epoch': 0.56}
{'loss': 0.059, 'learning_rate': 1.944224428521432e-05, 'epoch': 0.61}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0717, 'learning_rate': 2.2757833078524197e-05, 'epoch': 0.55}
{'loss': 0.0564, 'learning_rate': 2.313544180220671e-05, 'epoch': 0.54}
{'loss': 0.054, 'learning_rate': 2.1431597561200244e-05, 'epoch': 0.57}
{'loss': 0.0449, 'learning_rate': 2.2103925288732527e-05, 'epoch': 0.56}
{'loss': 0.0386, 'learning_rate': 1.9368564534251877e-05, 'epoch': 0.61}
{'loss': 0.0507, 'learning_rate': 2.2684153327561756e-05, 'epoch': 0.55}
{'loss': 0.0577, 'learning_rate': 2.306176205124427e-05, 'epoch': 0.54}
{'loss': 0.0366, 'learning_rate': 1.9294884783289435e-05, 'epoch': 0.61}
{'loss': 0.0602, 'learning_rate': 2.1357917810237803e-05, 'epoch': 0.57}
{'loss': 0.0602, 'learning_rate': 2.2030245537770086e-05, 'epoch': 0.56}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0493, 'learning_rate': 2.2610473576599314e-05, 'epoch': 0.55}
{'loss': 0.041, 'learning_rate': 1.9221205032326994e-05, 'epoch': 0.62}
{'loss': 0.0345, 'learning_rate': 2.2988082300281828e-05, 'epoch': 0.54}
{'loss': 0.0437, 'learning_rate': 2.1284238059275365e-05, 'epoch': 0.57}
{'loss': 0.076, 'learning_rate': 2.195656578680764e-05, 'epoch': 0.56}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0283, 'learning_rate': 2.2536793825636873e-05, 'epoch': 0.55}
{'loss': 0.059, 'learning_rate': 1.914752528136455e-05, 'epoch': 0.62}
{'loss': 0.0588, 'learning_rate': 2.2914402549319387e-05, 'epoch': 0.54}
{'loss': 0.0349, 'learning_rate': 2.121055830831292e-05, 'epoch': 0.58}
{'loss': 0.0365, 'learning_rate': 2.1882886035845203e-05, 'epoch': 0.56}
{'loss': 0.0681, 'learning_rate': 2.2463114074674428e-05, 'epoch': 0.55}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0509, 'learning_rate': 1.907384553040211e-05, 'epoch': 0.62}
{'loss': 0.0585, 'learning_rate': 2.2840722798356945e-05, 'epoch': 0.54}
{'loss': 0.0489, 'learning_rate': 2.113687855735048e-05, 'epoch': 0.58}
{'loss': 0.0569, 'learning_rate': 2.180920628488276e-05, 'epoch': 0.56}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0281, 'learning_rate': 2.238943432371199e-05, 'epoch': 0.55}
{'loss': 0.0501, 'learning_rate': 1.900016577943967e-05, 'epoch': 0.62}
{'loss': 0.0353, 'learning_rate': 2.27670430473945e-05, 'epoch': 0.55}
{'loss': 0.0604, 'learning_rate': 2.1063198806388037e-05, 'epoch': 0.58}
{'loss': 0.0424, 'learning_rate': 2.1735526533920317e-05, 'epoch': 0.57}
{'loss': 0.0546, 'learning_rate': 2.231575457274955e-05, 'epoch': 0.55}
{'loss': 0.0596, 'learning_rate': 1.8926486028477225e-05, 'epoch': 0.62}
{'loss': 0.0517, 'learning_rate': 2.269336329643206e-05, 'epoch': 0.55}
{'loss': 0.0474, 'learning_rate': 2.0989519055425592e-05, 'epoch': 0.58}
{'loss': 0.0672, 'learning_rate': 2.1661846782957875e-05, 'epoch': 0.57}
{'loss': 0.0399, 'learning_rate': 2.2242074821787104e-05, 'epoch': 0.56}
{'loss': 0.0269, 'learning_rate': 1.8852806277514783e-05, 'epoch': 0.62}
{'loss': 0.055, 'learning_rate': 2.261968354546962e-05, 'epoch': 0.55}
{'loss': 0.0358, 'learning_rate': 2.0915839304463154e-05, 'epoch': 0.58}
{'loss': 0.0562, 'learning_rate': 2.1588167031995434e-05, 'epoch': 0.57}
{'loss': 0.0607, 'learning_rate': 2.2168395070824662e-05, 'epoch': 0.56}
{'loss': 0.0743, 'learning_rate': 1.8779126526552342e-05, 'epoch': 0.62}
{'loss': 0.0764, 'learning_rate': 2.254600379450718e-05, 'epoch': 0.55}
{'loss': 0.063, 'learning_rate': 2.0842159553500713e-05, 'epoch': 0.58}
{'loss': 0.0683, 'learning_rate': 2.1514487281032992e-05, 'epoch': 0.57}
{'loss': 0.0543, 'learning_rate': 2.209471531986222e-05, 'epoch': 0.56}
{'loss': 0.0274, 'learning_rate': 1.87054467755899e-05, 'epoch': 0.63}
{'loss': 0.0392, 'learning_rate': 2.2472324043544735e-05, 'epoch': 0.55}
{'loss': 0.0574, 'learning_rate': 2.076847980253827e-05, 'epoch': 0.58}
{'loss': 0.0653, 'learning_rate': 2.144080753007055e-05, 'epoch': 0.57}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0376, 'learning_rate': 2.202103556889978e-05, 'epoch': 0.56}
{'loss': 0.0308, 'learning_rate': 1.863176702462746e-05, 'epoch': 0.63}
{'loss': 0.0371, 'learning_rate': 2.2398644292582293e-05, 'epoch': 0.55}
{'loss': 0.0365, 'learning_rate': 2.0694800051575826e-05, 'epoch': 0.59}
{'loss': 0.0522, 'learning_rate': 2.136712777910811e-05, 'epoch': 0.57}
{'loss': 0.0327, 'learning_rate': 2.1947355817937338e-05, 'epoch': 0.56}
{'loss': 0.0797, 'learning_rate': 1.8558087273665018e-05, 'epoch': 0.63}
{'loss': 0.0579, 'learning_rate': 2.232496454161985e-05, 'epoch': 0.55}
{'loss': 0.0967, 'learning_rate': 2.0621120300613385e-05, 'epoch': 0.59}
{'loss': 0.0516, 'learning_rate': 2.1873676066974896e-05, 'epoch': 0.56}
{'loss': 0.0305, 'learning_rate': 1.8484407522702576e-05, 'epoch': 0.63}
{'loss': 0.0502, 'learning_rate': 2.1293448028145668e-05, 'epoch': 0.57}
{'loss': 0.0479, 'learning_rate': 2.0547440549650947e-05, 'epoch': 0.59}
{'loss': 0.0534, 'learning_rate': 2.225128479065741e-05, 'epoch': 0.56}
{'loss': 0.0534, 'learning_rate': 2.1799996316012455e-05, 'epoch': 0.56}
{'loss': 0.0697, 'learning_rate': 1.841072777174013e-05, 'epoch': 0.63}
{'loss': 0.0603, 'learning_rate': 2.1219768277183223e-05, 'epoch': 0.58}
{'loss': 0.0469, 'learning_rate': 2.0473760798688502e-05, 'epoch': 0.59}
{'loss': 0.053, 'learning_rate': 2.217760503969497e-05, 'epoch': 0.56}
{'loss': 0.0574, 'learning_rate': 1.8337048020777693e-05, 'epoch': 0.63}
{'loss': 0.053, 'learning_rate': 2.172631656505001e-05, 'epoch': 0.57}
{'loss': 0.0575, 'learning_rate': 2.1146088526220785e-05, 'epoch': 0.58}
{'loss': 0.0263, 'learning_rate': 2.040008104772606e-05, 'epoch': 0.59}
{'loss': 0.0413, 'learning_rate': 2.2103925288732527e-05, 'epoch': 0.56}
{'loss': 0.0521, 'learning_rate': 2.1652636814087572e-05, 'epoch': 0.57}
{'loss': 0.066, 'learning_rate': 1.8263368269815252e-05, 'epoch': 0.64}
{'loss': 0.0438, 'learning_rate': 2.1072408775258344e-05, 'epoch': 0.58}
{'loss': 0.1069, 'learning_rate': 2.032640129676362e-05, 'epoch': 0.59}
{'loss': 0.0395, 'learning_rate': 2.2030245537770086e-05, 'epoch': 0.56}
{'loss': 0.061, 'learning_rate': 1.8189688518852807e-05, 'epoch': 0.64}
{'loss': 0.0504, 'learning_rate': 2.157895706312513e-05, 'epoch': 0.57}
{'loss': 0.0524, 'learning_rate': 2.09987290242959e-05, 'epoch': 0.58}
{'loss': 0.073, 'learning_rate': 2.0252721545801178e-05, 'epoch': 0.6}
{'loss': 0.0232, 'learning_rate': 2.195656578680764e-05, 'epoch': 0.56}
{'loss': 0.0391, 'learning_rate': 1.8116008767890365e-05, 'epoch': 0.64}
{'loss': 0.0539, 'learning_rate': 2.1505277312162686e-05, 'epoch': 0.57}
{'loss': 0.0705, 'learning_rate': 2.0925049273333457e-05, 'epoch': 0.58}
{'loss': 0.0387, 'learning_rate': 2.0179041794838736e-05, 'epoch': 0.6}
{'loss': 0.0594, 'learning_rate': 1.8042329016927924e-05, 'epoch': 0.64}
{'loss': 0.0752, 'learning_rate': 2.1882886035845203e-05, 'epoch': 0.56}
{'loss': 0.069, 'learning_rate': 2.1431597561200244e-05, 'epoch': 0.57}
{'loss': 0.0333, 'learning_rate': 2.0851369522371016e-05, 'epoch': 0.58}
{'loss': 0.0545, 'learning_rate': 1.7968649265965486e-05, 'epoch': 0.64}
{'loss': 0.073, 'learning_rate': 2.0105362043876295e-05, 'epoch': 0.6}
{'loss': 0.0651, 'learning_rate': 2.180920628488276e-05, 'epoch': 0.56}
{'loss': 0.0204, 'learning_rate': 2.1357917810237803e-05, 'epoch': 0.57}
{'loss': 0.062, 'learning_rate': 2.0777689771408578e-05, 'epoch': 0.58}
{'loss': 0.053, 'learning_rate': 1.789496951500304e-05, 'epoch': 0.64}
{'loss': 0.0477, 'learning_rate': 2.1284238059275365e-05, 'epoch': 0.57}
{'loss': 0.0494, 'learning_rate': 2.0031682292913853e-05, 'epoch': 0.6}
{'loss': 0.0411, 'learning_rate': 2.1735526533920317e-05, 'epoch': 0.57}
{'loss': 0.0652, 'learning_rate': 2.0704010020446133e-05, 'epoch': 0.59}
{'loss': 0.0529, 'learning_rate': 1.78212897640406e-05, 'epoch': 0.64}
{'loss': 0.0365, 'learning_rate': 2.121055830831292e-05, 'epoch': 0.58}
{'loss': 0.0292, 'learning_rate': 1.995800254195141e-05, 'epoch': 0.6}
{'loss': 0.0453, 'learning_rate': 2.1661846782957875e-05, 'epoch': 0.57}
{'loss': 0.0607, 'learning_rate': 2.063033026948369e-05, 'epoch': 0.59}
{'loss': 0.0769, 'learning_rate': 1.7747610013078158e-05, 'epoch': 0.65}
{'loss': 0.05, 'learning_rate': 2.113687855735048e-05, 'epoch': 0.58}
{'loss': 0.0501, 'learning_rate': 2.1588167031995434e-05, 'epoch': 0.57}
{'loss': 0.0626, 'learning_rate': 1.9884322790988967e-05, 'epoch': 0.6}
{'loss': 0.0635, 'learning_rate': 2.055665051852125e-05, 'epoch': 0.59}
{'loss': 0.0724, 'learning_rate': 1.7673930262115713e-05, 'epoch': 0.65}
{'loss': 0.0582, 'learning_rate': 2.1063198806388037e-05, 'epoch': 0.58}
{'loss': 0.0771, 'learning_rate': 2.1514487281032992e-05, 'epoch': 0.57}
{'loss': 0.0443, 'learning_rate': 1.981064304002653e-05, 'epoch': 0.6}
{'loss': 0.0448, 'learning_rate': 2.0482970767558805e-05, 'epoch': 0.59}
{'loss': 0.0347, 'learning_rate': 1.7600250511153275e-05, 'epoch': 0.65}
{'loss': 0.0555, 'learning_rate': 2.0989519055425592e-05, 'epoch': 0.58}
{'loss': 0.0275, 'learning_rate': 1.9736963289064088e-05, 'epoch': 0.61}
{'loss': 0.0582, 'learning_rate': 2.144080753007055e-05, 'epoch': 0.57}
{'loss': 0.0592, 'learning_rate': 1.7526570760190834e-05, 'epoch': 0.65}
{'loss': 0.0726, 'learning_rate': 2.0409291016596367e-05, 'epoch': 0.59}
{'loss': 0.0489, 'learning_rate': 2.0915839304463154e-05, 'epoch': 0.58}
{'loss': 0.0649, 'learning_rate': 2.136712777910811e-05, 'epoch': 0.57}
{'loss': 0.0511, 'learning_rate': 1.9663283538101643e-05, 'epoch': 0.61}
{'loss': 0.0587, 'learning_rate': 1.7452891009228392e-05, 'epoch': 0.65}
{'loss': 0.066, 'learning_rate': 2.0335611265633926e-05, 'epoch': 0.59}
{'loss': 0.0671, 'learning_rate': 2.0842159553500713e-05, 'epoch': 0.58}
{'loss': 0.0462, 'learning_rate': 2.1293448028145668e-05, 'epoch': 0.57}
{'loss': 0.0345, 'learning_rate': 1.95896037871392e-05, 'epoch': 0.61}
{'loss': 0.0387, 'learning_rate': 1.7379211258265948e-05, 'epoch': 0.65}
{'loss': 0.0713, 'learning_rate': 2.0261931514671484e-05, 'epoch': 0.6}
{'loss': 0.0403, 'learning_rate': 2.076847980253827e-05, 'epoch': 0.58}
{'loss': 0.037, 'learning_rate': 2.1219768277183223e-05, 'epoch': 0.58}
{'loss': 0.0497, 'learning_rate': 1.951592403617676e-05, 'epoch': 0.61}
{'loss': 0.0181, 'learning_rate': 1.7305531507303506e-05, 'epoch': 0.65}
{'loss': 0.0469, 'learning_rate': 2.0694800051575826e-05, 'epoch': 0.59}
{'loss': 0.0561, 'learning_rate': 2.018825176370904e-05, 'epoch': 0.6}
{'loss': 0.0427, 'learning_rate': 2.1146088526220785e-05, 'epoch': 0.58}
{'loss': 0.0406, 'learning_rate': 1.944224428521432e-05, 'epoch': 0.61}
{'loss': 0.0495, 'learning_rate': 1.7231851756341068e-05, 'epoch': 0.66}
{'loss': 0.0943, 'learning_rate': 2.0621120300613385e-05, 'epoch': 0.59}
{'loss': 0.0524, 'learning_rate': 2.0114572012746598e-05, 'epoch': 0.6}
{'loss': 0.0447, 'learning_rate': 2.1072408775258344e-05, 'epoch': 0.58}
{'loss': 0.0878, 'learning_rate': 1.9368564534251877e-05, 'epoch': 0.61}
{'loss': 0.057, 'learning_rate': 1.7158172005378623e-05, 'epoch': 0.66}
{'loss': 0.0393, 'learning_rate': 2.0547440549650947e-05, 'epoch': 0.59}
{'loss': 0.0501, 'learning_rate': 2.004089226178416e-05, 'epoch': 0.6}
{'loss': 0.037, 'learning_rate': 2.09987290242959e-05, 'epoch': 0.58}
{'loss': 0.0271, 'learning_rate': 1.9294884783289435e-05, 'epoch': 0.61}
{'loss': 0.0573, 'learning_rate': 1.7084492254416182e-05, 'epoch': 0.66}
{'loss': 0.0468, 'learning_rate': 2.0473760798688502e-05, 'epoch': 0.59}
{'loss': 0.0451, 'learning_rate': 1.9967212510821715e-05, 'epoch': 0.6}
{'loss': 0.0781, 'learning_rate': 2.0925049273333457e-05, 'epoch': 0.58}
{'loss': 0.0425, 'learning_rate': 1.9221205032326994e-05, 'epoch': 0.62}
{'loss': 0.0209, 'learning_rate': 1.701081250345374e-05, 'epoch': 0.66}
{'loss': 0.0581, 'learning_rate': 2.040008104772606e-05, 'epoch': 0.59}
{'loss': 0.0805, 'learning_rate': 1.9893532759859274e-05, 'epoch': 0.6}
{'loss': 0.0421, 'learning_rate': 2.0851369522371016e-05, 'epoch': 0.58}
{'loss': 0.0279, 'learning_rate': 1.914752528136455e-05, 'epoch': 0.62}
{'loss': 0.0817, 'learning_rate': 1.69371327524913e-05, 'epoch': 0.66}
{'loss': 0.0667, 'learning_rate': 2.032640129676362e-05, 'epoch': 0.59}
{'loss': 0.0574, 'learning_rate': 1.9819853008896832e-05, 'epoch': 0.6}
{'loss': 0.066, 'learning_rate': 2.0777689771408578e-05, 'epoch': 0.58}
{'loss': 0.022, 'learning_rate': 1.907384553040211e-05, 'epoch': 0.62}
{'loss': 0.0377, 'learning_rate': 1.6863453001528857e-05, 'epoch': 0.66}
{'loss': 0.0513, 'learning_rate': 2.0252721545801178e-05, 'epoch': 0.6}
{'loss': 0.0519, 'learning_rate': 1.974617325793439e-05, 'epoch': 0.61}
{'loss': 0.0656, 'learning_rate': 2.0704010020446133e-05, 'epoch': 0.59}
{'loss': 0.0564, 'learning_rate': 1.900016577943967e-05, 'epoch': 0.62}
{'loss': 0.0788, 'learning_rate': 1.6789773250566416e-05, 'epoch': 0.66}
{'loss': 0.0765, 'learning_rate': 2.0179041794838736e-05, 'epoch': 0.6}
{'loss': 0.045, 'learning_rate': 1.967249350697195e-05, 'epoch': 0.61}
{'loss': 0.0587, 'learning_rate': 2.063033026948369e-05, 'epoch': 0.59}
{'loss': 0.0981, 'learning_rate': 1.8926486028477225e-05, 'epoch': 0.62}
{'loss': 0.0487, 'learning_rate': 1.6716093499603974e-05, 'epoch': 0.67}
{'loss': 0.03, 'learning_rate': 2.0105362043876295e-05, 'epoch': 0.6}
{'loss': 0.0593, 'learning_rate': 1.9598813756009508e-05, 'epoch': 0.61}
{'loss': 0.0477, 'learning_rate': 2.055665051852125e-05, 'epoch': 0.59}
{'loss': 0.0535, 'learning_rate': 1.664241374864153e-05, 'epoch': 0.67}
{'loss': 0.0536, 'learning_rate': 1.8852806277514783e-05, 'epoch': 0.62}
{'loss': 0.0648, 'learning_rate': 2.0031682292913853e-05, 'epoch': 0.6}
{'loss': 0.0522, 'learning_rate': 1.9525134005047066e-05, 'epoch': 0.61}
{'loss': 0.0649, 'learning_rate': 1.6568733997679088e-05, 'epoch': 0.67}
{'loss': 0.0367, 'learning_rate': 2.0482970767558805e-05, 'epoch': 0.59}
{'loss': 0.0633, 'learning_rate': 1.8779126526552342e-05, 'epoch': 0.62}
{'loss': 0.039, 'learning_rate': 1.995800254195141e-05, 'epoch': 0.6}
{'loss': 0.0454, 'learning_rate': 1.945145425408462e-05, 'epoch': 0.61}
{'loss': 0.0295, 'learning_rate': 1.649505424671665e-05, 'epoch': 0.67}
{'loss': 0.0367, 'learning_rate': 2.0409291016596367e-05, 'epoch': 0.59}
{'loss': 0.0623, 'learning_rate': 1.87054467755899e-05, 'epoch': 0.63}
{'loss': 0.0631, 'learning_rate': 1.9884322790988967e-05, 'epoch': 0.6}
{'loss': 0.0391, 'learning_rate': 1.937777450312218e-05, 'epoch': 0.61}
{'loss': 0.0456, 'learning_rate': 1.642137449575421e-05, 'epoch': 0.67}
{'loss': 0.0337, 'learning_rate': 2.0335611265633926e-05, 'epoch': 0.59}
{'loss': 0.0442, 'learning_rate': 1.863176702462746e-05, 'epoch': 0.63}
{'loss': 0.0608, 'learning_rate': 1.981064304002653e-05, 'epoch': 0.6}
{'loss': 0.0454, 'learning_rate': 1.6347694744791764e-05, 'epoch': 0.67}
{'loss': 0.0591, 'learning_rate': 1.9304094752159742e-05, 'epoch': 0.61}
{'loss': 0.0644, 'learning_rate': 2.0261931514671484e-05, 'epoch': 0.6}
{'loss': 0.067, 'learning_rate': 1.8558087273665018e-05, 'epoch': 0.63}
{'loss': 0.0694, 'learning_rate': 1.9736963289064088e-05, 'epoch': 0.61}
{'loss': 0.0336, 'learning_rate': 1.6274014993829322e-05, 'epoch': 0.67}
{'loss': 0.0407, 'learning_rate': 1.92304150011973e-05, 'epoch': 0.62}
{'loss': 0.0616, 'learning_rate': 2.018825176370904e-05, 'epoch': 0.6}
{'loss': 0.0382, 'learning_rate': 1.8484407522702576e-05, 'epoch': 0.63}
{'loss': 0.0482, 'learning_rate': 1.9663283538101643e-05, 'epoch': 0.61}
{'loss': 0.0656, 'learning_rate': 1.620033524286688e-05, 'epoch': 0.68}
{'loss': 0.0439, 'learning_rate': 2.0114572012746598e-05, 'epoch': 0.6}
{'loss': 0.074, 'learning_rate': 1.841072777174013e-05, 'epoch': 0.63}
{'loss': 0.042, 'learning_rate': 1.9156735250234856e-05, 'epoch': 0.62}
{'loss': 0.0396, 'learning_rate': 1.95896037871392e-05, 'epoch': 0.61}
{'loss': 0.0366, 'learning_rate': 1.612665549190444e-05, 'epoch': 0.68}
{'loss': 0.0763, 'learning_rate': 2.004089226178416e-05, 'epoch': 0.6}
{'loss': 0.0384, 'learning_rate': 1.951592403617676e-05, 'epoch': 0.61}
{'loss': 0.0574, 'learning_rate': 1.8337048020777693e-05, 'epoch': 0.63}
{'loss': 0.0336, 'learning_rate': 1.9083055499272414e-05, 'epoch': 0.62}
{'loss': 0.0356, 'learning_rate': 1.6052975740941998e-05, 'epoch': 0.68}
{'loss': 0.0847, 'learning_rate': 1.9967212510821715e-05, 'epoch': 0.6}
{'loss': 0.0605, 'learning_rate': 1.944224428521432e-05, 'epoch': 0.61}
{'loss': 0.047, 'learning_rate': 1.8263368269815252e-05, 'epoch': 0.64}
{'loss': 0.0539, 'learning_rate': 1.9009375748309973e-05, 'epoch': 0.62}
{'loss': 0.0558, 'learning_rate': 1.5979295989979557e-05, 'epoch': 0.68}
{'loss': 0.0688, 'learning_rate': 1.9893532759859274e-05, 'epoch': 0.6}
{'loss': 0.0422, 'learning_rate': 1.9368564534251877e-05, 'epoch': 0.61}
{'loss': 0.0371, 'learning_rate': 1.8189688518852807e-05, 'epoch': 0.64}
{'loss': 0.0413, 'learning_rate': 1.893569599734753e-05, 'epoch': 0.62}
{'loss': 0.0508, 'learning_rate': 1.5905616239017112e-05, 'epoch': 0.68}
{'loss': 0.0292, 'learning_rate': 1.9294884783289435e-05, 'epoch': 0.61}
{'loss': 0.0488, 'learning_rate': 1.9819853008896832e-05, 'epoch': 0.6}
{'loss': 0.0419, 'learning_rate': 1.8116008767890365e-05, 'epoch': 0.64}
{'loss': 0.0477, 'learning_rate': 1.886201624638509e-05, 'epoch': 0.62}
{'loss': 0.0411, 'learning_rate': 1.583193648805467e-05, 'epoch': 0.68}
{'loss': 0.0359, 'learning_rate': 1.9221205032326994e-05, 'epoch': 0.62}
{'loss': 0.0389, 'learning_rate': 1.974617325793439e-05, 'epoch': 0.61}
{'loss': 0.0753, 'learning_rate': 1.8042329016927924e-05, 'epoch': 0.64}
{'loss': 0.07, 'learning_rate': 1.878833649542265e-05, 'epoch': 0.62}
{'loss': 0.0446, 'learning_rate': 1.5758256737092232e-05, 'epoch': 0.69}
{'loss': 0.0348, 'learning_rate': 1.914752528136455e-05, 'epoch': 0.62}
{'loss': 0.0449, 'learning_rate': 1.967249350697195e-05, 'epoch': 0.61}
{'loss': 0.0558, 'learning_rate': 1.7968649265965486e-05, 'epoch': 0.64}
{'loss': 0.0776, 'learning_rate': 1.568457698612979e-05, 'epoch': 0.69}
{'loss': 0.0438, 'learning_rate': 1.8714656744460207e-05, 'epoch': 0.63}
{'loss': 0.0571, 'learning_rate': 1.907384553040211e-05, 'epoch': 0.62}
{'loss': 0.0455, 'learning_rate': 1.9598813756009508e-05, 'epoch': 0.61}
{'loss': 0.0557, 'learning_rate': 1.789496951500304e-05, 'epoch': 0.64}
{'loss': 0.0582, 'learning_rate': 1.5610897235167346e-05, 'epoch': 0.69}
{'loss': 0.0365, 'learning_rate': 1.8640976993497762e-05, 'epoch': 0.63}
{'loss': 0.0737, 'learning_rate': 1.900016577943967e-05, 'epoch': 0.62}
{'loss': 0.0519, 'learning_rate': 1.9525134005047066e-05, 'epoch': 0.61}
{'loss': 0.0557, 'learning_rate': 1.78212897640406e-05, 'epoch': 0.64}
{'loss': 0.0754, 'learning_rate': 1.5537217484204904e-05, 'epoch': 0.69}
{'loss': 0.0626, 'learning_rate': 1.8567297242535324e-05, 'epoch': 0.63}
{'loss': 0.0411, 'learning_rate': 1.8926486028477225e-05, 'epoch': 0.62}
{'loss': 0.064, 'learning_rate': 1.945145425408462e-05, 'epoch': 0.61}
{'loss': 0.0215, 'learning_rate': 1.7747610013078158e-05, 'epoch': 0.65}
{'loss': 0.0493, 'learning_rate': 1.5463537733242463e-05, 'epoch': 0.69}
{'loss': 0.0393, 'learning_rate': 1.8493617491572883e-05, 'epoch': 0.63}
{'loss': 0.068, 'learning_rate': 1.8852806277514783e-05, 'epoch': 0.62}
{'loss': 0.058, 'learning_rate': 1.937777450312218e-05, 'epoch': 0.61}
{'loss': 0.0651, 'learning_rate': 1.7673930262115713e-05, 'epoch': 0.65}
{'loss': 0.0416, 'learning_rate': 1.538985798228002e-05, 'epoch': 0.69}
{'loss': 0.0408, 'learning_rate': 1.8419937740610438e-05, 'epoch': 0.63}
{'loss': 0.0172, 'learning_rate': 1.8779126526552342e-05, 'epoch': 0.62}
{'loss': 0.0531, 'learning_rate': 1.7600250511153275e-05, 'epoch': 0.65}
{'loss': 0.0821, 'learning_rate': 1.9304094752159742e-05, 'epoch': 0.61}
{'loss': 0.0457, 'learning_rate': 1.531617823131758e-05, 'epoch': 0.69}
{'loss': 0.059, 'learning_rate': 1.8346257989647996e-05, 'epoch': 0.63}
{'loss': 0.0381, 'learning_rate': 1.87054467755899e-05, 'epoch': 0.63}
{'loss': 0.0624, 'learning_rate': 1.7526570760190834e-05, 'epoch': 0.65}
{'loss': 0.0415, 'learning_rate': 1.92304150011973e-05, 'epoch': 0.62}
{'loss': 0.0357, 'learning_rate': 1.5242498480355139e-05, 'epoch': 0.7}
{'loss': 0.0577, 'learning_rate': 1.8272578238685555e-05, 'epoch': 0.64}
{'loss': 0.0726, 'learning_rate': 1.7452891009228392e-05, 'epoch': 0.65}
{'loss': 0.0577, 'learning_rate': 1.863176702462746e-05, 'epoch': 0.63}
{'loss': 0.0662, 'learning_rate': 1.5168818729392697e-05, 'epoch': 0.7}
{'loss': 0.0795, 'learning_rate': 1.9156735250234856e-05, 'epoch': 0.62}
{'loss': 0.0444, 'learning_rate': 1.8198898487723113e-05, 'epoch': 0.64}
{'loss': 0.0483, 'learning_rate': 1.7379211258265948e-05, 'epoch': 0.65}
{'loss': 0.0505, 'learning_rate': 1.8558087273665018e-05, 'epoch': 0.63}
{'loss': 0.0377, 'learning_rate': 1.5095138978430254e-05, 'epoch': 0.7}
{'loss': 0.0263, 'learning_rate': 1.9083055499272414e-05, 'epoch': 0.62}
{'loss': 0.0652, 'learning_rate': 1.8125218736760672e-05, 'epoch': 0.64}
{'loss': 0.0478, 'learning_rate': 1.7305531507303506e-05, 'epoch': 0.65}
{'loss': 0.0398, 'learning_rate': 1.8484407522702576e-05, 'epoch': 0.63}
{'loss': 0.0461, 'learning_rate': 1.5021459227467813e-05, 'epoch': 0.7}
{'loss': 0.0315, 'learning_rate': 1.9009375748309973e-05, 'epoch': 0.62}
{'loss': 0.0597, 'learning_rate': 1.805153898579823e-05, 'epoch': 0.64}
{'loss': 0.0222, 'learning_rate': 1.841072777174013e-05, 'epoch': 0.63}
{'loss': 0.0362, 'learning_rate': 1.7231851756341068e-05, 'epoch': 0.66}
{'loss': 0.0804, 'learning_rate': 1.4947779476505371e-05, 'epoch': 0.7}
{'loss': 0.0523, 'learning_rate': 1.893569599734753e-05, 'epoch': 0.62}
{'loss': 0.0423, 'learning_rate': 1.797785923483579e-05, 'epoch': 0.64}
{'loss': 0.041, 'learning_rate': 1.7158172005378623e-05, 'epoch': 0.66}
{'loss': 0.0474, 'learning_rate': 1.8337048020777693e-05, 'epoch': 0.63}
{'loss': 0.031, 'learning_rate': 1.4874099725542928e-05, 'epoch': 0.7}
{'loss': 0.0798, 'learning_rate': 1.886201624638509e-05, 'epoch': 0.62}
{'loss': 0.0442, 'learning_rate': 1.7904179483873344e-05, 'epoch': 0.64}
{'loss': 0.047, 'learning_rate': 1.7084492254416182e-05, 'epoch': 0.66}
{'loss': 0.0296, 'learning_rate': 1.8263368269815252e-05, 'epoch': 0.64}
{'loss': 0.0696, 'learning_rate': 1.4800419974580487e-05, 'epoch': 0.7}
{'loss': 0.0799, 'learning_rate': 1.878833649542265e-05, 'epoch': 0.62}
{'loss': 0.0419, 'learning_rate': 1.701081250345374e-05, 'epoch': 0.66}
{'loss': 0.0394, 'learning_rate': 1.7830499732910906e-05, 'epoch': 0.64}
{'loss': 0.0557, 'learning_rate': 1.8189688518852807e-05, 'epoch': 0.64}
{'loss': 0.0291, 'learning_rate': 1.4726740223618047e-05, 'epoch': 0.71}
{'loss': 0.048, 'learning_rate': 1.8714656744460207e-05, 'epoch': 0.63}
{'loss': 0.0312, 'learning_rate': 1.69371327524913e-05, 'epoch': 0.66}
{'loss': 0.0566, 'learning_rate': 1.8116008767890365e-05, 'epoch': 0.64}
{'loss': 0.0393, 'learning_rate': 1.7756819981948465e-05, 'epoch': 0.65}
{'loss': 0.047, 'learning_rate': 1.4653060472655605e-05, 'epoch': 0.71}
{'loss': 0.0347, 'learning_rate': 1.8640976993497762e-05, 'epoch': 0.63}
{'loss': 0.0693, 'learning_rate': 1.6863453001528857e-05, 'epoch': 0.66}
{'loss': 0.1013, 'learning_rate': 1.8042329016927924e-05, 'epoch': 0.64}
{'loss': 0.0503, 'learning_rate': 1.768314023098602e-05, 'epoch': 0.65}
{'loss': 0.0355, 'learning_rate': 1.4579380721693162e-05, 'epoch': 0.71}
{'loss': 0.028, 'learning_rate': 1.8567297242535324e-05, 'epoch': 0.63}
{'loss': 0.0323, 'learning_rate': 1.6789773250566416e-05, 'epoch': 0.66}
{'loss': 0.0343, 'learning_rate': 1.7968649265965486e-05, 'epoch': 0.64}
{'loss': 0.0276, 'learning_rate': 1.760946048002358e-05, 'epoch': 0.65}
{'loss': 0.0664, 'learning_rate': 1.450570097073072e-05, 'epoch': 0.71}
{'loss': 0.0688, 'learning_rate': 1.8493617491572883e-05, 'epoch': 0.63}
{'loss': 0.0398, 'learning_rate': 1.6716093499603974e-05, 'epoch': 0.67}
{'loss': 0.0617, 'learning_rate': 1.789496951500304e-05, 'epoch': 0.64}
{'loss': 0.0534, 'learning_rate': 1.443202121976828e-05, 'epoch': 0.71}
{'loss': 0.0684, 'learning_rate': 1.7535780729061137e-05, 'epoch': 0.65}
{'loss': 0.0523, 'learning_rate': 1.8419937740610438e-05, 'epoch': 0.63}
{'loss': 0.0409, 'learning_rate': 1.664241374864153e-05, 'epoch': 0.67}
{'loss': 0.045, 'learning_rate': 1.78212897640406e-05, 'epoch': 0.64}
{'loss': 0.0502, 'learning_rate': 1.4358341468805836e-05, 'epoch': 0.71}
{'loss': 0.0266, 'learning_rate': 1.74621009780987e-05, 'epoch': 0.65}
{'loss': 0.0501, 'learning_rate': 1.8346257989647996e-05, 'epoch': 0.63}
{'loss': 0.0253, 'learning_rate': 1.6568733997679088e-05, 'epoch': 0.67}
{'loss': 0.0321, 'learning_rate': 1.7747610013078158e-05, 'epoch': 0.65}
{'loss': 0.0536, 'learning_rate': 1.4284661717843395e-05, 'epoch': 0.71}
{'loss': 0.0471, 'learning_rate': 1.7388421227136254e-05, 'epoch': 0.65}
{'loss': 0.0394, 'learning_rate': 1.8272578238685555e-05, 'epoch': 0.64}
{'loss': 0.0467, 'learning_rate': 1.649505424671665e-05, 'epoch': 0.67}
{'loss': 0.0588, 'learning_rate': 1.7673930262115713e-05, 'epoch': 0.65}
{'loss': 0.0257, 'learning_rate': 1.4210981966880955e-05, 'epoch': 0.72}
{'loss': 0.0678, 'learning_rate': 1.7314741476173813e-05, 'epoch': 0.65}
{'loss': 0.0484, 'learning_rate': 1.8198898487723113e-05, 'epoch': 0.64}
{'loss': 0.0596, 'learning_rate': 1.642137449575421e-05, 'epoch': 0.67}
{'loss': 0.0365, 'learning_rate': 1.7600250511153275e-05, 'epoch': 0.65}
{'loss': 0.0449, 'learning_rate': 1.4137302215918513e-05, 'epoch': 0.72}
{'loss': 0.0286, 'learning_rate': 1.724106172521137e-05, 'epoch': 0.66}
{'loss': 0.0619, 'learning_rate': 1.6347694744791764e-05, 'epoch': 0.67}
{'loss': 0.0613, 'learning_rate': 1.8125218736760672e-05, 'epoch': 0.64}
{'loss': 0.0474, 'learning_rate': 1.7526570760190834e-05, 'epoch': 0.65}
{'loss': 0.0544, 'learning_rate': 1.4063622464956069e-05, 'epoch': 0.72}
{'loss': 0.0609, 'learning_rate': 1.6274014993829322e-05, 'epoch': 0.67}
{'loss': 0.0688, 'learning_rate': 1.7167381974248926e-05, 'epoch': 0.66}
{'loss': 0.0426, 'learning_rate': 1.805153898579823e-05, 'epoch': 0.64}
{'loss': 0.0514, 'learning_rate': 1.7452891009228392e-05, 'epoch': 0.65}
{'loss': 0.0681, 'learning_rate': 1.3989942713993629e-05, 'epoch': 0.72}
{'loss': 0.0699, 'learning_rate': 1.620033524286688e-05, 'epoch': 0.68}
{'loss': 0.0358, 'learning_rate': 1.7093702223286488e-05, 'epoch': 0.66}
{'loss': 0.0321, 'learning_rate': 1.797785923483579e-05, 'epoch': 0.64}
{'loss': 0.038, 'learning_rate': 1.7379211258265948e-05, 'epoch': 0.65}
{'loss': 0.0553, 'learning_rate': 1.3916262963031187e-05, 'epoch': 0.72}
{'loss': 0.0261, 'learning_rate': 1.612665549190444e-05, 'epoch': 0.68}
{'loss': 0.0499, 'learning_rate': 1.7020022472324047e-05, 'epoch': 0.66}
{'loss': 0.0364, 'learning_rate': 1.7904179483873344e-05, 'epoch': 0.64}
{'loss': 0.0459, 'learning_rate': 1.3842583212068744e-05, 'epoch': 0.72}
{'loss': 0.0255, 'learning_rate': 1.7305531507303506e-05, 'epoch': 0.65}
{'loss': 0.0501, 'learning_rate': 1.6052975740941998e-05, 'epoch': 0.68}
{'loss': 0.0495, 'learning_rate': 1.6946342721361605e-05, 'epoch': 0.66}
{'loss': 0.043, 'learning_rate': 1.7830499732910906e-05, 'epoch': 0.64}
{'loss': 0.0444, 'learning_rate': 1.3768903461106303e-05, 'epoch': 0.72}
{'loss': 0.0212, 'learning_rate': 1.7231851756341068e-05, 'epoch': 0.66}
{'loss': 0.0423, 'learning_rate': 1.5979295989979557e-05, 'epoch': 0.68}
{'loss': 0.026, 'learning_rate': 1.687266297039916e-05, 'epoch': 0.66}
{'loss': 0.0462, 'learning_rate': 1.7756819981948465e-05, 'epoch': 0.65}
{'loss': 0.0432, 'learning_rate': 1.3695223710143861e-05, 'epoch': 0.73}
{'loss': 0.0307, 'learning_rate': 1.7158172005378623e-05, 'epoch': 0.66}
{'loss': 0.0557, 'learning_rate': 1.5905616239017112e-05, 'epoch': 0.68}
{'loss': 0.0563, 'learning_rate': 1.679898321943672e-05, 'epoch': 0.66}
{'loss': 0.0371, 'learning_rate': 1.768314023098602e-05, 'epoch': 0.65}
{'loss': 0.0466, 'learning_rate': 1.3621543959181422e-05, 'epoch': 0.73}
{'loss': 0.0833, 'learning_rate': 1.7084492254416182e-05, 'epoch': 0.66}
{'loss': 0.0558, 'learning_rate': 1.583193648805467e-05, 'epoch': 0.68}
{'loss': 0.0559, 'learning_rate': 1.672530346847428e-05, 'epoch': 0.67}
{'loss': 0.0264, 'learning_rate': 1.760946048002358e-05, 'epoch': 0.65}
{'loss': 0.0161, 'learning_rate': 1.3547864208218977e-05, 'epoch': 0.73}
{'loss': 0.0541, 'learning_rate': 1.701081250345374e-05, 'epoch': 0.66}
{'loss': 0.0512, 'learning_rate': 1.5758256737092232e-05, 'epoch': 0.69}
{'loss': 0.0431, 'learning_rate': 1.6651623717511836e-05, 'epoch': 0.67}
{'loss': 0.0315, 'learning_rate': 1.3474184457256537e-05, 'epoch': 0.73}
{'loss': 0.073, 'learning_rate': 1.7535780729061137e-05, 'epoch': 0.65}
{'loss': 0.0593, 'learning_rate': 1.69371327524913e-05, 'epoch': 0.66}
{'loss': 0.0493, 'learning_rate': 1.568457698612979e-05, 'epoch': 0.69}
{'loss': 0.0436, 'learning_rate': 1.6577943966549395e-05, 'epoch': 0.67}
{'loss': 0.0681, 'learning_rate': 1.3400504706294096e-05, 'epoch': 0.73}
{'loss': 0.0653, 'learning_rate': 1.74621009780987e-05, 'epoch': 0.65}
{'loss': 0.0687, 'learning_rate': 1.6863453001528857e-05, 'epoch': 0.66}
{'loss': 0.0644, 'learning_rate': 1.5610897235167346e-05, 'epoch': 0.69}
{'loss': 0.0352, 'learning_rate': 1.6504264215586953e-05, 'epoch': 0.67}
{'loss': 0.0438, 'learning_rate': 1.332682495533165e-05, 'epoch': 0.73}
{'loss': 0.0517, 'learning_rate': 1.7388421227136254e-05, 'epoch': 0.65}
{'loss': 0.0567, 'learning_rate': 1.6789773250566416e-05, 'epoch': 0.66}
{'loss': 0.0563, 'learning_rate': 1.5537217484204904e-05, 'epoch': 0.69}
{'loss': 0.048, 'learning_rate': 1.3253145204369211e-05, 'epoch': 0.74}
{'loss': 0.0845, 'learning_rate': 1.6430584464624512e-05, 'epoch': 0.67}
{'loss': 0.065, 'learning_rate': 1.7314741476173813e-05, 'epoch': 0.65}
{'loss': 0.0507, 'learning_rate': 1.6716093499603974e-05, 'epoch': 0.67}
{'loss': 0.0493, 'learning_rate': 1.5463537733242463e-05, 'epoch': 0.69}
{'loss': 0.0592, 'learning_rate': 1.317946545340677e-05, 'epoch': 0.74}
{'loss': 0.0674, 'learning_rate': 1.664241374864153e-05, 'epoch': 0.67}
{'loss': 0.0488, 'learning_rate': 1.724106172521137e-05, 'epoch': 0.66}
{'loss': 0.0263, 'learning_rate': 1.635690471366207e-05, 'epoch': 0.67}
{'loss': 0.0332, 'learning_rate': 1.538985798228002e-05, 'epoch': 0.69}
{'loss': 0.0327, 'learning_rate': 1.3105785702444326e-05, 'epoch': 0.74}
{'loss': 0.0373, 'learning_rate': 1.6568733997679088e-05, 'epoch': 0.67}
{'loss': 0.0509, 'learning_rate': 1.7167381974248926e-05, 'epoch': 0.66}
{'loss': 0.0772, 'learning_rate': 1.628322496269963e-05, 'epoch': 0.67}
{'loss': 0.047, 'learning_rate': 1.531617823131758e-05, 'epoch': 0.69}
{'loss': 0.0495, 'learning_rate': 1.3032105951481885e-05, 'epoch': 0.74}
{'loss': 0.0489, 'learning_rate': 1.649505424671665e-05, 'epoch': 0.67}
{'loss': 0.0536, 'learning_rate': 1.7093702223286488e-05, 'epoch': 0.66}
{'loss': 0.0399, 'learning_rate': 1.6209545211737187e-05, 'epoch': 0.68}
{'loss': 0.0488, 'learning_rate': 1.2958426200519443e-05, 'epoch': 0.74}
{'loss': 0.0899, 'learning_rate': 1.5242498480355139e-05, 'epoch': 0.7}
{'loss': 0.0492, 'learning_rate': 1.642137449575421e-05, 'epoch': 0.67}
{'loss': 0.0345, 'learning_rate': 1.7020022472324047e-05, 'epoch': 0.66}
{'loss': 0.0323, 'learning_rate': 1.6135865460774743e-05, 'epoch': 0.68}
{'loss': 0.0447, 'learning_rate': 1.2884746449557004e-05, 'epoch': 0.74}
{'loss': 0.0608, 'learning_rate': 1.5168818729392697e-05, 'epoch': 0.7}
{'loss': 0.0392, 'learning_rate': 1.6347694744791764e-05, 'epoch': 0.67}
{'loss': 0.0373, 'learning_rate': 1.6946342721361605e-05, 'epoch': 0.66}
{'loss': 0.0509, 'learning_rate': 1.60621857098123e-05, 'epoch': 0.68}
{'loss': 0.0329, 'learning_rate': 1.2811066698594559e-05, 'epoch': 0.74}
{'loss': 0.0693, 'learning_rate': 1.6274014993829322e-05, 'epoch': 0.67}
{'loss': 0.0413, 'learning_rate': 1.5095138978430254e-05, 'epoch': 0.7}
{'loss': 0.0298, 'learning_rate': 1.687266297039916e-05, 'epoch': 0.66}
{'loss': 0.077, 'learning_rate': 1.5988505958849863e-05, 'epoch': 0.68}
{'loss': 0.038, 'learning_rate': 1.2737386947632119e-05, 'epoch': 0.75}
{'loss': 0.0543, 'learning_rate': 1.620033524286688e-05, 'epoch': 0.68}
{'loss': 0.0733, 'learning_rate': 1.5021459227467813e-05, 'epoch': 0.7}
{'loss': 0.0486, 'learning_rate': 1.679898321943672e-05, 'epoch': 0.66}
{'loss': 0.0783, 'learning_rate': 1.591482620788742e-05, 'epoch': 0.68}
{'loss': 0.0689, 'learning_rate': 1.2663707196669678e-05, 'epoch': 0.75}
{'loss': 0.0526, 'learning_rate': 1.612665549190444e-05, 'epoch': 0.68}
{'loss': 0.0339, 'learning_rate': 1.4947779476505371e-05, 'epoch': 0.7}
{'loss': 0.0498, 'learning_rate': 1.672530346847428e-05, 'epoch': 0.67}
{'loss': 0.0555, 'learning_rate': 1.2590027445707234e-05, 'epoch': 0.75}
{'loss': 0.0642, 'learning_rate': 1.5841146456924977e-05, 'epoch': 0.68}
{'loss': 0.0478, 'learning_rate': 1.6052975740941998e-05, 'epoch': 0.68}
{'loss': 0.0539, 'learning_rate': 1.4874099725542928e-05, 'epoch': 0.7}
{'loss': 0.0495, 'learning_rate': 1.6651623717511836e-05, 'epoch': 0.67}
{'loss': 0.0466, 'learning_rate': 1.2516347694744793e-05, 'epoch': 0.75}
{'loss': 0.0322, 'learning_rate': 1.5767466705962535e-05, 'epoch': 0.69}
{'loss': 0.0236, 'learning_rate': 1.5979295989979557e-05, 'epoch': 0.68}
{'loss': 0.0572, 'learning_rate': 1.4800419974580487e-05, 'epoch': 0.7}
{'loss': 0.0569, 'learning_rate': 1.6577943966549395e-05, 'epoch': 0.67}
{'loss': 0.0438, 'learning_rate': 1.2442667943782352e-05, 'epoch': 0.75}
{'loss': 0.0897, 'learning_rate': 1.5693786955000094e-05, 'epoch': 0.69}
{'loss': 0.0451, 'learning_rate': 1.5905616239017112e-05, 'epoch': 0.68}
{'loss': 0.0381, 'learning_rate': 1.4726740223618047e-05, 'epoch': 0.71}
{'loss': 0.0618, 'learning_rate': 1.6504264215586953e-05, 'epoch': 0.67}
{'loss': 0.0246, 'learning_rate': 1.236898819281991e-05, 'epoch': 0.75}
{'loss': 0.0407, 'learning_rate': 1.583193648805467e-05, 'epoch': 0.68}
{'loss': 0.0388, 'learning_rate': 1.5620107204037652e-05, 'epoch': 0.69}
{'loss': 0.0696, 'learning_rate': 1.4653060472655605e-05, 'epoch': 0.71}
{'loss': 0.0521, 'learning_rate': 1.6430584464624512e-05, 'epoch': 0.67}
{'loss': 0.039, 'learning_rate': 1.2295308441857469e-05, 'epoch': 0.75}
{'loss': 0.0728, 'learning_rate': 1.5758256737092232e-05, 'epoch': 0.69}
{'loss': 0.0514, 'learning_rate': 1.554642745307521e-05, 'epoch': 0.69}
{'loss': 0.0366, 'learning_rate': 1.4579380721693162e-05, 'epoch': 0.71}
{'loss': 0.0532, 'learning_rate': 1.635690471366207e-05, 'epoch': 0.67}
{'loss': 0.0779, 'learning_rate': 1.2221628690895026e-05, 'epoch': 0.76}
{'loss': 0.0375, 'learning_rate': 1.568457698612979e-05, 'epoch': 0.69}
{'loss': 0.0353, 'learning_rate': 1.547274770211277e-05, 'epoch': 0.69}
{'loss': 0.0701, 'learning_rate': 1.450570097073072e-05, 'epoch': 0.71}
{'loss': 0.0603, 'learning_rate': 1.628322496269963e-05, 'epoch': 0.67}
{'loss': 0.0652, 'learning_rate': 1.2147948939932584e-05, 'epoch': 0.76}
{'loss': 0.0433, 'learning_rate': 1.5610897235167346e-05, 'epoch': 0.69}
{'loss': 0.0669, 'learning_rate': 1.5399067951150325e-05, 'epoch': 0.69}
{'loss': 0.0629, 'learning_rate': 1.443202121976828e-05, 'epoch': 0.71}
{'loss': 0.0362, 'learning_rate': 1.6209545211737187e-05, 'epoch': 0.68}
{'loss': 0.0751, 'learning_rate': 1.2074269188970143e-05, 'epoch': 0.76}
{'loss': 0.0443, 'learning_rate': 1.5537217484204904e-05, 'epoch': 0.69}
{'loss': 0.0712, 'learning_rate': 1.5325388200187883e-05, 'epoch': 0.69}
{'loss': 0.0351, 'learning_rate': 1.4358341468805836e-05, 'epoch': 0.71}
{'loss': 0.033, 'learning_rate': 1.6135865460774743e-05, 'epoch': 0.68}
{'loss': 0.0317, 'learning_rate': 1.2000589438007701e-05, 'epoch': 0.76}
{'loss': 0.0363, 'learning_rate': 1.5463537733242463e-05, 'epoch': 0.69}
{'loss': 0.0274, 'learning_rate': 1.5251708449225443e-05, 'epoch': 0.7}
{'loss': 0.0377, 'learning_rate': 1.4284661717843395e-05, 'epoch': 0.71}
{'loss': 0.0536, 'learning_rate': 1.192690968704526e-05, 'epoch': 0.76}
{'loss': 0.0562, 'learning_rate': 1.60621857098123e-05, 'epoch': 0.68}
{'loss': 0.0468, 'learning_rate': 1.538985798228002e-05, 'epoch': 0.69}
{'loss': 0.0332, 'learning_rate': 1.5178028698263002e-05, 'epoch': 0.7}
{'loss': 0.0397, 'learning_rate': 1.4210981966880955e-05, 'epoch': 0.72}
{'loss': 0.0586, 'learning_rate': 1.1853229936082817e-05, 'epoch': 0.76}
{'loss': 0.0487, 'learning_rate': 1.5988505958849863e-05, 'epoch': 0.68}
{'loss': 0.0505, 'learning_rate': 1.531617823131758e-05, 'epoch': 0.69}
{'loss': 0.0469, 'learning_rate': 1.5104348947300559e-05, 'epoch': 0.7}
{'loss': 0.0408, 'learning_rate': 1.1779550185120377e-05, 'epoch': 0.76}
{'loss': 0.0618, 'learning_rate': 1.4137302215918513e-05, 'epoch': 0.72}
{'loss': 0.0293, 'learning_rate': 1.591482620788742e-05, 'epoch': 0.68}
{'loss': 0.0228, 'learning_rate': 1.5242498480355139e-05, 'epoch': 0.7}
{'loss': 0.0315, 'learning_rate': 1.4063622464956069e-05, 'epoch': 0.72}
{'loss': 0.0347, 'learning_rate': 1.5030669196338117e-05, 'epoch': 0.7}
{'loss': 0.0499, 'learning_rate': 1.1705870434157934e-05, 'epoch': 0.77}
{'loss': 0.0548, 'learning_rate': 1.5841146456924977e-05, 'epoch': 0.68}
{'loss': 0.0334, 'learning_rate': 1.5168818729392697e-05, 'epoch': 0.7}
{'loss': 0.0395, 'learning_rate': 1.3989942713993629e-05, 'epoch': 0.72}
{'loss': 0.0688, 'learning_rate': 1.4956989445375678e-05, 'epoch': 0.7}
{'loss': 0.0364, 'learning_rate': 1.1632190683195492e-05, 'epoch': 0.77}
{'loss': 0.062, 'learning_rate': 1.5767466705962535e-05, 'epoch': 0.69}
{'loss': 0.0352, 'learning_rate': 1.5095138978430254e-05, 'epoch': 0.7}
{'loss': 0.0411, 'learning_rate': 1.155851093223305e-05, 'epoch': 0.77}
{'loss': 0.0384, 'learning_rate': 1.3916262963031187e-05, 'epoch': 0.72}
{'loss': 0.09, 'learning_rate': 1.4883309694413233e-05, 'epoch': 0.7}
{'loss': 0.0527, 'learning_rate': 1.5693786955000094e-05, 'epoch': 0.69}
{'loss': 0.0499, 'learning_rate': 1.5021459227467813e-05, 'epoch': 0.7}
{'loss': 0.0563, 'learning_rate': 1.1484831181270608e-05, 'epoch': 0.77}
{'loss': 0.0592, 'learning_rate': 1.3842583212068744e-05, 'epoch': 0.72}
{'loss': 0.0541, 'learning_rate': 1.4809629943450793e-05, 'epoch': 0.7}
{'loss': 0.0665, 'learning_rate': 1.5620107204037652e-05, 'epoch': 0.69}
{'loss': 0.0301, 'learning_rate': 1.4947779476505371e-05, 'epoch': 0.7}
{'loss': 0.0383, 'learning_rate': 1.1411151430308168e-05, 'epoch': 0.77}
{'loss': 0.0463, 'learning_rate': 1.3768903461106303e-05, 'epoch': 0.72}
{'loss': 0.0297, 'learning_rate': 1.4735950192488352e-05, 'epoch': 0.71}
{'loss': 0.0695, 'learning_rate': 1.554642745307521e-05, 'epoch': 0.69}
{'loss': 0.0808, 'learning_rate': 1.4874099725542928e-05, 'epoch': 0.7}
{'loss': 0.0364, 'learning_rate': 1.1337471679345725e-05, 'epoch': 0.77}
{'loss': 0.0283, 'learning_rate': 1.3695223710143861e-05, 'epoch': 0.73}
{'loss': 0.058, 'learning_rate': 1.4800419974580487e-05, 'epoch': 0.7}
{'loss': 0.0586, 'learning_rate': 1.466227044152591e-05, 'epoch': 0.71}
{'loss': 0.0589, 'learning_rate': 1.547274770211277e-05, 'epoch': 0.69}
{'loss': 0.066, 'learning_rate': 1.1263791928383283e-05, 'epoch': 0.78}
{'loss': 0.018, 'learning_rate': 1.3621543959181422e-05, 'epoch': 0.73}
{'loss': 0.048, 'learning_rate': 1.4726740223618047e-05, 'epoch': 0.71}
{'loss': 0.0639, 'learning_rate': 1.5399067951150325e-05, 'epoch': 0.69}
{'loss': 0.0909, 'learning_rate': 1.4588590690563467e-05, 'epoch': 0.71}
{'loss': 0.0674, 'learning_rate': 1.1190112177420842e-05, 'epoch': 0.78}
{'loss': 0.0595, 'learning_rate': 1.4653060472655605e-05, 'epoch': 0.71}
{'loss': 0.0396, 'learning_rate': 1.3547864208218977e-05, 'epoch': 0.73}
{'loss': 0.0298, 'learning_rate': 1.5325388200187883e-05, 'epoch': 0.69}
{'loss': 0.0326, 'learning_rate': 1.4514910939601025e-05, 'epoch': 0.71}
{'loss': 0.0449, 'learning_rate': 1.1116432426458399e-05, 'epoch': 0.78}
{'loss': 0.0428, 'learning_rate': 1.4579380721693162e-05, 'epoch': 0.71}
{'loss': 0.0679, 'learning_rate': 1.3474184457256537e-05, 'epoch': 0.73}
{'loss': 0.0514, 'learning_rate': 1.5251708449225443e-05, 'epoch': 0.7}
{'loss': 0.0704, 'learning_rate': 1.4441231188638584e-05, 'epoch': 0.71}
{'loss': 0.0373, 'learning_rate': 1.1042752675495959e-05, 'epoch': 0.78}
{'loss': 0.0671, 'learning_rate': 1.450570097073072e-05, 'epoch': 0.71}
{'loss': 0.0406, 'learning_rate': 1.3400504706294096e-05, 'epoch': 0.73}
{'loss': 0.0594, 'learning_rate': 1.5178028698263002e-05, 'epoch': 0.7}
{'loss': 0.0762, 'learning_rate': 1.4367551437676141e-05, 'epoch': 0.71}
{'loss': 0.0209, 'learning_rate': 1.0969072924533516e-05, 'epoch': 0.78}
{'loss': 0.0457, 'learning_rate': 1.443202121976828e-05, 'epoch': 0.71}
{'loss': 0.0404, 'learning_rate': 1.332682495533165e-05, 'epoch': 0.73}
{'loss': 0.0441, 'learning_rate': 1.5104348947300559e-05, 'epoch': 0.7}
{'loss': 0.0503, 'learning_rate': 1.42938716867137e-05, 'epoch': 0.71}
{'loss': 0.0548, 'learning_rate': 1.0895393173571076e-05, 'epoch': 0.78}
{'loss': 0.0427, 'learning_rate': 1.4358341468805836e-05, 'epoch': 0.71}
{'loss': 0.0266, 'learning_rate': 1.3253145204369211e-05, 'epoch': 0.74}
{'loss': 0.0417, 'learning_rate': 1.5030669196338117e-05, 'epoch': 0.7}
{'loss': 0.0442, 'learning_rate': 1.422019193575126e-05, 'epoch': 0.72}
{'loss': 0.0473, 'learning_rate': 1.0821713422608633e-05, 'epoch': 0.78}
{'loss': 0.0405, 'learning_rate': 1.4284661717843395e-05, 'epoch': 0.71}
{'loss': 0.0505, 'learning_rate': 1.317946545340677e-05, 'epoch': 0.74}
{'loss': 0.0491, 'learning_rate': 1.4956989445375678e-05, 'epoch': 0.7}
{'loss': 0.0556, 'learning_rate': 1.074803367164619e-05, 'epoch': 0.79}
{'loss': 0.0644, 'learning_rate': 1.4146512184788818e-05, 'epoch': 0.72}
{'loss': 0.0552, 'learning_rate': 1.4210981966880955e-05, 'epoch': 0.72}
{'loss': 0.0629, 'learning_rate': 1.3105785702444326e-05, 'epoch': 0.74}
{'loss': 0.0352, 'learning_rate': 1.4883309694413233e-05, 'epoch': 0.7}
{'loss': 0.0385, 'learning_rate': 1.067435392068375e-05, 'epoch': 0.79}
{'loss': 0.0414, 'learning_rate': 1.4072832433826375e-05, 'epoch': 0.72}
{'loss': 0.0449, 'learning_rate': 1.4137302215918513e-05, 'epoch': 0.72}
{'loss': 0.042, 'learning_rate': 1.3032105951481885e-05, 'epoch': 0.74}
{'loss': 0.0671, 'learning_rate': 1.0600674169721307e-05, 'epoch': 0.79}
{'loss': 0.0332, 'learning_rate': 1.4809629943450793e-05, 'epoch': 0.7}
{'loss': 0.0464, 'learning_rate': 1.3999152682863934e-05, 'epoch': 0.72}
{'loss': 0.0344, 'learning_rate': 1.4063622464956069e-05, 'epoch': 0.72}
{'loss': 0.0889, 'learning_rate': 1.0526994418758867e-05, 'epoch': 0.79}
{'loss': 0.0416, 'learning_rate': 1.2958426200519443e-05, 'epoch': 0.74}
{'loss': 0.0448, 'learning_rate': 1.4735950192488352e-05, 'epoch': 0.71}
{'loss': 0.0212, 'learning_rate': 1.3925472931901492e-05, 'epoch': 0.72}
{'loss': 0.0365, 'learning_rate': 1.3989942713993629e-05, 'epoch': 0.72}
{'loss': 0.0344, 'learning_rate': 1.0453314667796424e-05, 'epoch': 0.79}
{'loss': 0.0483, 'learning_rate': 1.2884746449557004e-05, 'epoch': 0.74}
{'loss': 0.0681, 'learning_rate': 1.466227044152591e-05, 'epoch': 0.71}
{'loss': 0.0441, 'learning_rate': 1.3851793180939049e-05, 'epoch': 0.72}
{'loss': 0.0229, 'learning_rate': 1.3916262963031187e-05, 'epoch': 0.72}
{'loss': 0.0495, 'learning_rate': 1.0379634916833982e-05, 'epoch': 0.79}
{'loss': 0.0306, 'learning_rate': 1.2811066698594559e-05, 'epoch': 0.74}
{'loss': 0.0371, 'learning_rate': 1.4588590690563467e-05, 'epoch': 0.71}
{'loss': 0.0375, 'learning_rate': 1.3842583212068744e-05, 'epoch': 0.72}
{'loss': 0.0487, 'learning_rate': 1.3778113429976608e-05, 'epoch': 0.72}
{'loss': 0.0337, 'learning_rate': 1.0305955165871541e-05, 'epoch': 0.79}
{'loss': 0.051, 'learning_rate': 1.2737386947632119e-05, 'epoch': 0.75}
{'loss': 0.052, 'learning_rate': 1.4514910939601025e-05, 'epoch': 0.71}
{'loss': 0.0309, 'learning_rate': 1.3768903461106303e-05, 'epoch': 0.72}
{'loss': 0.0597, 'learning_rate': 1.3704433679014168e-05, 'epoch': 0.73}
{'loss': 0.0307, 'learning_rate': 1.0232275414909098e-05, 'epoch': 0.8}
{'loss': 0.0519, 'learning_rate': 1.2663707196669678e-05, 'epoch': 0.75}
{'loss': 0.0756, 'learning_rate': 1.4441231188638584e-05, 'epoch': 0.71}
{'loss': 0.0703, 'learning_rate': 1.3695223710143861e-05, 'epoch': 0.73}
{'loss': 0.0307, 'learning_rate': 1.3630753928051726e-05, 'epoch': 0.73}
{'loss': 0.037, 'learning_rate': 1.0158595663946658e-05, 'epoch': 0.8}
{'loss': 0.0559, 'learning_rate': 1.2590027445707234e-05, 'epoch': 0.75}
{'loss': 0.0696, 'learning_rate': 1.3621543959181422e-05, 'epoch': 0.73}
{'loss': 0.0344, 'learning_rate': 1.4367551437676141e-05, 'epoch': 0.71}
{'loss': 0.0285, 'learning_rate': 1.0084915912984215e-05, 'epoch': 0.8}
{'loss': 0.06, 'learning_rate': 1.3557074177089282e-05, 'epoch': 0.73}
{'loss': 0.0594, 'learning_rate': 1.3547864208218977e-05, 'epoch': 0.73}
{'loss': 0.0364, 'learning_rate': 1.2516347694744793e-05, 'epoch': 0.75}
{'loss': 0.0521, 'learning_rate': 1.42938716867137e-05, 'epoch': 0.71}
{'loss': 0.0316, 'learning_rate': 1.0011236162021773e-05, 'epoch': 0.8}
{'loss': 0.0719, 'learning_rate': 1.3483394426126842e-05, 'epoch': 0.73}
{'loss': 0.0363, 'learning_rate': 1.3474184457256537e-05, 'epoch': 0.73}
{'loss': 0.0277, 'learning_rate': 1.2442667943782352e-05, 'epoch': 0.75}
{'loss': 0.0394, 'learning_rate': 1.422019193575126e-05, 'epoch': 0.72}
{'loss': 0.068, 'learning_rate': 9.937556411059332e-06, 'epoch': 0.8}
{'loss': 0.0308, 'learning_rate': 1.34097146751644e-05, 'epoch': 0.73}
{'loss': 0.0436, 'learning_rate': 1.3400504706294096e-05, 'epoch': 0.73}
{'loss': 0.0252, 'learning_rate': 1.236898819281991e-05, 'epoch': 0.75}
{'loss': 0.0486, 'learning_rate': 9.863876660096889e-06, 'epoch': 0.8}
{'loss': 0.0365, 'learning_rate': 1.4146512184788818e-05, 'epoch': 0.72}
{'loss': 0.0651, 'learning_rate': 1.3336034924201957e-05, 'epoch': 0.73}
{'loss': 0.0648, 'learning_rate': 1.332682495533165e-05, 'epoch': 0.73}
{'loss': 0.0478, 'learning_rate': 1.2295308441857469e-05, 'epoch': 0.75}
{'loss': 0.0375, 'learning_rate': 9.790196909134449e-06, 'epoch': 0.8}
{'loss': 0.0366, 'learning_rate': 1.4072832433826375e-05, 'epoch': 0.72}
{'loss': 0.0419, 'learning_rate': 1.3262355173239516e-05, 'epoch': 0.74}
{'loss': 0.0211, 'learning_rate': 1.3253145204369211e-05, 'epoch': 0.74}
{'loss': 0.0349, 'learning_rate': 9.716517158172006e-06, 'epoch': 0.81}
{'loss': 0.0532, 'learning_rate': 1.2221628690895026e-05, 'epoch': 0.76}
{'loss': 0.0507, 'learning_rate': 1.3999152682863934e-05, 'epoch': 0.72}
{'loss': 0.0399, 'learning_rate': 1.317946545340677e-05, 'epoch': 0.74}
{'loss': 0.0477, 'learning_rate': 1.3188675422277074e-05, 'epoch': 0.74}
{'loss': 0.0433, 'learning_rate': 9.642837407209565e-06, 'epoch': 0.81}
{'loss': 0.0482, 'learning_rate': 1.2147948939932584e-05, 'epoch': 0.76}
{'loss': 0.0616, 'learning_rate': 1.3925472931901492e-05, 'epoch': 0.72}
{'loss': 0.0304, 'learning_rate': 1.3105785702444326e-05, 'epoch': 0.74}
{'loss': 0.0543, 'learning_rate': 1.3114995671314634e-05, 'epoch': 0.74}
{'loss': 0.0363, 'learning_rate': 9.569157656247123e-06, 'epoch': 0.81}
{'loss': 0.0686, 'learning_rate': 1.2074269188970143e-05, 'epoch': 0.76}
{'loss': 0.0447, 'learning_rate': 1.3851793180939049e-05, 'epoch': 0.72}
{'loss': 0.0437, 'learning_rate': 1.3032105951481885e-05, 'epoch': 0.74}
{'loss': 0.0308, 'learning_rate': 1.304131592035219e-05, 'epoch': 0.74}
{'loss': 0.0442, 'learning_rate': 9.495477905284682e-06, 'epoch': 0.81}
{'loss': 0.0283, 'learning_rate': 1.2000589438007701e-05, 'epoch': 0.76}
{'loss': 0.0347, 'learning_rate': 1.3778113429976608e-05, 'epoch': 0.72}
{'loss': 0.0676, 'learning_rate': 1.2958426200519443e-05, 'epoch': 0.74}
{'loss': 0.0353, 'learning_rate': 1.296763616938975e-05, 'epoch': 0.74}
{'loss': 0.0574, 'learning_rate': 9.42179815432224e-06, 'epoch': 0.81}
{'loss': 0.0288, 'learning_rate': 1.192690968704526e-05, 'epoch': 0.76}
{'loss': 0.0742, 'learning_rate': 1.3704433679014168e-05, 'epoch': 0.73}
{'loss': 0.0485, 'learning_rate': 1.2884746449557004e-05, 'epoch': 0.74}
{'loss': 0.0616, 'learning_rate': 9.348118403359797e-06, 'epoch': 0.81}
{'loss': 0.0589, 'learning_rate': 1.2893956418427308e-05, 'epoch': 0.74}
{'loss': 0.0199, 'learning_rate': 1.1853229936082817e-05, 'epoch': 0.76}
{'loss': 0.0496, 'learning_rate': 1.3630753928051726e-05, 'epoch': 0.73}
{'loss': 0.0509, 'learning_rate': 1.2811066698594559e-05, 'epoch': 0.74}
{'loss': 0.0379, 'learning_rate': 9.274438652397356e-06, 'epoch': 0.81}
{'loss': 0.0666, 'learning_rate': 1.2820276667464865e-05, 'epoch': 0.74}
{'loss': 0.0424, 'learning_rate': 1.1779550185120377e-05, 'epoch': 0.76}
{'loss': 0.0212, 'learning_rate': 1.3557074177089282e-05, 'epoch': 0.73}
{'loss': 0.0612, 'learning_rate': 1.2737386947632119e-05, 'epoch': 0.75}
{'loss': 0.0301, 'learning_rate': 9.200758901434914e-06, 'epoch': 0.82}
{'loss': 0.0354, 'learning_rate': 1.2746596916502424e-05, 'epoch': 0.75}
{'loss': 0.0592, 'learning_rate': 1.1705870434157934e-05, 'epoch': 0.77}
{'loss': 0.0257, 'learning_rate': 1.2663707196669678e-05, 'epoch': 0.75}
{'loss': 0.0454, 'learning_rate': 1.3483394426126842e-05, 'epoch': 0.73}
{'loss': 0.0568, 'learning_rate': 9.127079150472473e-06, 'epoch': 0.82}
{'loss': 0.0285, 'learning_rate': 1.2672917165539982e-05, 'epoch': 0.75}
{'loss': 0.0446, 'learning_rate': 1.2590027445707234e-05, 'epoch': 0.75}
{'loss': 0.0737, 'learning_rate': 1.1632190683195492e-05, 'epoch': 0.77}
{'loss': 0.058, 'learning_rate': 1.34097146751644e-05, 'epoch': 0.73}
{'loss': 0.045, 'learning_rate': 9.053399399510031e-06, 'epoch': 0.82}
{'loss': 0.0468, 'learning_rate': 1.259923741457754e-05, 'epoch': 0.75}
{'loss': 0.0402, 'learning_rate': 1.2516347694744793e-05, 'epoch': 0.75}
{'loss': 0.0375, 'learning_rate': 1.155851093223305e-05, 'epoch': 0.77}
{'loss': 0.0345, 'learning_rate': 8.97971964854759e-06, 'epoch': 0.82}
{'loss': 0.0434, 'learning_rate': 1.3336034924201957e-05, 'epoch': 0.73}
{'loss': 0.061, 'learning_rate': 1.2525557663615098e-05, 'epoch': 0.75}
{'loss': 0.0703, 'learning_rate': 1.2442667943782352e-05, 'epoch': 0.75}
{'loss': 0.0562, 'learning_rate': 8.906039897585147e-06, 'epoch': 0.82}
{'loss': 0.0566, 'learning_rate': 1.3262355173239516e-05, 'epoch': 0.74}
{'loss': 0.0319, 'learning_rate': 1.1484831181270608e-05, 'epoch': 0.77}
{'loss': 0.0605, 'learning_rate': 1.2451877912652656e-05, 'epoch': 0.75}
{'loss': 0.0665, 'learning_rate': 1.236898819281991e-05, 'epoch': 0.75}
{'loss': 0.0337, 'learning_rate': 8.832360146622705e-06, 'epoch': 0.82}
{'loss': 0.0467, 'learning_rate': 1.1411151430308168e-05, 'epoch': 0.77}
{'loss': 0.0413, 'learning_rate': 1.3188675422277074e-05, 'epoch': 0.74}
{'loss': 0.0404, 'learning_rate': 1.2378198161690215e-05, 'epoch': 0.75}
{'loss': 0.0254, 'learning_rate': 1.2295308441857469e-05, 'epoch': 0.75}
{'loss': 0.0324, 'learning_rate': 8.758680395660264e-06, 'epoch': 0.83}
{'loss': 0.0371, 'learning_rate': 1.1337471679345725e-05, 'epoch': 0.77}
{'loss': 0.0328, 'learning_rate': 1.3114995671314634e-05, 'epoch': 0.74}
{'loss': 0.0391, 'learning_rate': 1.2304518410727773e-05, 'epoch': 0.75}
{'loss': 0.033, 'learning_rate': 1.2221628690895026e-05, 'epoch': 0.76}
{'loss': 0.0436, 'learning_rate': 8.685000644697822e-06, 'epoch': 0.83}
{'loss': 0.0352, 'learning_rate': 1.1263791928383283e-05, 'epoch': 0.78}
{'loss': 0.0413, 'learning_rate': 1.304131592035219e-05, 'epoch': 0.74}
{'loss': 0.0356, 'learning_rate': 1.2147948939932584e-05, 'epoch': 0.76}
{'loss': 0.0672, 'learning_rate': 8.61132089373538e-06, 'epoch': 0.83}
{'loss': 0.0466, 'learning_rate': 1.2230838659765332e-05, 'epoch': 0.76}
{'loss': 0.0717, 'learning_rate': 1.1190112177420842e-05, 'epoch': 0.78}
{'loss': 0.0322, 'learning_rate': 1.296763616938975e-05, 'epoch': 0.74}
{'loss': 0.0376, 'learning_rate': 1.2074269188970143e-05, 'epoch': 0.76}
{'loss': 0.0547, 'learning_rate': 8.537641142772938e-06, 'epoch': 0.83}
{'loss': 0.05, 'learning_rate': 1.215715890880289e-05, 'epoch': 0.76}
{'loss': 0.0591, 'learning_rate': 1.1116432426458399e-05, 'epoch': 0.78}
{'loss': 0.0311, 'learning_rate': 1.2893956418427308e-05, 'epoch': 0.74}
{'loss': 0.0586, 'learning_rate': 1.2000589438007701e-05, 'epoch': 0.76}
{'loss': 0.0478, 'learning_rate': 8.463961391810496e-06, 'epoch': 0.83}
{'loss': 0.0914, 'learning_rate': 1.2083479157840447e-05, 'epoch': 0.76}
{'loss': 0.0438, 'learning_rate': 1.2820276667464865e-05, 'epoch': 0.74}
{'loss': 0.0398, 'learning_rate': 1.1042752675495959e-05, 'epoch': 0.78}
{'loss': 0.0641, 'learning_rate': 1.192690968704526e-05, 'epoch': 0.76}
{'loss': 0.046, 'learning_rate': 8.390281640848055e-06, 'epoch': 0.83}
{'loss': 0.0528, 'learning_rate': 1.2009799406878006e-05, 'epoch': 0.76}
{'loss': 0.0287, 'learning_rate': 1.2746596916502424e-05, 'epoch': 0.75}
{'loss': 0.0495, 'learning_rate': 1.0969072924533516e-05, 'epoch': 0.78}
{'loss': 0.0732, 'learning_rate': 1.1853229936082817e-05, 'epoch': 0.76}
{'loss': 0.0471, 'learning_rate': 8.316601889885613e-06, 'epoch': 0.83}
{'loss': 0.0407, 'learning_rate': 1.1936119655915564e-05, 'epoch': 0.76}
{'loss': 0.0425, 'learning_rate': 1.2672917165539982e-05, 'epoch': 0.75}
{'loss': 0.0531, 'learning_rate': 1.0895393173571076e-05, 'epoch': 0.78}
{'loss': 0.0439, 'learning_rate': 1.1779550185120377e-05, 'epoch': 0.76}
{'loss': 0.0335, 'learning_rate': 8.242922138923172e-06, 'epoch': 0.84}
{'loss': 0.0407, 'learning_rate': 1.1862439904953123e-05, 'epoch': 0.76}
{'loss': 0.0529, 'learning_rate': 1.259923741457754e-05, 'epoch': 0.75}
{'loss': 0.0518, 'learning_rate': 1.0821713422608633e-05, 'epoch': 0.78}
{'loss': 0.0366, 'learning_rate': 1.1705870434157934e-05, 'epoch': 0.77}
{'loss': 0.0539, 'learning_rate': 8.169242387960729e-06, 'epoch': 0.84}
{'loss': 0.057, 'learning_rate': 1.1788760153990682e-05, 'epoch': 0.76}
{'loss': 0.0511, 'learning_rate': 1.2525557663615098e-05, 'epoch': 0.75}
{'loss': 0.0753, 'learning_rate': 1.074803367164619e-05, 'epoch': 0.79}
{'loss': 0.0304, 'learning_rate': 1.1632190683195492e-05, 'epoch': 0.77}
{'loss': 0.0455, 'learning_rate': 8.095562636998289e-06, 'epoch': 0.84}
{'loss': 0.0332, 'learning_rate': 1.1715080403028238e-05, 'epoch': 0.77}
{'loss': 0.0555, 'learning_rate': 1.2451877912652656e-05, 'epoch': 0.75}
{'loss': 0.0335, 'learning_rate': 1.067435392068375e-05, 'epoch': 0.79}
{'loss': 0.0456, 'learning_rate': 8.021882886035846e-06, 'epoch': 0.84}
{'loss': 0.0443, 'learning_rate': 1.155851093223305e-05, 'epoch': 0.77}
{'loss': 0.0399, 'learning_rate': 1.1641400652065797e-05, 'epoch': 0.77}
{'loss': 0.0881, 'learning_rate': 1.2378198161690215e-05, 'epoch': 0.75}
{'loss': 0.0656, 'learning_rate': 7.948203135073404e-06, 'epoch': 0.84}
{'loss': 0.0559, 'learning_rate': 1.1484831181270608e-05, 'epoch': 0.77}
{'loss': 0.0361, 'learning_rate': 1.0609884138591612e-05, 'epoch': 0.79}
{'loss': 0.0542, 'learning_rate': 7.874523384110963e-06, 'epoch': 0.84}
{'loss': 0.0615, 'learning_rate': 1.1411151430308168e-05, 'epoch': 0.77}
{'loss': 0.0747, 'learning_rate': 1.1567720901103356e-05, 'epoch': 0.77}
{'loss': 0.0741, 'learning_rate': 1.2304518410727773e-05, 'epoch': 0.75}
{'loss': 0.0743, 'learning_rate': 1.0536204387629172e-05, 'epoch': 0.79}
{'loss': 0.0218, 'learning_rate': 7.80084363314852e-06, 'epoch': 0.84}
{'loss': 0.0619, 'learning_rate': 1.1337471679345725e-05, 'epoch': 0.77}
{'loss': 0.0319, 'learning_rate': 1.1494041150140914e-05, 'epoch': 0.77}
{'loss': 0.0235, 'learning_rate': 1.2230838659765332e-05, 'epoch': 0.76}
{'loss': 0.0322, 'learning_rate': 1.0462524636666729e-05, 'epoch': 0.79}
{'loss': 0.0366, 'learning_rate': 7.72716388218608e-06, 'epoch': 0.85}
{'loss': 0.0347, 'learning_rate': 1.1263791928383283e-05, 'epoch': 0.78}
{'loss': 0.0496, 'learning_rate': 1.1420361399178473e-05, 'epoch': 0.77}
{'loss': 0.0523, 'learning_rate': 1.215715890880289e-05, 'epoch': 0.76}
{'loss': 0.0574, 'learning_rate': 1.0388844885704289e-05, 'epoch': 0.79}
{'loss': 0.0295, 'learning_rate': 7.653484131223637e-06, 'epoch': 0.85}
{'loss': 0.0395, 'learning_rate': 1.1190112177420842e-05, 'epoch': 0.78}
{'loss': 0.0343, 'learning_rate': 1.134668164821603e-05, 'epoch': 0.77}
{'loss': 0.0621, 'learning_rate': 1.2083479157840447e-05, 'epoch': 0.76}
{'loss': 0.0336, 'learning_rate': 1.0315165134741846e-05, 'epoch': 0.79}
{'loss': 0.0159, 'learning_rate': 7.579804380261196e-06, 'epoch': 0.85}
{'loss': 0.0272, 'learning_rate': 1.1116432426458399e-05, 'epoch': 0.78}
{'loss': 0.0448, 'learning_rate': 1.127300189725359e-05, 'epoch': 0.78}
{'loss': 0.04, 'learning_rate': 1.2009799406878006e-05, 'epoch': 0.76}
{'loss': 0.0285, 'learning_rate': 1.0241485383779403e-05, 'epoch': 0.8}
{'loss': 0.0704, 'learning_rate': 7.506124629298754e-06, 'epoch': 0.85}
{'loss': 0.046, 'learning_rate': 1.1042752675495959e-05, 'epoch': 0.78}
{'loss': 0.0521, 'learning_rate': 1.1936119655915564e-05, 'epoch': 0.76}
{'loss': 0.0505, 'learning_rate': 1.1199322146291147e-05, 'epoch': 0.78}
{'loss': 0.0378, 'learning_rate': 1.0167805632816963e-05, 'epoch': 0.8}
{'loss': 0.0357, 'learning_rate': 7.432444878336312e-06, 'epoch': 0.85}
{'loss': 0.0445, 'learning_rate': 1.0969072924533516e-05, 'epoch': 0.78}
{'loss': 0.0404, 'learning_rate': 1.1125642395328705e-05, 'epoch': 0.78}
{'loss': 0.0463, 'learning_rate': 1.1862439904953123e-05, 'epoch': 0.76}
{'loss': 0.0857, 'learning_rate': 1.009412588185452e-05, 'epoch': 0.8}
{'loss': 0.0279, 'learning_rate': 7.35876512737387e-06, 'epoch': 0.85}
{'loss': 0.0968, 'learning_rate': 1.0895393173571076e-05, 'epoch': 0.78}
{'loss': 0.0356, 'learning_rate': 1.1051962644366264e-05, 'epoch': 0.78}
{'loss': 0.0384, 'learning_rate': 1.1788760153990682e-05, 'epoch': 0.76}
{'loss': 0.0911, 'learning_rate': 7.285085376411428e-06, 'epoch': 0.85}
{'loss': 0.0625, 'learning_rate': 1.002044613089208e-05, 'epoch': 0.8}
{'loss': 0.06, 'learning_rate': 1.0821713422608633e-05, 'epoch': 0.78}
{'loss': 0.0256, 'learning_rate': 1.097828289340382e-05, 'epoch': 0.78}
{'loss': 0.0115, 'learning_rate': 1.1715080403028238e-05, 'epoch': 0.77}
{'loss': 0.0378, 'learning_rate': 9.946766379929637e-06, 'epoch': 0.8}
{'loss': 0.0222, 'learning_rate': 7.211405625448987e-06, 'epoch': 0.86}
{'loss': 0.0835, 'learning_rate': 1.074803367164619e-05, 'epoch': 0.79}
{'loss': 0.0475, 'learning_rate': 1.1641400652065797e-05, 'epoch': 0.77}
{'loss': 0.0524, 'learning_rate': 1.090460314244138e-05, 'epoch': 0.78}
{'loss': 0.0351, 'learning_rate': 7.137725874486545e-06, 'epoch': 0.86}
{'loss': 0.0603, 'learning_rate': 9.873086628967195e-06, 'epoch': 0.8}
{'loss': 0.0494, 'learning_rate': 1.067435392068375e-05, 'epoch': 0.79}
{'loss': 0.0377, 'learning_rate': 1.1567720901103356e-05, 'epoch': 0.77}
{'loss': 0.0377, 'learning_rate': 7.064046123524103e-06, 'epoch': 0.86}
{'loss': 0.0785, 'learning_rate': 1.0830923391478938e-05, 'epoch': 0.78}
{'loss': 0.0419, 'learning_rate': 9.799406878004754e-06, 'epoch': 0.8}
{'loss': 0.056, 'learning_rate': 1.0600674169721307e-05, 'epoch': 0.79}
{'loss': 0.0706, 'learning_rate': 6.990366372561662e-06, 'epoch': 0.86}
{'loss': 0.1007, 'learning_rate': 1.1494041150140914e-05, 'epoch': 0.77}
{'loss': 0.0407, 'learning_rate': 1.0757243640516496e-05, 'epoch': 0.79}
{'loss': 0.0552, 'learning_rate': 1.0526994418758867e-05, 'epoch': 0.79}
{'loss': 0.0491, 'learning_rate': 9.72572712704231e-06, 'epoch': 0.81}
{'loss': 0.0567, 'learning_rate': 6.916686621599219e-06, 'epoch': 0.86}
{'loss': 0.0648, 'learning_rate': 1.1420361399178473e-05, 'epoch': 0.77}
{'loss': 0.0381, 'learning_rate': 1.0453314667796424e-05, 'epoch': 0.79}
{'loss': 0.055, 'learning_rate': 1.0683563889554055e-05, 'epoch': 0.79}
{'loss': 0.0469, 'learning_rate': 9.652047376079871e-06, 'epoch': 0.81}
{'loss': 0.063, 'learning_rate': 6.843006870636778e-06, 'epoch': 0.86}
{'loss': 0.0544, 'learning_rate': 1.134668164821603e-05, 'epoch': 0.77}
{'loss': 0.0486, 'learning_rate': 1.0379634916833982e-05, 'epoch': 0.79}
{'loss': 0.0472, 'learning_rate': 1.0609884138591612e-05, 'epoch': 0.79}
{'loss': 0.0401, 'learning_rate': 9.578367625117428e-06, 'epoch': 0.81}
{'loss': 0.0376, 'learning_rate': 6.769327119674336e-06, 'epoch': 0.86}
{'loss': 0.0376, 'learning_rate': 1.0305955165871541e-05, 'epoch': 0.79}
{'loss': 0.0409, 'learning_rate': 1.127300189725359e-05, 'epoch': 0.78}
{'loss': 0.0088, 'learning_rate': 1.0536204387629172e-05, 'epoch': 0.79}
{'loss': 0.0396, 'learning_rate': 9.504687874154986e-06, 'epoch': 0.81}
{'loss': 0.0466, 'learning_rate': 6.695647368711895e-06, 'epoch': 0.87}
{'loss': 0.0402, 'learning_rate': 1.0232275414909098e-05, 'epoch': 0.8}
{'loss': 0.0415, 'learning_rate': 1.1199322146291147e-05, 'epoch': 0.78}
{'loss': 0.0516, 'learning_rate': 1.0462524636666729e-05, 'epoch': 0.79}
{'loss': 0.0444, 'learning_rate': 9.431008123192545e-06, 'epoch': 0.81}
{'loss': 0.036, 'learning_rate': 6.621967617749453e-06, 'epoch': 0.87}
{'loss': 0.042, 'learning_rate': 1.0158595663946658e-05, 'epoch': 0.8}
{'loss': 0.0467, 'learning_rate': 1.1125642395328705e-05, 'epoch': 0.78}
{'loss': 0.0254, 'learning_rate': 9.357328372230103e-06, 'epoch': 0.81}
{'loss': 0.0688, 'learning_rate': 1.0388844885704289e-05, 'epoch': 0.79}
{'loss': 0.0514, 'learning_rate': 6.548287866787011e-06, 'epoch': 0.87}
{'loss': 0.0567, 'learning_rate': 1.0084915912984215e-05, 'epoch': 0.8}
{'loss': 0.0676, 'learning_rate': 1.1051962644366264e-05, 'epoch': 0.78}
{'loss': 0.0348, 'learning_rate': 9.283648621267662e-06, 'epoch': 0.81}
{'loss': 0.0252, 'learning_rate': 1.0315165134741846e-05, 'epoch': 0.79}
{'loss': 0.0566, 'learning_rate': 6.474608115824569e-06, 'epoch': 0.87}
{'loss': 0.0848, 'learning_rate': 1.0011236162021773e-05, 'epoch': 0.8}
{'loss': 0.041, 'learning_rate': 1.097828289340382e-05, 'epoch': 0.78}
{'loss': 0.0666, 'learning_rate': 9.209968870305219e-06, 'epoch': 0.82}
{'loss': 0.0598, 'learning_rate': 1.0241485383779403e-05, 'epoch': 0.8}
{'loss': 0.0703, 'learning_rate': 6.400928364862127e-06, 'epoch': 0.87}
{'loss': 0.0284, 'learning_rate': 9.937556411059332e-06, 'epoch': 0.8}
{'loss': 0.0588, 'learning_rate': 1.090460314244138e-05, 'epoch': 0.78}
{'loss': 0.0425, 'learning_rate': 9.136289119342777e-06, 'epoch': 0.82}
{'loss': 0.0503, 'learning_rate': 6.3272486138996864e-06, 'epoch': 0.87}
{'loss': 0.0488, 'learning_rate': 1.0167805632816963e-05, 'epoch': 0.8}
{'loss': 0.0344, 'learning_rate': 9.863876660096889e-06, 'epoch': 0.8}
{'loss': 0.0824, 'learning_rate': 1.0830923391478938e-05, 'epoch': 0.78}
{'loss': 0.0294, 'learning_rate': 6.253568862937244e-06, 'epoch': 0.88}
{'loss': 0.0635, 'learning_rate': 9.062609368380336e-06, 'epoch': 0.82}
{'loss': 0.0384, 'learning_rate': 1.009412588185452e-05, 'epoch': 0.8}
{'loss': 0.0248, 'learning_rate': 9.790196909134449e-06, 'epoch': 0.8}
{'loss': 0.0464, 'learning_rate': 1.0757243640516496e-05, 'epoch': 0.79}
{'loss': 0.0142, 'learning_rate': 6.179889111974802e-06, 'epoch': 0.88}
{'loss': 0.041, 'learning_rate': 8.988929617417895e-06, 'epoch': 0.82}
{'loss': 0.058, 'learning_rate': 1.002044613089208e-05, 'epoch': 0.8}
{'loss': 0.0527, 'learning_rate': 9.716517158172006e-06, 'epoch': 0.81}
{'loss': 0.0452, 'learning_rate': 1.0683563889554055e-05, 'epoch': 0.79}
{'loss': 0.0459, 'learning_rate': 6.10620936101236e-06, 'epoch': 0.88}
{'loss': 0.022, 'learning_rate': 8.915249866455453e-06, 'epoch': 0.82}
{'loss': 0.0295, 'learning_rate': 9.642837407209565e-06, 'epoch': 0.81}
{'loss': 0.0601, 'learning_rate': 9.946766379929637e-06, 'epoch': 0.8}
{'loss': 0.0293, 'learning_rate': 6.032529610049919e-06, 'epoch': 0.88}
{'loss': 0.0311, 'learning_rate': 1.0609884138591612e-05, 'epoch': 0.79}
{'loss': 0.0568, 'learning_rate': 8.84157011549301e-06, 'epoch': 0.82}
{'loss': 0.06, 'learning_rate': 9.569157656247123e-06, 'epoch': 0.81}
{'loss': 0.0287, 'learning_rate': 9.873086628967195e-06, 'epoch': 0.8}
{'loss': 0.0376, 'learning_rate': 5.9588498590874775e-06, 'epoch': 0.88}
{'loss': 0.0327, 'learning_rate': 1.0536204387629172e-05, 'epoch': 0.79}
{'loss': 0.0356, 'learning_rate': 8.767890364530568e-06, 'epoch': 0.83}
{'loss': 0.0582, 'learning_rate': 9.495477905284682e-06, 'epoch': 0.81}
{'loss': 0.0391, 'learning_rate': 9.799406878004754e-06, 'epoch': 0.8}
{'loss': 0.0587, 'learning_rate': 5.885170108125035e-06, 'epoch': 0.88}
{'loss': 0.0255, 'learning_rate': 1.0462524636666729e-05, 'epoch': 0.79}
{'loss': 0.0322, 'learning_rate': 8.694210613568127e-06, 'epoch': 0.83}
{'loss': 0.0542, 'learning_rate': 9.42179815432224e-06, 'epoch': 0.81}
{'loss': 0.027, 'learning_rate': 9.72572712704231e-06, 'epoch': 0.81}
{'loss': 0.0221, 'learning_rate': 5.811490357162593e-06, 'epoch': 0.88}
{'loss': 0.0494, 'learning_rate': 1.0388844885704289e-05, 'epoch': 0.79}
{'loss': 0.0126, 'learning_rate': 8.620530862605686e-06, 'epoch': 0.83}
{'loss': 0.048, 'learning_rate': 9.348118403359797e-06, 'epoch': 0.81}
{'loss': 0.0418, 'learning_rate': 9.652047376079871e-06, 'epoch': 0.81}
{'loss': 0.0588, 'learning_rate': 5.7378106062001514e-06, 'epoch': 0.89}
{'loss': 0.0486, 'learning_rate': 1.0315165134741846e-05, 'epoch': 0.79}
{'loss': 0.0485, 'learning_rate': 9.274438652397356e-06, 'epoch': 0.81}
{'loss': 0.0773, 'learning_rate': 8.546851111643244e-06, 'epoch': 0.83}
{'loss': 0.0477, 'learning_rate': 9.578367625117428e-06, 'epoch': 0.81}
{'loss': 0.0308, 'learning_rate': 5.66413085523771e-06, 'epoch': 0.89}
{'loss': 0.0334, 'learning_rate': 1.0241485383779403e-05, 'epoch': 0.8}
{'loss': 0.0345, 'learning_rate': 9.200758901434914e-06, 'epoch': 0.82}
{'loss': 0.0454, 'learning_rate': 8.473171360680803e-06, 'epoch': 0.83}
{'loss': 0.0404, 'learning_rate': 9.504687874154986e-06, 'epoch': 0.81}
{'loss': 0.0379, 'learning_rate': 5.5904511042752685e-06, 'epoch': 0.89}
{'loss': 0.0348, 'learning_rate': 1.0167805632816963e-05, 'epoch': 0.8}
{'loss': 0.0431, 'learning_rate': 9.127079150472473e-06, 'epoch': 0.82}
{'loss': 0.0282, 'learning_rate': 8.39949160971836e-06, 'epoch': 0.83}
{'loss': 0.0684, 'learning_rate': 9.431008123192545e-06, 'epoch': 0.81}
{'loss': 0.0409, 'learning_rate': 5.516771353312827e-06, 'epoch': 0.89}
{'loss': 0.0306, 'learning_rate': 1.009412588185452e-05, 'epoch': 0.8}
{'loss': 0.0526, 'learning_rate': 9.053399399510031e-06, 'epoch': 0.82}
{'loss': 0.0508, 'learning_rate': 8.325811858755918e-06, 'epoch': 0.83}
{'loss': 0.0583, 'learning_rate': 9.357328372230103e-06, 'epoch': 0.81}
{'loss': 0.0359, 'learning_rate': 5.443091602350385e-06, 'epoch': 0.89}
{'loss': 0.0749, 'learning_rate': 1.002044613089208e-05, 'epoch': 0.8}
{'loss': 0.0289, 'learning_rate': 8.97971964854759e-06, 'epoch': 0.82}
{'loss': 0.0471, 'learning_rate': 8.252132107793477e-06, 'epoch': 0.84}
{'loss': 0.0541, 'learning_rate': 9.283648621267662e-06, 'epoch': 0.81}
{'loss': 0.0197, 'learning_rate': 5.3694118513879425e-06, 'epoch': 0.89}
{'loss': 0.038, 'learning_rate': 8.906039897585147e-06, 'epoch': 0.82}
{'loss': 0.0564, 'learning_rate': 9.946766379929637e-06, 'epoch': 0.8}
{'loss': 0.0514, 'learning_rate': 8.178452356831035e-06, 'epoch': 0.84}
{'loss': 0.0438, 'learning_rate': 5.295732100425501e-06, 'epoch': 0.89}
{'loss': 0.0664, 'learning_rate': 9.209968870305219e-06, 'epoch': 0.82}
{'loss': 0.0263, 'learning_rate': 8.832360146622705e-06, 'epoch': 0.82}
{'loss': 0.0497, 'learning_rate': 9.873086628967195e-06, 'epoch': 0.8}
{'loss': 0.0805, 'learning_rate': 8.104772605868594e-06, 'epoch': 0.84}
{'loss': 0.042, 'learning_rate': 5.2220523494630596e-06, 'epoch': 0.9}
{'loss': 0.0442, 'learning_rate': 9.136289119342777e-06, 'epoch': 0.82}
{'loss': 0.0482, 'learning_rate': 8.758680395660264e-06, 'epoch': 0.83}
{'loss': 0.0585, 'learning_rate': 9.799406878004754e-06, 'epoch': 0.8}
{'loss': 0.0451, 'learning_rate': 8.03109285490615e-06, 'epoch': 0.84}
{'loss': 0.0429, 'learning_rate': 5.148372598500618e-06, 'epoch': 0.9}
{'loss': 0.0431, 'learning_rate': 9.062609368380336e-06, 'epoch': 0.82}
{'loss': 0.0302, 'learning_rate': 8.685000644697822e-06, 'epoch': 0.83}
{'loss': 0.0531, 'learning_rate': 9.72572712704231e-06, 'epoch': 0.81}
{'loss': 0.0537, 'learning_rate': 7.95741310394371e-06, 'epoch': 0.84}
{'loss': 0.0693, 'learning_rate': 5.074692847538176e-06, 'epoch': 0.9}
{'loss': 0.0375, 'learning_rate': 8.988929617417895e-06, 'epoch': 0.82}
{'loss': 0.0276, 'learning_rate': 8.61132089373538e-06, 'epoch': 0.83}
{'loss': 0.0227, 'learning_rate': 9.652047376079871e-06, 'epoch': 0.81}
{'loss': 0.0252, 'learning_rate': 5.001013096575734e-06, 'epoch': 0.9}
{'loss': 0.0421, 'learning_rate': 7.883733352981268e-06, 'epoch': 0.84}
{'loss': 0.053, 'learning_rate': 8.915249866455453e-06, 'epoch': 0.82}
{'loss': 0.0646, 'learning_rate': 8.537641142772938e-06, 'epoch': 0.83}
{'loss': 0.0453, 'learning_rate': 9.578367625117428e-06, 'epoch': 0.81}
{'loss': 0.0489, 'learning_rate': 4.927333345613292e-06, 'epoch': 0.9}
{'loss': 0.0492, 'learning_rate': 7.810053602018826e-06, 'epoch': 0.84}
{'loss': 0.0556, 'learning_rate': 8.84157011549301e-06, 'epoch': 0.82}
{'loss': 0.061, 'learning_rate': 8.463961391810496e-06, 'epoch': 0.83}
{'loss': 0.0328, 'learning_rate': 9.504687874154986e-06, 'epoch': 0.81}
{'loss': 0.038, 'learning_rate': 4.853653594650851e-06, 'epoch': 0.9}
{'loss': 0.0578, 'learning_rate': 7.736373851056385e-06, 'epoch': 0.85}
{'loss': 0.0196, 'learning_rate': 8.390281640848055e-06, 'epoch': 0.83}
{'loss': 0.0231, 'learning_rate': 8.767890364530568e-06, 'epoch': 0.83}
{'loss': 0.0771, 'learning_rate': 9.431008123192545e-06, 'epoch': 0.81}
{'loss': 0.0515, 'learning_rate': 4.779973843688409e-06, 'epoch': 0.9}
{'loss': 0.0422, 'learning_rate': 7.662694100093942e-06, 'epoch': 0.85}
{'loss': 0.0435, 'learning_rate': 8.316601889885613e-06, 'epoch': 0.83}
{'loss': 0.0666, 'learning_rate': 8.694210613568127e-06, 'epoch': 0.83}
{'loss': 0.0281, 'learning_rate': 9.357328372230103e-06, 'epoch': 0.81}
{'loss': 0.0747, 'learning_rate': 4.706294092725967e-06, 'epoch': 0.91}
{'loss': 0.0413, 'learning_rate': 7.589014349131501e-06, 'epoch': 0.85}
{'loss': 0.0463, 'learning_rate': 8.242922138923172e-06, 'epoch': 0.84}
{'loss': 0.029, 'learning_rate': 8.620530862605686e-06, 'epoch': 0.83}
{'loss': 0.0427, 'learning_rate': 9.283648621267662e-06, 'epoch': 0.81}
{'loss': 0.0651, 'learning_rate': 4.632614341763525e-06, 'epoch': 0.91}
{'loss': 0.0476, 'learning_rate': 7.515334598169059e-06, 'epoch': 0.85}
{'loss': 0.0318, 'learning_rate': 8.169242387960729e-06, 'epoch': 0.84}
{'loss': 0.0532, 'learning_rate': 8.546851111643244e-06, 'epoch': 0.83}
{'loss': 0.0736, 'learning_rate': 4.558934590801084e-06, 'epoch': 0.91}
{'loss': 0.0405, 'learning_rate': 9.209968870305219e-06, 'epoch': 0.82}
{'loss': 0.0349, 'learning_rate': 7.441654847206616e-06, 'epoch': 0.85}
{'loss': 0.0396, 'learning_rate': 8.095562636998289e-06, 'epoch': 0.84}
{'loss': 0.017, 'learning_rate': 8.473171360680803e-06, 'epoch': 0.83}
{'loss': 0.0507, 'learning_rate': 4.485254839838642e-06, 'epoch': 0.91}
{'loss': 0.0329, 'learning_rate': 9.136289119342777e-06, 'epoch': 0.82}
{'loss': 0.0404, 'learning_rate': 7.367975096244176e-06, 'epoch': 0.85}
{'loss': 0.064, 'learning_rate': 8.021882886035846e-06, 'epoch': 0.84}
{'loss': 0.0393, 'learning_rate': 4.4115750888762e-06, 'epoch': 0.91}
{'loss': 0.0405, 'learning_rate': 8.39949160971836e-06, 'epoch': 0.83}
{'loss': 0.0582, 'learning_rate': 9.062609368380336e-06, 'epoch': 0.82}
{'loss': 0.0301, 'learning_rate': 7.2942953452817335e-06, 'epoch': 0.85}
{'loss': 0.0555, 'learning_rate': 7.948203135073404e-06, 'epoch': 0.84}
{'loss': 0.0534, 'learning_rate': 4.337895337913758e-06, 'epoch': 0.91}
{'loss': 0.0506, 'learning_rate': 8.325811858755918e-06, 'epoch': 0.83}
{'loss': 0.0449, 'learning_rate': 8.988929617417895e-06, 'epoch': 0.82}
{'loss': 0.042, 'learning_rate': 7.874523384110963e-06, 'epoch': 0.84}
{'loss': 0.0515, 'learning_rate': 7.220615594319292e-06, 'epoch': 0.86}
{'loss': 0.0399, 'learning_rate': 4.2642155869513164e-06, 'epoch': 0.92}
{'loss': 0.0487, 'learning_rate': 8.252132107793477e-06, 'epoch': 0.84}
{'loss': 0.0208, 'learning_rate': 8.915249866455453e-06, 'epoch': 0.82}
{'loss': 0.0592, 'learning_rate': 7.810053602018826e-06, 'epoch': 0.84}
{'loss': 0.0443, 'learning_rate': 7.14693584335685e-06, 'epoch': 0.86}
{'loss': 0.0355, 'learning_rate': 4.190535835988875e-06, 'epoch': 0.92}
{'loss': 0.0344, 'learning_rate': 8.178452356831035e-06, 'epoch': 0.84}
{'loss': 0.0576, 'learning_rate': 8.84157011549301e-06, 'epoch': 0.82}
{'loss': 0.0241, 'learning_rate': 7.736373851056385e-06, 'epoch': 0.85}
{'loss': 0.0217, 'learning_rate': 7.073256092394409e-06, 'epoch': 0.86}
{'loss': 0.0692, 'learning_rate': 4.1168560850264335e-06, 'epoch': 0.92}
{'loss': 0.0793, 'learning_rate': 8.104772605868594e-06, 'epoch': 0.84}
{'loss': 0.035, 'learning_rate': 8.767890364530568e-06, 'epoch': 0.83}
{'loss': 0.0508, 'learning_rate': 7.662694100093942e-06, 'epoch': 0.85}
{'loss': 0.0497, 'learning_rate': 6.999576341431967e-06, 'epoch': 0.86}
{'loss': 0.0398, 'learning_rate': 4.043176334063991e-06, 'epoch': 0.92}
{'loss': 0.0465, 'learning_rate': 8.03109285490615e-06, 'epoch': 0.84}
{'loss': 0.0403, 'learning_rate': 8.694210613568127e-06, 'epoch': 0.83}
{'loss': 0.0688, 'learning_rate': 7.589014349131501e-06, 'epoch': 0.85}
{'loss': 0.0402, 'learning_rate': 6.9258965904695245e-06, 'epoch': 0.86}
{'loss': 0.0403, 'learning_rate': 3.969496583101549e-06, 'epoch': 0.92}
{'loss': 0.0332, 'learning_rate': 7.95741310394371e-06, 'epoch': 0.84}
{'loss': 0.0119, 'learning_rate': 8.620530862605686e-06, 'epoch': 0.83}
{'loss': 0.0482, 'learning_rate': 7.515334598169059e-06, 'epoch': 0.85}
{'loss': 0.0566, 'learning_rate': 6.852216839507084e-06, 'epoch': 0.86}
{'loss': 0.0478, 'learning_rate': 3.8958168321391075e-06, 'epoch': 0.92}
{'loss': 0.03, 'learning_rate': 7.883733352981268e-06, 'epoch': 0.84}
{'loss': 0.0378, 'learning_rate': 8.546851111643244e-06, 'epoch': 0.83}
{'loss': 0.0744, 'learning_rate': 7.441654847206616e-06, 'epoch': 0.85}
{'loss': 0.0477, 'learning_rate': 3.822137081176666e-06, 'epoch': 0.92}
{'loss': 0.0328, 'learning_rate': 6.778537088544641e-06, 'epoch': 0.86}
{'loss': 0.0518, 'learning_rate': 8.473171360680803e-06, 'epoch': 0.83}
{'loss': 0.0475, 'learning_rate': 7.810053602018826e-06, 'epoch': 0.84}
{'loss': 0.027, 'learning_rate': 7.367975096244176e-06, 'epoch': 0.85}
{'loss': 0.0428, 'learning_rate': 3.7484573302142246e-06, 'epoch': 0.93}
{'loss': 0.0595, 'learning_rate': 6.7048573375822e-06, 'epoch': 0.87}
{'loss': 0.0593, 'learning_rate': 8.39949160971836e-06, 'epoch': 0.83}
{'loss': 0.0477, 'learning_rate': 7.2942953452817335e-06, 'epoch': 0.85}
{'loss': 0.0383, 'learning_rate': 7.736373851056385e-06, 'epoch': 0.85}
{'loss': 0.0297, 'learning_rate': 3.6747775792517827e-06, 'epoch': 0.93}
{'loss': 0.0255, 'learning_rate': 6.631177586619758e-06, 'epoch': 0.87}
{'loss': 0.0651, 'learning_rate': 7.220615594319292e-06, 'epoch': 0.86}
{'loss': 0.0244, 'learning_rate': 8.325811858755918e-06, 'epoch': 0.83}
{'loss': 0.0489, 'learning_rate': 7.662694100093942e-06, 'epoch': 0.85}
{'loss': 0.0408, 'learning_rate': 6.557497835657317e-06, 'epoch': 0.87}
{'loss': 0.0432, 'learning_rate': 3.6010978282893412e-06, 'epoch': 0.93}
{'loss': 0.0505, 'learning_rate': 7.14693584335685e-06, 'epoch': 0.86}
{'loss': 0.0541, 'learning_rate': 8.252132107793477e-06, 'epoch': 0.84}
{'loss': 0.0537, 'learning_rate': 6.483818084694875e-06, 'epoch': 0.87}
{'loss': 0.0809, 'learning_rate': 7.589014349131501e-06, 'epoch': 0.85}
{'loss': 0.0305, 'learning_rate': 3.527418077326899e-06, 'epoch': 0.93}
{'loss': 0.0104, 'learning_rate': 7.073256092394409e-06, 'epoch': 0.86}
{'loss': 0.0375, 'learning_rate': 6.410138333732433e-06, 'epoch': 0.87}
{'loss': 0.0132, 'learning_rate': 8.178452356831035e-06, 'epoch': 0.84}
{'loss': 0.031, 'learning_rate': 3.453738326364457e-06, 'epoch': 0.93}
{'loss': 0.0697, 'learning_rate': 7.515334598169059e-06, 'epoch': 0.85}
{'loss': 0.068, 'learning_rate': 6.336458582769991e-06, 'epoch': 0.87}
{'loss': 0.0444, 'learning_rate': 8.104772605868594e-06, 'epoch': 0.84}
{'loss': 0.062, 'learning_rate': 6.999576341431967e-06, 'epoch': 0.86}
{'loss': 0.0437, 'learning_rate': 3.3800585754020156e-06, 'epoch': 0.93}
{'loss': 0.0359, 'learning_rate': 7.441654847206616e-06, 'epoch': 0.85}
{'loss': 0.0528, 'learning_rate': 6.262778831807549e-06, 'epoch': 0.88}
{'loss': 0.0241, 'learning_rate': 8.03109285490615e-06, 'epoch': 0.84}
{'loss': 0.0333, 'learning_rate': 6.9258965904695245e-06, 'epoch': 0.86}
{'loss': 0.0462, 'learning_rate': 3.3063788244395737e-06, 'epoch': 0.93}
{'loss': 0.0384, 'learning_rate': 7.367975096244176e-06, 'epoch': 0.85}
{'loss': 0.0425, 'learning_rate': 6.852216839507084e-06, 'epoch': 0.86}
{'loss': 0.0349, 'learning_rate': 7.95741310394371e-06, 'epoch': 0.84}
{'loss': 0.0282, 'learning_rate': 6.1890990808451074e-06, 'epoch': 0.88}
{'loss': 0.0576, 'learning_rate': 3.2326990734771323e-06, 'epoch': 0.94}
{'loss': 0.0595, 'learning_rate': 7.2942953452817335e-06, 'epoch': 0.85}
{'loss': 0.0306, 'learning_rate': 6.778537088544641e-06, 'epoch': 0.86}
{'loss': 0.052, 'learning_rate': 7.883733352981268e-06, 'epoch': 0.84}
{'loss': 0.0227, 'learning_rate': 3.1590193225146904e-06, 'epoch': 0.94}
{'loss': 0.0395, 'learning_rate': 6.115419329882666e-06, 'epoch': 0.88}
{'loss': 0.0249, 'learning_rate': 7.220615594319292e-06, 'epoch': 0.86}
{'loss': 0.0566, 'learning_rate': 6.7048573375822e-06, 'epoch': 0.87}
{'loss': 0.052, 'learning_rate': 3.0853395715522485e-06, 'epoch': 0.94}
{'loss': 0.033, 'learning_rate': 7.810053602018826e-06, 'epoch': 0.84}
{'loss': 0.0673, 'learning_rate': 6.041739578920224e-06, 'epoch': 0.88}
{'loss': 0.0294, 'learning_rate': 7.14693584335685e-06, 'epoch': 0.86}
{'loss': 0.0161, 'learning_rate': 6.631177586619758e-06, 'epoch': 0.87}
{'loss': 0.0255, 'learning_rate': 3.011659820589807e-06, 'epoch': 0.94}
{'loss': 0.0275, 'learning_rate': 7.736373851056385e-06, 'epoch': 0.85}
{'loss': 0.0434, 'learning_rate': 5.968059827957782e-06, 'epoch': 0.88}
{'loss': 0.0253, 'learning_rate': 7.073256092394409e-06, 'epoch': 0.86}
{'loss': 0.0329, 'learning_rate': 2.9379800696273648e-06, 'epoch': 0.94}
{'loss': 0.0335, 'learning_rate': 6.557497835657317e-06, 'epoch': 0.87}
{'loss': 0.0366, 'learning_rate': 7.662694100093942e-06, 'epoch': 0.85}
{'loss': 0.0264, 'learning_rate': 5.894380076995341e-06, 'epoch': 0.88}
{'loss': 0.0577, 'learning_rate': 6.999576341431967e-06, 'epoch': 0.86}
{'loss': 0.0386, 'learning_rate': 2.8643003186649233e-06, 'epoch': 0.94}
{'loss': 0.0305, 'learning_rate': 6.483818084694875e-06, 'epoch': 0.87}
{'loss': 0.0707, 'learning_rate': 7.589014349131501e-06, 'epoch': 0.85}
{'loss': 0.0385, 'learning_rate': 5.8207003260328985e-06, 'epoch': 0.88}
{'loss': 0.0308, 'learning_rate': 6.9258965904695245e-06, 'epoch': 0.86}
{'loss': 0.0538, 'learning_rate': 2.7906205677024815e-06, 'epoch': 0.94}
{'loss': 0.0432, 'learning_rate': 6.410138333732433e-06, 'epoch': 0.87}
{'loss': 0.0526, 'learning_rate': 7.515334598169059e-06, 'epoch': 0.85}
{'loss': 0.026, 'learning_rate': 5.747020575070457e-06, 'epoch': 0.89}
{'loss': 0.0647, 'learning_rate': 6.852216839507084e-06, 'epoch': 0.86}
{'loss': 0.0289, 'learning_rate': 6.336458582769991e-06, 'epoch': 0.87}
{'loss': 0.0389, 'learning_rate': 2.7169408167400396e-06, 'epoch': 0.95}
{'loss': 0.0251, 'learning_rate': 7.441654847206616e-06, 'epoch': 0.85}
{'loss': 0.0235, 'learning_rate': 5.673340824108015e-06, 'epoch': 0.89}
{'loss': 0.0481, 'learning_rate': 6.778537088544641e-06, 'epoch': 0.86}
{'loss': 0.0564, 'learning_rate': 2.643261065777598e-06, 'epoch': 0.95}
{'loss': 0.0549, 'learning_rate': 6.262778831807549e-06, 'epoch': 0.88}
{'loss': 0.0367, 'learning_rate': 7.367975096244176e-06, 'epoch': 0.85}
{'loss': 0.0498, 'learning_rate': 5.599661073145573e-06, 'epoch': 0.89}
{'loss': 0.0467, 'learning_rate': 6.7048573375822e-06, 'epoch': 0.87}
{'loss': 0.0347, 'learning_rate': 2.5695813148151563e-06, 'epoch': 0.95}
{'loss': 0.0554, 'learning_rate': 6.1890990808451074e-06, 'epoch': 0.88}
{'loss': 0.0202, 'learning_rate': 7.2942953452817335e-06, 'epoch': 0.85}
{'loss': 0.0369, 'learning_rate': 5.525981322183132e-06, 'epoch': 0.89}
{'loss': 0.0568, 'learning_rate': 6.631177586619758e-06, 'epoch': 0.87}
{'loss': 0.0629, 'learning_rate': 2.4959015638527144e-06, 'epoch': 0.95}
{'loss': 0.0457, 'learning_rate': 6.115419329882666e-06, 'epoch': 0.88}
{'loss': 0.0264, 'learning_rate': 7.220615594319292e-06, 'epoch': 0.86}
{'loss': 0.0209, 'learning_rate': 5.45230157122069e-06, 'epoch': 0.89}
{'loss': 0.0321, 'learning_rate': 2.4222218128902725e-06, 'epoch': 0.95}
{'loss': 0.0501, 'learning_rate': 6.557497835657317e-06, 'epoch': 0.87}
{'loss': 0.0446, 'learning_rate': 6.041739578920224e-06, 'epoch': 0.88}
{'loss': 0.0344, 'learning_rate': 7.14693584335685e-06, 'epoch': 0.86}
{'loss': 0.0286, 'learning_rate': 5.378621820258248e-06, 'epoch': 0.89}
{'loss': 0.0501, 'learning_rate': 2.348542061927831e-06, 'epoch': 0.95}
{'loss': 0.0366, 'learning_rate': 5.968059827957782e-06, 'epoch': 0.88}
{'loss': 0.0449, 'learning_rate': 6.483818084694875e-06, 'epoch': 0.87}
{'loss': 0.0321, 'learning_rate': 7.073256092394409e-06, 'epoch': 0.86}
{'loss': 0.0673, 'learning_rate': 5.304942069295806e-06, 'epoch': 0.89}
{'loss': 0.0402, 'learning_rate': 2.274862310965389e-06, 'epoch': 0.95}
{'loss': 0.0321, 'learning_rate': 5.894380076995341e-06, 'epoch': 0.88}
{'loss': 0.0433, 'learning_rate': 6.410138333732433e-06, 'epoch': 0.87}
{'loss': 0.0898, 'learning_rate': 6.999576341431967e-06, 'epoch': 0.86}
{'loss': 0.0388, 'learning_rate': 5.231262318333364e-06, 'epoch': 0.9}
{'loss': 0.0523, 'learning_rate': 2.2011825600029473e-06, 'epoch': 0.96}
{'loss': 0.0208, 'learning_rate': 5.8207003260328985e-06, 'epoch': 0.88}
{'loss': 0.0243, 'learning_rate': 6.336458582769991e-06, 'epoch': 0.87}
{'loss': 0.0466, 'learning_rate': 6.9258965904695245e-06, 'epoch': 0.86}
{'loss': 0.0415, 'learning_rate': 5.157582567370923e-06, 'epoch': 0.9}
{'loss': 0.0219, 'learning_rate': 2.127502809040506e-06, 'epoch': 0.96}
{'loss': 0.0342, 'learning_rate': 5.747020575070457e-06, 'epoch': 0.89}
{'loss': 0.0485, 'learning_rate': 6.262778831807549e-06, 'epoch': 0.88}
{'loss': 0.068, 'learning_rate': 6.852216839507084e-06, 'epoch': 0.86}
{'loss': 0.0561, 'learning_rate': 5.083902816408481e-06, 'epoch': 0.9}
{'loss': 0.051, 'learning_rate': 2.053823058078064e-06, 'epoch': 0.96}
{'loss': 0.0388, 'learning_rate': 5.673340824108015e-06, 'epoch': 0.89}
{'loss': 0.0397, 'learning_rate': 6.1890990808451074e-06, 'epoch': 0.88}
{'loss': 0.0571, 'learning_rate': 6.778537088544641e-06, 'epoch': 0.86}
{'loss': 0.0344, 'learning_rate': 5.01022306544604e-06, 'epoch': 0.9}
{'loss': 0.0207, 'learning_rate': 1.980143307115622e-06, 'epoch': 0.96}
{'loss': 0.025, 'learning_rate': 5.599661073145573e-06, 'epoch': 0.89}
{'loss': 0.0358, 'learning_rate': 6.115419329882666e-06, 'epoch': 0.88}
{'loss': 0.0239, 'learning_rate': 6.7048573375822e-06, 'epoch': 0.87}
{'loss': 0.0539, 'learning_rate': 1.9064635561531804e-06, 'epoch': 0.96}
{'loss': 0.0399, 'learning_rate': 4.936543314483598e-06, 'epoch': 0.9}
{'loss': 0.0377, 'learning_rate': 5.525981322183132e-06, 'epoch': 0.89}
{'loss': 0.0833, 'learning_rate': 6.041739578920224e-06, 'epoch': 0.88}
{'loss': 0.0272, 'learning_rate': 6.631177586619758e-06, 'epoch': 0.87}
{'loss': 0.0217, 'learning_rate': 1.8327838051907388e-06, 'epoch': 0.96}
{'loss': 0.0714, 'learning_rate': 4.862863563521155e-06, 'epoch': 0.9}
{'loss': 0.0379, 'learning_rate': 5.45230157122069e-06, 'epoch': 0.89}
{'loss': 0.0351, 'learning_rate': 5.968059827957782e-06, 'epoch': 0.88}
{'loss': 0.0418, 'learning_rate': 6.557497835657317e-06, 'epoch': 0.87}
{'loss': 0.0285, 'learning_rate': 1.7591040542282969e-06, 'epoch': 0.97}
{'loss': 0.0347, 'learning_rate': 4.789183812558714e-06, 'epoch': 0.9}
{'loss': 0.0446, 'learning_rate': 5.378621820258248e-06, 'epoch': 0.89}
{'loss': 0.0609, 'learning_rate': 5.894380076995341e-06, 'epoch': 0.88}
{'loss': 0.0111, 'learning_rate': 6.483818084694875e-06, 'epoch': 0.87}
{'loss': 0.0498, 'learning_rate': 1.6854243032658552e-06, 'epoch': 0.97}
{'loss': 0.0149, 'learning_rate': 4.7155040615962725e-06, 'epoch': 0.91}
{'loss': 0.0148, 'learning_rate': 5.304942069295806e-06, 'epoch': 0.89}
{'loss': 0.0423, 'learning_rate': 5.8207003260328985e-06, 'epoch': 0.88}
{'loss': 0.0556, 'learning_rate': 6.410138333732433e-06, 'epoch': 0.87}
{'loss': 0.0588, 'learning_rate': 1.6117445523034136e-06, 'epoch': 0.97}
{'loss': 0.0549, 'learning_rate': 4.641824310633831e-06, 'epoch': 0.91}
{'loss': 0.0497, 'learning_rate': 5.231262318333364e-06, 'epoch': 0.9}
{'loss': 0.0546, 'learning_rate': 6.336458582769991e-06, 'epoch': 0.87}
{'loss': 0.0463, 'learning_rate': 5.747020575070457e-06, 'epoch': 0.89}
{'loss': 0.0258, 'learning_rate': 1.5380648013409717e-06, 'epoch': 0.97}
{'loss': 0.0265, 'learning_rate': 4.568144559671389e-06, 'epoch': 0.91}
{'loss': 0.0425, 'learning_rate': 5.157582567370923e-06, 'epoch': 0.9}
{'loss': 0.0501, 'learning_rate': 6.262778831807549e-06, 'epoch': 0.88}
{'loss': 0.0421, 'learning_rate': 5.673340824108015e-06, 'epoch': 0.89}
{'loss': 0.0287, 'learning_rate': 1.4643850503785298e-06, 'epoch': 0.97}
{'loss': 0.0385, 'learning_rate': 5.083902816408481e-06, 'epoch': 0.9}
{'loss': 0.0423, 'learning_rate': 4.494464808708947e-06, 'epoch': 0.91}
{'loss': 0.0707, 'learning_rate': 6.1890990808451074e-06, 'epoch': 0.88}
{'loss': 0.0201, 'learning_rate': 1.3907052994160881e-06, 'epoch': 0.97}
{'loss': 0.0185, 'learning_rate': 5.599661073145573e-06, 'epoch': 0.89}
{'loss': 0.0529, 'learning_rate': 5.01022306544604e-06, 'epoch': 0.9}
{'loss': 0.0348, 'learning_rate': 4.420785057746505e-06, 'epoch': 0.91}
{'loss': 0.0292, 'learning_rate': 1.3170255484536465e-06, 'epoch': 0.97}
{'loss': 0.02, 'learning_rate': 6.115419329882666e-06, 'epoch': 0.88}
{'loss': 0.0483, 'learning_rate': 5.525981322183132e-06, 'epoch': 0.89}
{'loss': 0.0564, 'learning_rate': 4.936543314483598e-06, 'epoch': 0.9}
{'loss': 0.0646, 'learning_rate': 4.3471053067840635e-06, 'epoch': 0.91}
{'loss': 0.0215, 'learning_rate': 1.2433457974912046e-06, 'epoch': 0.98}
{'loss': 0.0214, 'learning_rate': 6.041739578920224e-06, 'epoch': 0.88}
{'loss': 0.0455, 'learning_rate': 5.45230157122069e-06, 'epoch': 0.89}
{'loss': 0.0587, 'learning_rate': 4.862863563521155e-06, 'epoch': 0.9}
{'loss': 0.0412, 'learning_rate': 4.273425555821622e-06, 'epoch': 0.92}
{'loss': 0.0421, 'learning_rate': 1.169666046528763e-06, 'epoch': 0.98}
{'loss': 0.0628, 'learning_rate': 5.968059827957782e-06, 'epoch': 0.88}
{'loss': 0.0483, 'learning_rate': 4.789183812558714e-06, 'epoch': 0.9}
{'loss': 0.0376, 'learning_rate': 5.378621820258248e-06, 'epoch': 0.89}
{'loss': 0.0413, 'learning_rate': 4.19974580485918e-06, 'epoch': 0.92}
{'loss': 0.0438, 'learning_rate': 1.095986295566321e-06, 'epoch': 0.98}
{'loss': 0.0799, 'learning_rate': 5.894380076995341e-06, 'epoch': 0.88}
{'loss': 0.0673, 'learning_rate': 5.304942069295806e-06, 'epoch': 0.89}
{'loss': 0.0237, 'learning_rate': 4.7155040615962725e-06, 'epoch': 0.91}
{'loss': 0.0486, 'learning_rate': 4.126066053896738e-06, 'epoch': 0.92}
{'loss': 0.0473, 'learning_rate': 1.0223065446038794e-06, 'epoch': 0.98}
{'loss': 0.0526, 'learning_rate': 5.8207003260328985e-06, 'epoch': 0.88}
{'loss': 0.0504, 'learning_rate': 4.641824310633831e-06, 'epoch': 0.91}
{'loss': 0.0659, 'learning_rate': 5.231262318333364e-06, 'epoch': 0.9}
{'loss': 0.0364, 'learning_rate': 4.052386302934297e-06, 'epoch': 0.92}
{'loss': 0.0732, 'learning_rate': 9.486267936414376e-07, 'epoch': 0.98}
{'loss': 0.0447, 'learning_rate': 5.747020575070457e-06, 'epoch': 0.89}
{'loss': 0.036, 'learning_rate': 4.568144559671389e-06, 'epoch': 0.91}
{'loss': 0.054, 'learning_rate': 5.157582567370923e-06, 'epoch': 0.9}
{'loss': 0.0414, 'learning_rate': 3.978706551971855e-06, 'epoch': 0.92}
{'loss': 0.0358, 'learning_rate': 8.749470426789959e-07, 'epoch': 0.98}
{'loss': 0.0415, 'learning_rate': 5.673340824108015e-06, 'epoch': 0.89}
{'loss': 0.0609, 'learning_rate': 4.494464808708947e-06, 'epoch': 0.91}
{'loss': 0.0207, 'learning_rate': 5.083902816408481e-06, 'epoch': 0.9}
{'loss': 0.0359, 'learning_rate': 3.905026801009413e-06, 'epoch': 0.92}
{'loss': 0.0339, 'learning_rate': 8.012672917165541e-07, 'epoch': 0.98}
{'loss': 0.0389, 'learning_rate': 4.420785057746505e-06, 'epoch': 0.91}
{'loss': 0.0317, 'learning_rate': 5.599661073145573e-06, 'epoch': 0.89}
{'loss': 0.0369, 'learning_rate': 5.01022306544604e-06, 'epoch': 0.9}
{'loss': 0.0435, 'learning_rate': 3.831347050046971e-06, 'epoch': 0.92}
{'loss': 0.0564, 'learning_rate': 7.275875407541123e-07, 'epoch': 0.99}
{'loss': 0.065, 'learning_rate': 4.3471053067840635e-06, 'epoch': 0.91}
{'loss': 0.0191, 'learning_rate': 5.525981322183132e-06, 'epoch': 0.89}
{'loss': 0.0456, 'learning_rate': 4.936543314483598e-06, 'epoch': 0.9}
{'loss': 0.0548, 'learning_rate': 3.7576672990845293e-06, 'epoch': 0.93}
{'loss': 0.061, 'learning_rate': 6.539077897916705e-07, 'epoch': 0.99}
{'loss': 0.0282, 'learning_rate': 4.273425555821622e-06, 'epoch': 0.92}
{'loss': 0.0516, 'learning_rate': 5.45230157122069e-06, 'epoch': 0.89}
{'loss': 0.0322, 'learning_rate': 3.683987548122088e-06, 'epoch': 0.93}
{'loss': 0.0617, 'learning_rate': 4.862863563521155e-06, 'epoch': 0.9}
{'loss': 0.0306, 'learning_rate': 5.802280388292289e-07, 'epoch': 0.99}
{'loss': 0.0492, 'learning_rate': 4.19974580485918e-06, 'epoch': 0.92}
{'loss': 0.0567, 'learning_rate': 5.378621820258248e-06, 'epoch': 0.89}
{'loss': 0.0335, 'learning_rate': 3.610307797159646e-06, 'epoch': 0.93}
{'loss': 0.0754, 'learning_rate': 4.789183812558714e-06, 'epoch': 0.9}
{'loss': 0.0544, 'learning_rate': 5.065482878667871e-07, 'epoch': 0.99}
{'loss': 0.0462, 'learning_rate': 4.126066053896738e-06, 'epoch': 0.92}
{'loss': 0.0516, 'learning_rate': 5.304942069295806e-06, 'epoch': 0.89}
{'loss': 0.0671, 'learning_rate': 3.5366280461972046e-06, 'epoch': 0.93}
{'loss': 0.0228, 'learning_rate': 4.328685369043453e-07, 'epoch': 0.99}
{'loss': 0.0141, 'learning_rate': 4.7155040615962725e-06, 'epoch': 0.91}
{'loss': 0.0358, 'learning_rate': 4.052386302934297e-06, 'epoch': 0.92}
{'loss': 0.0258, 'learning_rate': 5.231262318333364e-06, 'epoch': 0.9}
{'loss': 0.0343, 'learning_rate': 3.5918878594190356e-07, 'epoch': 0.99}
{'loss': 0.0423, 'learning_rate': 3.4629482952347623e-06, 'epoch': 0.93}
{'loss': 0.0398, 'learning_rate': 4.641824310633831e-06, 'epoch': 0.91}
{'loss': 0.0584, 'learning_rate': 3.978706551971855e-06, 'epoch': 0.92}
{'loss': 0.0499, 'learning_rate': 5.157582567370923e-06, 'epoch': 0.9}
{'loss': 0.0331, 'learning_rate': 2.855090349794618e-07, 'epoch': 0.99}
{'loss': 0.031, 'learning_rate': 3.3892685442723204e-06, 'epoch': 0.93}
{'loss': 0.029, 'learning_rate': 4.568144559671389e-06, 'epoch': 0.91}
{'loss': 0.0378, 'learning_rate': 3.905026801009413e-06, 'epoch': 0.92}
{'loss': 0.0478, 'learning_rate': 5.083902816408481e-06, 'epoch': 0.9}
{'loss': 0.0395, 'learning_rate': 2.1182928401702002e-07, 'epoch': 1.0}
{'loss': 0.026, 'learning_rate': 3.315588793309879e-06, 'epoch': 0.93}
{'loss': 0.0301, 'learning_rate': 4.494464808708947e-06, 'epoch': 0.91}
{'loss': 0.017, 'learning_rate': 3.831347050046971e-06, 'epoch': 0.92}
{'loss': 0.0385, 'learning_rate': 5.01022306544604e-06, 'epoch': 0.9}
{'loss': 0.027, 'learning_rate': 1.3814953305457828e-07, 'epoch': 1.0}
{'loss': 0.0317, 'learning_rate': 3.2419090423474375e-06, 'epoch': 0.94}
{'loss': 0.0454, 'learning_rate': 4.420785057746505e-06, 'epoch': 0.91}
{'loss': 0.0321, 'learning_rate': 3.7576672990845293e-06, 'epoch': 0.93}
{'loss': 0.0533, 'learning_rate': 6.446978209213654e-08, 'epoch': 1.0}
{'loss': 0.0607, 'learning_rate': 4.936543314483598e-06, 'epoch': 0.9}
{'loss': 0.0484, 'learning_rate': 3.1682292913849956e-06, 'epoch': 0.94}
{'loss': 0.0459, 'learning_rate': 4.3471053067840635e-06, 'epoch': 0.91}
{'loss': 0.0471, 'learning_rate': 3.683987548122088e-06, 'epoch': 0.93}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0832, 'learning_rate': 4.862863563521155e-06, 'epoch': 0.9}
{'loss': 0.0224, 'learning_rate': 3.0945495404225537e-06, 'epoch': 0.94}
{'loss': 0.0434, 'learning_rate': 4.273425555821622e-06, 'epoch': 0.92}
{'loss': 0.0408, 'learning_rate': 3.610307797159646e-06, 'epoch': 0.93}
{'loss': 0.0529, 'learning_rate': 4.789183812558714e-06, 'epoch': 0.9}
{'loss': 0.0611, 'learning_rate': 3.020869789460112e-06, 'epoch': 0.94}
{'loss': 0.0399, 'learning_rate': 4.19974580485918e-06, 'epoch': 0.92}
{'loss': 0.037, 'learning_rate': 3.5366280461972046e-06, 'epoch': 0.93}
{'loss': 0.0321, 'learning_rate': 4.7155040615962725e-06, 'epoch': 0.91}
{'loss': 0.0609, 'learning_rate': 2.9471900384976704e-06, 'epoch': 0.94}
{'loss': 0.029, 'learning_rate': 4.126066053896738e-06, 'epoch': 0.92}
{'loss': 0.0688, 'learning_rate': 3.4629482952347623e-06, 'epoch': 0.93}
{'loss': 0.0142, 'learning_rate': 4.641824310633831e-06, 'epoch': 0.91}
{'loss': 0.0492, 'learning_rate': 2.8735102875352285e-06, 'epoch': 0.94}
{'loss': 0.0302, 'learning_rate': 3.3892685442723204e-06, 'epoch': 0.93}
{'loss': 0.0206, 'learning_rate': 4.052386302934297e-06, 'epoch': 0.92}
{'loss': 0.0534, 'learning_rate': 4.568144559671389e-06, 'epoch': 0.91}
{'loss': 0.0386, 'learning_rate': 2.7998305365727866e-06, 'epoch': 0.94}
{'loss': 0.0242, 'learning_rate': 3.315588793309879e-06, 'epoch': 0.93}
{'loss': 0.0744, 'learning_rate': 3.978706551971855e-06, 'epoch': 0.92}
{'loss': 0.041, 'learning_rate': 4.494464808708947e-06, 'epoch': 0.91}
{'loss': 0.063, 'learning_rate': 2.726150785610345e-06, 'epoch': 0.95}
{'loss': 0.0367, 'learning_rate': 3.2419090423474375e-06, 'epoch': 0.94}
{'loss': 0.0347, 'learning_rate': 3.905026801009413e-06, 'epoch': 0.92}
{'loss': 0.0531, 'learning_rate': 4.420785057746505e-06, 'epoch': 0.91}
{'loss': 0.0246, 'learning_rate': 2.652471034647903e-06, 'epoch': 0.95}
{'loss': 0.0554, 'learning_rate': 3.1682292913849956e-06, 'epoch': 0.94}
{'loss': 0.0704, 'learning_rate': 3.831347050046971e-06, 'epoch': 0.92}
{'loss': 0.0578, 'learning_rate': 4.3471053067840635e-06, 'epoch': 0.91}
{'loss': 0.0317, 'learning_rate': 2.5787912836854614e-06, 'epoch': 0.95}
{'loss': 0.0594, 'learning_rate': 3.0945495404225537e-06, 'epoch': 0.94}
{'loss': 0.0335, 'learning_rate': 3.7576672990845293e-06, 'epoch': 0.93}
{'loss': 0.0351, 'learning_rate': 4.273425555821622e-06, 'epoch': 0.92}
{'loss': 0.0487, 'learning_rate': 2.50511153272302e-06, 'epoch': 0.95}
{'loss': 0.0515, 'learning_rate': 3.020869789460112e-06, 'epoch': 0.94}
{'loss': 0.0512, 'learning_rate': 3.683987548122088e-06, 'epoch': 0.93}
{'loss': 0.0361, 'learning_rate': 4.19974580485918e-06, 'epoch': 0.92}
{'loss': 0.0282, 'learning_rate': 2.4314317817605777e-06, 'epoch': 0.95}
{'loss': 0.0408, 'learning_rate': 2.9471900384976704e-06, 'epoch': 0.94}
{'loss': 0.0271, 'learning_rate': 3.610307797159646e-06, 'epoch': 0.93}
{'loss': 0.0379, 'learning_rate': 4.126066053896738e-06, 'epoch': 0.92}
{'loss': 0.0407, 'learning_rate': 2.3577520307981362e-06, 'epoch': 0.95}
{'loss': 0.0453, 'learning_rate': 2.8735102875352285e-06, 'epoch': 0.94}
{'loss': 0.0439, 'learning_rate': 3.5366280461972046e-06, 'epoch': 0.93}
{'loss': 0.0478, 'learning_rate': 4.052386302934297e-06, 'epoch': 0.92}
{'loss': 0.0504, 'learning_rate': 2.2840722798356944e-06, 'epoch': 0.95}
{'loss': 0.0453, 'learning_rate': 2.7998305365727866e-06, 'epoch': 0.94}
{'loss': 0.0459, 'learning_rate': 3.4629482952347623e-06, 'epoch': 0.93}
{'loss': 0.0116, 'learning_rate': 3.978706551971855e-06, 'epoch': 0.92}
{'loss': 0.0274, 'learning_rate': 2.2103925288732525e-06, 'epoch': 0.96}
{'loss': 0.0233, 'learning_rate': 2.726150785610345e-06, 'epoch': 0.95}
{'loss': 0.0391, 'learning_rate': 3.3892685442723204e-06, 'epoch': 0.93}
{'loss': 0.0701, 'learning_rate': 3.905026801009413e-06, 'epoch': 0.92}
{'loss': 0.0324, 'learning_rate': 2.136712777910811e-06, 'epoch': 0.96}
{'loss': 0.0644, 'learning_rate': 2.652471034647903e-06, 'epoch': 0.95}
{'loss': 0.0385, 'learning_rate': 3.315588793309879e-06, 'epoch': 0.93}
{'loss': 0.0419, 'learning_rate': 3.831347050046971e-06, 'epoch': 0.92}
{'loss': 0.0327, 'learning_rate': 2.5787912836854614e-06, 'epoch': 0.95}
{'loss': 0.0463, 'learning_rate': 2.063033026948369e-06, 'epoch': 0.96}
{'loss': 0.0241, 'learning_rate': 3.2419090423474375e-06, 'epoch': 0.94}
{'loss': 0.0287, 'learning_rate': 3.7576672990845293e-06, 'epoch': 0.93}
{'loss': 0.0578, 'learning_rate': 2.50511153272302e-06, 'epoch': 0.95}
{'eval_loss': 0.044028397649526596, 'eval_accuracy': 0.9860828124460407, 'eval_f1': 0.9860962566844919, 'eval_precision': 0.9885518624840037, 'eval_recall': 0.983652820318684, 'eval_runtime': 355.1439, 'eval_samples_per_second': 163.072, 'eval_steps_per_second': 20.386, 'epoch': 1.0}
{'loss': 0.047, 'learning_rate': 1.9893532759859277e-06, 'epoch': 0.96}
{'train_runtime': 15054.3617, 'train_samples_per_second': 23.082, 'train_steps_per_second': 0.361, 'train_loss': 0.06415148869329755, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0235, 'learning_rate': 3.1682292913849956e-06, 'epoch': 0.94}
{'loss': 0.0649, 'learning_rate': 3.683987548122088e-06, 'epoch': 0.93}
{'loss': 0.0213, 'learning_rate': 2.4314317817605777e-06, 'epoch': 0.95}
{'loss': 0.0438, 'learning_rate': 1.9156735250234854e-06, 'epoch': 0.96}
{'loss': 0.0459, 'learning_rate': 3.0945495404225537e-06, 'epoch': 0.94}
{'loss': 0.0261, 'learning_rate': 2.3577520307981362e-06, 'epoch': 0.95}
{'loss': 0.0756, 'learning_rate': 3.610307797159646e-06, 'epoch': 0.93}
{'loss': 0.0372, 'learning_rate': 1.841993774061044e-06, 'epoch': 0.96}
{'loss': 0.0366, 'learning_rate': 3.020869789460112e-06, 'epoch': 0.94}
{'loss': 0.0581, 'learning_rate': 2.2840722798356944e-06, 'epoch': 0.95}
{'loss': 0.0427, 'learning_rate': 3.5366280461972046e-06, 'epoch': 0.93}
{'loss': 0.0264, 'learning_rate': 1.7683140230986023e-06, 'epoch': 0.97}
{'loss': 0.0155, 'learning_rate': 2.2103925288732525e-06, 'epoch': 0.96}
{'loss': 0.0581, 'learning_rate': 2.9471900384976704e-06, 'epoch': 0.94}
{'loss': 0.0303, 'learning_rate': 3.4629482952347623e-06, 'epoch': 0.93}
{'loss': 0.0436, 'learning_rate': 1.6946342721361602e-06, 'epoch': 0.97}
{'loss': 0.0452, 'learning_rate': 2.136712777910811e-06, 'epoch': 0.96}
{'loss': 0.0522, 'learning_rate': 2.8735102875352285e-06, 'epoch': 0.94}
{'loss': 0.0438, 'learning_rate': 3.3892685442723204e-06, 'epoch': 0.93}
{'loss': 0.0535, 'learning_rate': 1.6209545211737187e-06, 'epoch': 0.97}
{'loss': 0.0693, 'learning_rate': 2.063033026948369e-06, 'epoch': 0.96}
{'loss': 0.0221, 'learning_rate': 3.315588793309879e-06, 'epoch': 0.93}
{'loss': 0.0332, 'learning_rate': 2.7998305365727866e-06, 'epoch': 0.94}
{'loss': 0.0447, 'learning_rate': 1.5472747702112769e-06, 'epoch': 0.97}
{'loss': 0.0532, 'learning_rate': 1.9893532759859277e-06, 'epoch': 0.96}
{'loss': 0.0227, 'learning_rate': 3.2419090423474375e-06, 'epoch': 0.94}
{'loss': 0.0197, 'learning_rate': 2.726150785610345e-06, 'epoch': 0.95}
{'loss': 0.0556, 'learning_rate': 1.4735950192488352e-06, 'epoch': 0.97}
{'loss': 0.0542, 'learning_rate': 1.9156735250234854e-06, 'epoch': 0.96}
{'loss': 0.0506, 'learning_rate': 3.1682292913849956e-06, 'epoch': 0.94}
{'loss': 0.0708, 'learning_rate': 2.652471034647903e-06, 'epoch': 0.95}
{'loss': 0.0514, 'learning_rate': 1.3999152682863933e-06, 'epoch': 0.97}
{'loss': 0.0814, 'learning_rate': 1.841993774061044e-06, 'epoch': 0.96}
{'loss': 0.0473, 'learning_rate': 3.0945495404225537e-06, 'epoch': 0.94}
{'loss': 0.0501, 'learning_rate': 2.5787912836854614e-06, 'epoch': 0.95}
{'loss': 0.0171, 'learning_rate': 1.3262355173239514e-06, 'epoch': 0.97}
{'loss': 0.054, 'learning_rate': 1.7683140230986023e-06, 'epoch': 0.97}
{'loss': 0.0707, 'learning_rate': 3.020869789460112e-06, 'epoch': 0.94}
{'loss': 0.0446, 'learning_rate': 2.50511153272302e-06, 'epoch': 0.95}
{'loss': 0.0334, 'learning_rate': 1.25255576636151e-06, 'epoch': 0.98}
{'loss': 0.0525, 'learning_rate': 1.6946342721361602e-06, 'epoch': 0.97}
{'loss': 0.0177, 'learning_rate': 2.9471900384976704e-06, 'epoch': 0.94}
{'loss': 0.0473, 'learning_rate': 2.4314317817605777e-06, 'epoch': 0.95}
{'loss': 0.0215, 'learning_rate': 1.1788760153990681e-06, 'epoch': 0.98}
{'loss': 0.0308, 'learning_rate': 1.6209545211737187e-06, 'epoch': 0.97}
{'loss': 0.0364, 'learning_rate': 2.8735102875352285e-06, 'epoch': 0.94}
{'loss': 0.0494, 'learning_rate': 2.3577520307981362e-06, 'epoch': 0.95}
{'loss': 0.0412, 'learning_rate': 1.1051962644366262e-06, 'epoch': 0.98}
{'loss': 0.0536, 'learning_rate': 1.5472747702112769e-06, 'epoch': 0.97}
{'loss': 0.0482, 'learning_rate': 2.7998305365727866e-06, 'epoch': 0.94}
{'loss': 0.07, 'learning_rate': 2.2840722798356944e-06, 'epoch': 0.95}
{'loss': 0.1129, 'learning_rate': 1.0315165134741846e-06, 'epoch': 0.98}
{'loss': 0.0434, 'learning_rate': 1.4735950192488352e-06, 'epoch': 0.97}
{'loss': 0.0678, 'learning_rate': 2.726150785610345e-06, 'epoch': 0.95}
{'loss': 0.0536, 'learning_rate': 2.2103925288732525e-06, 'epoch': 0.96}
{'loss': 0.0765, 'learning_rate': 9.578367625117427e-07, 'epoch': 0.98}
{'loss': 0.0369, 'learning_rate': 1.3999152682863933e-06, 'epoch': 0.97}
{'loss': 0.0258, 'learning_rate': 2.652471034647903e-06, 'epoch': 0.95}
{'loss': 0.0265, 'learning_rate': 2.136712777910811e-06, 'epoch': 0.96}
{'loss': 0.0315, 'learning_rate': 8.841570115493011e-07, 'epoch': 0.98}
{'loss': 0.03, 'learning_rate': 1.3262355173239514e-06, 'epoch': 0.97}
{'loss': 0.0303, 'learning_rate': 2.5787912836854614e-06, 'epoch': 0.95}
{'loss': 0.0439, 'learning_rate': 2.063033026948369e-06, 'epoch': 0.96}
{'loss': 0.0413, 'learning_rate': 1.25255576636151e-06, 'epoch': 0.98}
{'loss': 0.0421, 'learning_rate': 8.104772605868594e-07, 'epoch': 0.98}
{'loss': 0.0276, 'learning_rate': 2.50511153272302e-06, 'epoch': 0.95}
{'loss': 0.0355, 'learning_rate': 1.9893532759859277e-06, 'epoch': 0.96}
{'loss': 0.0453, 'learning_rate': 1.1788760153990681e-06, 'epoch': 0.98}
{'loss': 0.0392, 'learning_rate': 7.367975096244176e-07, 'epoch': 0.99}
{'loss': 0.0276, 'learning_rate': 2.4314317817605777e-06, 'epoch': 0.95}
{'loss': 0.0596, 'learning_rate': 1.9156735250234854e-06, 'epoch': 0.96}
{'loss': 0.0495, 'learning_rate': 1.1051962644366262e-06, 'epoch': 0.98}
{'loss': 0.0491, 'learning_rate': 6.631177586619757e-07, 'epoch': 0.99}
{'loss': 0.0419, 'learning_rate': 2.3577520307981362e-06, 'epoch': 0.95}
{'loss': 0.0442, 'learning_rate': 1.841993774061044e-06, 'epoch': 0.96}
{'loss': 0.0302, 'learning_rate': 1.0315165134741846e-06, 'epoch': 0.98}
{'loss': 0.0329, 'learning_rate': 5.894380076995341e-07, 'epoch': 0.99}
{'loss': 0.0509, 'learning_rate': 2.2840722798356944e-06, 'epoch': 0.95}
{'loss': 0.0737, 'learning_rate': 1.7683140230986023e-06, 'epoch': 0.97}
{'loss': 0.0484, 'learning_rate': 9.578367625117427e-07, 'epoch': 0.98}
{'loss': 0.0263, 'learning_rate': 5.157582567370923e-07, 'epoch': 0.99}
{'loss': 0.0541, 'learning_rate': 2.2103925288732525e-06, 'epoch': 0.96}
{'loss': 0.0248, 'learning_rate': 8.841570115493011e-07, 'epoch': 0.98}
{'loss': 0.0585, 'learning_rate': 1.6946342721361602e-06, 'epoch': 0.97}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.059, 'learning_rate': 4.4207850577465057e-07, 'epoch': 0.99}
{'loss': 0.0537, 'learning_rate': 2.136712777910811e-06, 'epoch': 0.96}
{'loss': 0.0856, 'learning_rate': 8.104772605868594e-07, 'epoch': 0.98}
{'loss': 0.041, 'learning_rate': 1.6209545211737187e-06, 'epoch': 0.97}
{'loss': 0.0461, 'learning_rate': 3.683987548122088e-07, 'epoch': 0.99}
{'loss': 0.0578, 'learning_rate': 2.063033026948369e-06, 'epoch': 0.96}
{'loss': 0.0357, 'learning_rate': 7.367975096244176e-07, 'epoch': 0.99}
{'loss': 0.035, 'learning_rate': 1.5472747702112769e-06, 'epoch': 0.97}
{'loss': 0.0213, 'learning_rate': 2.9471900384976703e-07, 'epoch': 0.99}
{'loss': 0.0393, 'learning_rate': 1.9893532759859277e-06, 'epoch': 0.96}
{'loss': 0.0439, 'learning_rate': 6.631177586619757e-07, 'epoch': 0.99}
{'loss': 0.0217, 'learning_rate': 1.4735950192488352e-06, 'epoch': 0.97}
{'loss': 0.0579, 'learning_rate': 2.2103925288732528e-07, 'epoch': 1.0}
{'loss': 0.0398, 'learning_rate': 1.9156735250234854e-06, 'epoch': 0.96}
{'loss': 0.0461, 'learning_rate': 5.894380076995341e-07, 'epoch': 0.99}
{'loss': 0.0524, 'learning_rate': 1.3999152682863933e-06, 'epoch': 0.97}
{'loss': 0.0702, 'learning_rate': 1.4735950192488351e-07, 'epoch': 1.0}
{'loss': 0.0405, 'learning_rate': 1.841993774061044e-06, 'epoch': 0.96}
{'loss': 0.0613, 'learning_rate': 5.157582567370923e-07, 'epoch': 0.99}
{'loss': 0.0394, 'learning_rate': 1.3262355173239514e-06, 'epoch': 0.97}
{'loss': 0.0418, 'learning_rate': 7.367975096244176e-08, 'epoch': 1.0}
{'loss': 0.0425, 'learning_rate': 1.7683140230986023e-06, 'epoch': 0.97}
{'loss': 0.0551, 'learning_rate': 4.4207850577465057e-07, 'epoch': 0.99}
{'loss': 0.0309, 'learning_rate': 1.25255576636151e-06, 'epoch': 0.98}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0171, 'learning_rate': 1.6946342721361602e-06, 'epoch': 0.97}
{'loss': 0.0316, 'learning_rate': 3.683987548122088e-07, 'epoch': 0.99}
{'loss': 0.0472, 'learning_rate': 1.1788760153990681e-06, 'epoch': 0.98}
{'loss': 0.0495, 'learning_rate': 2.9471900384976703e-07, 'epoch': 0.99}
{'loss': 0.0291, 'learning_rate': 1.6209545211737187e-06, 'epoch': 0.97}
{'loss': 0.0566, 'learning_rate': 1.1051962644366262e-06, 'epoch': 0.98}
{'loss': 0.0284, 'learning_rate': 2.2103925288732528e-07, 'epoch': 1.0}
{'loss': 0.0518, 'learning_rate': 1.5472747702112769e-06, 'epoch': 0.97}
{'loss': 0.0418, 'learning_rate': 1.0315165134741846e-06, 'epoch': 0.98}
{'loss': 0.0354, 'learning_rate': 1.4735950192488351e-07, 'epoch': 1.0}
{'loss': 0.037, 'learning_rate': 1.4735950192488352e-06, 'epoch': 0.97}
{'loss': 0.0457, 'learning_rate': 9.578367625117427e-07, 'epoch': 0.98}
{'loss': 0.0513, 'learning_rate': 7.367975096244176e-08, 'epoch': 1.0}
{'loss': 0.0504, 'learning_rate': 1.3999152682863933e-06, 'epoch': 0.97}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0658, 'learning_rate': 8.841570115493011e-07, 'epoch': 0.98}
{'loss': 0.0329, 'learning_rate': 1.3262355173239514e-06, 'epoch': 0.97}
{'loss': 0.0485, 'learning_rate': 8.104772605868594e-07, 'epoch': 0.98}
{'loss': 0.0595, 'learning_rate': 1.25255576636151e-06, 'epoch': 0.98}
{'loss': 0.0308, 'learning_rate': 7.367975096244176e-07, 'epoch': 0.99}
{'loss': 0.0267, 'learning_rate': 1.1788760153990681e-06, 'epoch': 0.98}
{'loss': 0.0287, 'learning_rate': 6.631177586619757e-07, 'epoch': 0.99}
{'loss': 0.0713, 'learning_rate': 1.1051962644366262e-06, 'epoch': 0.98}
{'loss': 0.0429, 'learning_rate': 5.894380076995341e-07, 'epoch': 0.99}
{'loss': 0.033, 'learning_rate': 1.0315165134741846e-06, 'epoch': 0.98}
{'loss': 0.0572, 'learning_rate': 5.157582567370923e-07, 'epoch': 0.99}
{'loss': 0.0318, 'learning_rate': 9.578367625117427e-07, 'epoch': 0.98}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0469, 'learning_rate': 4.4207850577465057e-07, 'epoch': 0.99}
{'loss': 0.0532, 'learning_rate': 8.841570115493011e-07, 'epoch': 0.98}
{'loss': 0.0437, 'learning_rate': 3.683987548122088e-07, 'epoch': 0.99}
{'loss': 0.058, 'learning_rate': 8.104772605868594e-07, 'epoch': 0.98}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0142, 'learning_rate': 2.9471900384976703e-07, 'epoch': 0.99}
{'loss': 0.0283, 'learning_rate': 7.367975096244176e-07, 'epoch': 0.99}
{'loss': 0.0478, 'learning_rate': 2.2103925288732528e-07, 'epoch': 1.0}
{'loss': 0.0346, 'learning_rate': 6.631177586619757e-07, 'epoch': 0.99}
{'eval_loss': 0.0417143739759922, 'eval_accuracy': 0.9862382152847325, 'eval_f1': 0.9862493745794587, 'eval_precision': 0.9888596734016053, 'eval_recall': 0.983652820318684, 'eval_runtime': 345.5122, 'eval_samples_per_second': 167.618, 'eval_steps_per_second': 20.954, 'epoch': 1.0}
{'train_runtime': 16018.256, 'train_samples_per_second': 21.693, 'train_steps_per_second': 0.339, 'train_loss': 0.06319075650460901, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.039, 'learning_rate': 1.4735950192488351e-07, 'epoch': 1.0}
{'loss': 0.0432, 'learning_rate': 5.894380076995341e-07, 'epoch': 0.99}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0918, 'learning_rate': 5.157582567370923e-07, 'epoch': 0.99}
{'loss': 0.0514, 'learning_rate': 7.367975096244176e-08, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0463, 'learning_rate': 4.4207850577465057e-07, 'epoch': 0.99}
{'loss': 0.0613, 'learning_rate': 3.683987548122088e-07, 'epoch': 0.99}
{'loss': 0.0474, 'learning_rate': 2.9471900384976703e-07, 'epoch': 0.99}
{'eval_loss': 0.04174569994211197, 'eval_accuracy': 0.986445419069655, 'eval_f1': 0.9864250263717641, 'eval_precision': 0.9913451511991658, 'eval_recall': 0.9815534982964518, 'eval_runtime': 349.7921, 'eval_samples_per_second': 165.567, 'eval_steps_per_second': 20.698, 'epoch': 1.0}
{'train_runtime': 15284.9568, 'train_samples_per_second': 22.734, 'train_steps_per_second': 0.355, 'train_loss': 0.06320332555447873, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.038, 'learning_rate': 2.2103925288732528e-07, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.0632, 'learning_rate': 1.4735950192488351e-07, 'epoch': 1.0}
{'loss': 0.0314, 'learning_rate': 7.367975096244176e-08, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_loss': 0.04314026981592178, 'eval_accuracy': 0.9859964775356563, 'eval_f1': 0.9860010702017848, 'eval_precision': 0.9890912868818396, 'eval_recall': 0.9829301029011942, 'eval_runtime': 391.9804, 'eval_samples_per_second': 147.747, 'eval_steps_per_second': 18.47, 'epoch': 1.0}
{'train_runtime': 16378.4605, 'train_samples_per_second': 21.216, 'train_steps_per_second': 0.331, 'train_loss': 0.06400844947719557, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_loss': 0.04281715676188469, 'eval_accuracy': 0.9857892737507339, 'eval_f1': 0.9857654323122956, 'eval_precision': 0.990855354659249, 'eval_recall': 0.9807275355336064, 'eval_runtime': 370.4319, 'eval_samples_per_second': 156.342, 'eval_steps_per_second': 19.545, 'epoch': 1.0}
{'train_runtime': 16034.5311, 'train_samples_per_second': 21.671, 'train_steps_per_second': 0.339, 'train_loss': 0.06322731973552247, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
